{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import e2c_1 as e2c_util\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\", \"2\", \"3\"\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import datetime\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "\n",
    "\n",
    "# GPU memory management\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.session specification\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.75\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(x, t_decoded):\n",
    "    '''\n",
    "    Reconstruction loss for the plain VAE\n",
    "    '''\n",
    "    v = 0.1\n",
    "    # return K.mean(K.sum((K.batch_flatten(x) - K.batch_flatten(t_decoded)) ** 2 / (2*v) + 0.5*K.log(2*np.pi*v), axis=-1))\n",
    "    return K.mean(K.sum((K.batch_flatten(x) - K.batch_flatten(t_decoded)) ** 2 / (2*v), axis=-1))\n",
    "    # return K.sum((K.batch_flatten(x) - K.batch_flatten(t_decoded)) ** 2, axis=-1)\n",
    "\n",
    "\n",
    "def l2_reg_loss(qm):\n",
    "    # 0.5 * (torch.log(pv) - torch.log(qv) + qv / pv + (qm - pm).pow(2) / pv - 1)\n",
    "    # -0.5 * K.sum(1 + t_log_var - K.square(t_mean) - K.exp(t_log_var), axis=-1)\n",
    "#     kl = -0.5 * (1 - p_logv + q_logv - K.exp(q_logv) / K.exp(p_logv) - K.square(qm - pm) / K.exp(p_logv))\n",
    "    l2_reg = 0.5*K.square(qm)\n",
    "    return K.mean(K.sum(l2_reg, axis=-1))\n",
    "\n",
    "\n",
    "def get_flux_loss(m, state, state_pred):\n",
    "    '''\n",
    "    @params:  state, state_pred shape (batch_size, 60, 60, 2)\n",
    "              p, p_pred shape (batch_size, 60, 60, 1)\n",
    "              m shape (batch_size, 60, 60, 1)\n",
    "    \n",
    "    @return:  loss_flux: scalar\n",
    "    \n",
    "    Only consider discrepancies in total flux, not in phases (saturation not used) \n",
    "    '''\n",
    "    \n",
    "    perm = K.exp(m)\n",
    "    p = K.expand_dims(state[:, :, :, 1], -1)\n",
    "    p_pred = K.expand_dims(state_pred[:, :, :, 1], -1)\n",
    "\n",
    "    #print(K.int_shape(xxx))\n",
    "    \n",
    "    tran_x = 1./perm[:, 1:, ...] + 1./perm[:, :-1, ...]\n",
    "    tran_y = 1./perm[:, :, 1:, ...] + 1./perm[:, :, :-1, ...]\n",
    "    flux_x = (p[:, 1:, ...] - p[:, :-1, ...]) / tran_x\n",
    "    flux_y = (p[:, :, 1:, :] - p[:, :, :-1, :]) / tran_y\n",
    "    flux_x_pred = (p_pred[:, 1:, ...] - p_pred[:, :-1, ...]) / tran_x\n",
    "    flux_y_pred = (p_pred[:, :, 1:, :] - p_pred[:, :, :-1, :]) / tran_y\n",
    "\n",
    "    loss_x = K.sum(K.abs(K.batch_flatten(flux_x) - K.batch_flatten(flux_x_pred)), axis=-1)\n",
    "    loss_y = K.sum(K.abs(K.batch_flatten(flux_y) - K.batch_flatten(flux_y_pred)), axis=-1)\n",
    "\n",
    "    loss_flux = K.mean(loss_x + loss_y)\n",
    "    return loss_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_sat_loss(state, state_pred):\n",
    "    \n",
    "    sat_threshold = 0.105\n",
    "    sat = K.expand_dims(state[:, :, :, 0], -1)\n",
    "    sat_pred = K.expand_dims(state_pred[:, :, :, 0], -1)\n",
    "    \n",
    "    \n",
    "    sat_bool = K.greater_equal(sat, sat_threshold) #will return boolean values\n",
    "    sat_bin = K.cast(sat_bool, dtype=K.floatx()) #will convert bool to 0 and 1  \n",
    "    \n",
    "    sat_pred_bool = K.greater_equal(sat_pred, sat_threshold) #will return boolean values\n",
    "    sat_pred_bin = K.cast(sat_pred_bool, dtype=K.floatx()) #will convert bool to 0 and 1  \n",
    "    \n",
    "#     binary_loss = K.sum(K.abs(K.batch_flatten(sat_bin) - K.batch_flatten(sat_pred_bin)), axis=-1)\n",
    "    \n",
    "    binary_loss = losses.binary_crossentropy(sat_bin, sat_pred_bin)\n",
    "    return K.mean(binary_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_well_bhp_loss(state, state_pred, wl_mask):\n",
    "    '''\n",
    "    @params: state: shape (batch_size, 60, 60, 2)\n",
    "             state_pred: shape (batch_size, 60, 60, 2)\n",
    "             prod_well_loc: shape (batch_size, 5, 2)\n",
    "             \n",
    "    p_true: shape (batch_size, 60, 60, 1)\n",
    "    p_pred: shape (batch_size, 60, 60, 1)\n",
    "    \n",
    "    @return: bhp_loss: scalar\n",
    "    '''\n",
    "    \n",
    "    p_true = K.expand_dims(state[:, :, :, 1], -1) # shape (batch_size, 60, 60 ,1)\n",
    "    p_pred = K.expand_dims(state_pred[:, :, :, 1], -1)\n",
    "    \n",
    "    bhp_loss = K.mean(K.sum(K.abs(p_true -p_pred) * wl_mask[:,:,np.newaxis], axis=(1,2,3)))\n",
    "    \n",
    "    return bhp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_e2c(latent_dim, u_dim, input_shape, sigma=0.0):\n",
    "    '''\n",
    "    Creates a E2C.\n",
    "\n",
    "    Args:\n",
    "        latent_dim: dimensionality of latent space\n",
    "        return_kl_loss_op: whether to return the operation for\n",
    "                           computing the KL divergence loss.\n",
    "\n",
    "    Returns:\n",
    "        The VAE model. If return_kl_loss_op is True, then the\n",
    "        operation for computing the KL divergence loss is\n",
    "        additionally returned.\n",
    "    '''\n",
    "\n",
    "    encoder_, hidden_shapes_ = e2c_util.create_encoder(latent_dim, input_shape, sigma=sigma)\n",
    "    decoder_ = e2c_util.create_decoder(latent_dim, input_shape, hidden_shapes_)\n",
    "    transition_ = e2c_util.create_trans(latent_dim, u_dim)\n",
    "#     wc_encoder_ = e2c_util.create_wc_encoder(latent_dim, input_shape)\n",
    "    wc_encoder_, _ = e2c_util.create_encoder(latent_dim, input_shape) # input_shape (60,60,2)\n",
    "\n",
    "    return encoder_, decoder_, transition_, wc_encoder_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Create plain E2C model and associated loss operations\n",
    "## -- It should be put int0 main function from this part on,\n",
    "## -- if converted to .py file\n",
    "\n",
    "\n",
    "################### case specification ######################\n",
    "\n",
    "# data_dir = '/data/cees/zjin/lstm_rom/datasets/9W_BHP/'\n",
    "# data_dir = '/data/cees/zjin/lstm_rom/datasets/9W_BHP_RATE/'\n",
    "# data_dir = '/data/cees/zjin/lstm_rom/datasets/9W_MS_BHP_RATE/'\n",
    "data_dir = '/data3/Astro/personal/zjin/datasets/9W_MS_BHP_RATE_GAU/'\n",
    "# data_dir = '/data3/Astro/personal/zjin/datasets/7W_CHA/'\n",
    "\n",
    "output_dir = '/data3/Astro/lstm_rom/e2c_larry/saved_models/'\n",
    "\n",
    "# case_name = '9w_bhp'\n",
    "# case_name = '9w_bhp_rate'\n",
    "case_name = '9w_ms_bhp_rate'\n",
    "# case_name = '7w_cha'\n",
    "\n",
    "# case_suffix = '_single_out_rel_2'\n",
    "# case_suffix = '_fix_wl_rel_8'\n",
    "case_suffix = '_var_wl_rel_1'\n",
    "\n",
    "train_suffix = '_with_p'\n",
    "\n",
    "model_suffix = '_flux_loss'\n",
    "# model_suffix = '_ae_no_l2_ep_10'\n",
    "# model_suffix = '_no_fl'\n",
    "\n",
    "\n",
    "n_train_run = 300\n",
    "n_eval_run = 100\n",
    "num_t = 20 \n",
    "# dt = 200 // num_t\n",
    "dt = 100\n",
    "n_train_step = n_train_run * num_t\n",
    "n_eval_step = n_eval_run * num_t\n",
    "\n",
    "\n",
    "train_file = case_name + '_e2c_train' + case_suffix + train_suffix + '_n%d_dt%dday_nt%d_nrun%d.mat' %(n_train_step, dt, num_t, n_train_run)\n",
    "eval_file = case_name + '_e2c_eval' + case_suffix + train_suffix +'_n%d_dt%dday_nt%d_nrun%d.mat' %(n_eval_step, dt, num_t, n_eval_run)\n",
    "\n",
    "#################### model specification ##################\n",
    "epoch = 20\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4\n",
    "latent_dim = 50\n",
    "\n",
    "# u_dim = 9*2 # control dimension\n",
    "u_dim = latent_dim # wwll control encoded to latent dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "hf_r = h5py.File(data_dir + train_file, 'r')\n",
    "state_t_train = np.array(hf_r.get('state_t'))\n",
    "state_t1_train = np.array(hf_r.get('state_t1'))\n",
    "bhp_train = np.array(hf_r.get('bhp'))\n",
    "dt_train = np.array(hf_r.get('dt'))\n",
    "wl_mask_train = np.array(hf_r.get('wl_mask'))\n",
    "hf_r.close()\n",
    "\n",
    "num_train = state_t_train.shape[0]\n",
    "# dt_train = np.ones((num_train,1)) # dt=20days, normalized to 1\n",
    "\n",
    "hf_r = h5py.File(data_dir + eval_file, 'r')\n",
    "state_t_eval = np.array(hf_r.get('state_t'))\n",
    "state_t1_eval = np.array(hf_r.get('state_t1'))\n",
    "bhp_eval = np.array(hf_r.get('bhp'))\n",
    "dt_eval = np.array(hf_r.get('dt'))\n",
    "wl_mask_eval = np.array(hf_r.get('wl_mask'))\n",
    "hf_r.close()\n",
    "\n",
    "num_eval = state_t_eval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m shape is  (60, 60, 1)\n",
      "m_eval shape is  (2000, 60, 60, 1)\n",
      "m shape is  (6000, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "# m = np.loadtxt(\"/data/cees/zjin/lstm_rom/sim_runs/case6_9w_bhp_rate_ms_h5/template/logk1.dat\")\n",
    "m = np.loadtxt(\"/data3/Astro/personal/zjin/sim_runs/case8_9w_bhp_rate_ms_gau/template/logk1.dat\") # Gaussian\n",
    "# m = np.loadtxt(\"/data3/Astro/personal/zjin/sim_runs/case9_cha/template/logk1.dat\") # channelized\n",
    "\n",
    "m = m.reshape(60, 60, 1)\n",
    "print('m shape is ', m.shape)\n",
    "#     m_tf = K.placeholder((batch_size, 60, 60 ,1))\n",
    "m_tf = Input(shape=(60, 60, 1))\n",
    "\n",
    "\n",
    "m_eval = np.repeat(np.expand_dims(m, axis = 0), state_t_eval.shape[0], axis = 0)\n",
    "print(\"m_eval shape is \", m_eval.shape)\n",
    "m = np.repeat(np.expand_dims(m,axis = 0), state_t_train.shape[0], axis = 0)\n",
    "print(\"m shape is \", m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(e2c_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct E2C\n",
    "input_shape = (60, 60, 2)\n",
    "\n",
    "#############################################\n",
    "# here we use a UAE framework, sigma = 0.0001\n",
    "#############################################\n",
    "encoder, decoder, transition, wc_encoder = create_e2c(latent_dim, u_dim, input_shape, sigma=0.0) \n",
    "\n",
    "\n",
    "xt = Input(shape=input_shape)\n",
    "xt1 = Input(shape=input_shape)\n",
    "# ut = Input(shape=(u_dim, ))\n",
    "ut = Input(shape=input_shape) # (60,60,2)\n",
    "dt = Input(shape=(1,))\n",
    "wl_mask = Input(shape=(60, 60)) # both prod and inj\n",
    "\n",
    "zt= encoder(xt)\n",
    "xt_rec = decoder(zt)\n",
    "\n",
    "ut_encoded = wc_encoder(ut)\n",
    "zt1 = encoder(xt1)\n",
    "\n",
    "zt1_pred = transition([zt, ut_encoded, dt])\n",
    "xt1_pred = decoder(zt1_pred)\n",
    "\n",
    "# Compute loss\n",
    "loss_rec_t = reconstruction_loss(xt, xt_rec)\n",
    "loss_rec_t1 = reconstruction_loss(xt1, xt1_pred)\n",
    "\n",
    "loss_flux_t = get_flux_loss(m_tf, xt, xt_rec) / 1000.\n",
    "loss_flux_t1 = get_flux_loss(m_tf, xt1, xt1_pred) / 1000.\n",
    "\n",
    "binary_sat_loss_t = get_binary_sat_loss(xt, xt_rec) * 1\n",
    "binary_sat_loss_t1 = get_binary_sat_loss(xt1, xt1_pred) * 1\n",
    "\n",
    "loss_prod_bhp_t = get_well_bhp_loss(xt, xt_rec, wl_mask) * 1\n",
    "loss_prod_bhp_t1 = get_well_bhp_loss(xt1, xt1_pred, wl_mask) * 1\n",
    "\n",
    "loss_l2_reg = l2_reg_loss(zt)  # log(1.) = 0.\n",
    "\n",
    "\n",
    "## -- loss bound: combine data losses\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_l2_reg  + loss_flux_t + loss_flux_t1\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_kl + binary_sat_loss_t + binary_sat_loss_t1\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_l2_reg  + loss_flux_t + loss_flux_t1 + loss_prod_bhp_t + loss_prod_bhp_t1 # JCP 2019 Gaussian case\n",
    "# loss_bound = loss_rec_t + loss_rec_t1  + loss_flux_t + loss_flux_t1 + loss_prod_bhp_t + loss_prod_bhp_t1 # UAE **\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_l2_reg # no flux/bhp loss comparison\n",
    "\n",
    "loss_bound = loss_rec_t + loss_flux_t + loss_prod_bhp_t # just reconstruction\n",
    "\n",
    "\n",
    "# Use zt_logvar to approximate zt1_logvar_pred\n",
    "loss_trans = l2_reg_loss(zt1_pred - zt1)\n",
    "# loss_trans = kl_normal_loss(zt1_mean_pred, zt1_logvar_pred, zt1_mean, zt1_logvar)\n",
    "\n",
    "\n",
    "trans_loss_weight = 1.0 # lambda in E2C paper Eq. (11)\n",
    "loss = loss_bound + trans_loss_weight * loss_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-27-14:21\n"
     ]
    }
   ],
   "source": [
    "## log for tensorboard\n",
    "def write_summary(value, tag, summary_writer, global_step):\n",
    "    \"\"\"Write a single summary value to tensorboard\"\"\"\n",
    "    summary = tf.Summary()\n",
    "    summary.value.add(tag=tag, simple_value=value)\n",
    "    summary_writer.add_summary(summary, global_step)\n",
    "\n",
    "## used to generate log directory\n",
    "currentDT = datetime.datetime.now()\n",
    "current_time = str(currentDT).replace(\" \", \"-\")[:-10]\n",
    "print(current_time)\n",
    "\n",
    "suffix = ''\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('logs/' + case_name + case_suffix + '_ep' + str(epoch) + '_tr' + str(n_train_run) + '_' + current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Batch 1/1500, Loss 8098.198242, Loss rec 7705.566895, loss rec t1 8593.504883, loss kl 0.235634, loss_trans 0.269626, loss flux 74.871117, loss flux t1 79.320747, prod bhp loss 317.490845, prod bhp loss t1 335.844727\n",
      "Epoch 1/20, Batch 101/1500, Loss 672.066284, Loss rec 438.714111, loss rec t1 514.988953, loss kl 4.465278, loss_trans 0.055676, loss flux 201.718079, loss flux t1 221.134216, prod bhp loss 31.578407, prod bhp loss t1 39.385521\n",
      "Epoch 1/20, Batch 201/1500, Loss 613.011353, Loss rec 419.524841, loss rec t1 491.309418, loss kl 7.946171, loss_trans 0.069762, loss flux 164.019135, loss flux t1 171.242691, prod bhp loss 29.397640, prod bhp loss t1 34.851440\n",
      "Epoch 1/20, Batch 301/1500, Loss 508.416168, Loss rec 346.250397, loss rec t1 437.358429, loss kl 5.795074, loss_trans 0.158001, loss flux 128.610870, loss flux t1 128.702271, prod bhp loss 33.396908, prod bhp loss t1 43.457573\n",
      "Epoch 1/20, Batch 401/1500, Loss 396.329956, Loss rec 261.361969, loss rec t1 311.616272, loss kl 5.001407, loss_trans 0.179302, loss flux 117.061378, loss flux t1 116.840630, prod bhp loss 17.727312, prod bhp loss t1 26.023067\n",
      "Epoch 1/20, Batch 501/1500, Loss 426.657501, Loss rec 285.937805, loss rec t1 373.638794, loss kl 5.149981, loss_trans 0.213264, loss flux 118.179008, loss flux t1 118.548637, prod bhp loss 22.327435, prod bhp loss t1 32.283875\n",
      "Epoch 1/20, Batch 601/1500, Loss 425.371552, Loss rec 266.276093, loss rec t1 308.218018, loss kl 6.858302, loss_trans 0.168250, loss flux 135.417740, loss flux t1 131.160065, prod bhp loss 23.509476, prod bhp loss t1 23.843027\n",
      "Epoch 1/20, Batch 701/1500, Loss 319.436432, Loss rec 199.404877, loss rec t1 240.501038, loss kl 3.828962, loss_trans 0.117139, loss flux 104.186798, loss flux t1 104.337753, prod bhp loss 15.727627, prod bhp loss t1 20.232376\n",
      "Epoch 1/20, Batch 801/1500, Loss 284.523163, Loss rec 167.807648, loss rec t1 217.512497, loss kl 2.890722, loss_trans 0.125770, loss flux 99.570335, loss flux t1 96.328850, prod bhp loss 17.019413, prod bhp loss t1 21.953270\n",
      "Epoch 1/20, Batch 901/1500, Loss 308.246063, Loss rec 174.595551, loss rec t1 190.707947, loss kl 5.756560, loss_trans 0.100309, loss flux 111.644478, loss flux t1 109.081161, prod bhp loss 21.905729, prod bhp loss t1 23.049850\n",
      "Epoch 1/20, Batch 1001/1500, Loss 204.507462, Loss rec 111.263428, loss rec t1 151.766190, loss kl 1.635122, loss_trans 0.078793, loss flux 78.387047, loss flux t1 78.618660, prod bhp loss 14.778185, prod bhp loss t1 21.942921\n",
      "Epoch 1/20, Batch 1101/1500, Loss 240.273102, Loss rec 136.628235, loss rec t1 198.442642, loss kl 3.415724, loss_trans 0.091250, loss flux 89.213783, loss flux t1 85.441643, prod bhp loss 14.339847, prod bhp loss t1 25.146969\n",
      "Epoch 1/20, Batch 1201/1500, Loss 261.032196, Loss rec 147.752136, loss rec t1 190.313873, loss kl 3.914903, loss_trans 0.083797, loss flux 94.146706, loss flux t1 91.009018, prod bhp loss 19.049551, prod bhp loss t1 24.165108\n",
      "Epoch 1/20, Batch 1301/1500, Loss 164.594971, Loss rec 85.636482, loss rec t1 102.338394, loss kl 1.167176, loss_trans 0.052340, loss flux 67.690422, loss flux t1 67.151535, prod bhp loss 11.215736, prod bhp loss t1 14.803576\n",
      "Epoch 1/20, Batch 1401/1500, Loss 153.192001, Loss rec 77.736496, loss rec t1 144.389435, loss kl 1.851866, loss_trans 0.111770, loss flux 63.158936, loss flux t1 75.684807, prod bhp loss 12.184797, prod bhp loss t1 24.125727\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 1/20, Train loss 191.630707, Eval loss 193.953873\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 2/20, Batch 1/1500, Loss 159.377289, Loss rec 85.097603, loss rec t1 138.332642, loss kl 1.551028, loss_trans 0.113771, loss flux 59.224949, loss flux t1 73.844444, prod bhp loss 14.940966, prod bhp loss t1 21.936153\n",
      "Epoch 2/20, Batch 101/1500, Loss 209.008896, Loss rec 134.949371, loss rec t1 217.098541, loss kl 0.711475, loss_trans 0.103662, loss flux 50.817913, loss flux t1 64.937416, prod bhp loss 23.137943, prod bhp loss t1 31.520029\n",
      "Epoch 2/20, Batch 201/1500, Loss 166.294006, Loss rec 89.406281, loss rec t1 154.323639, loss kl 1.867002, loss_trans 0.095743, loss flux 64.256920, loss flux t1 78.517471, prod bhp loss 12.535050, prod bhp loss t1 22.748459\n",
      "Epoch 2/20, Batch 301/1500, Loss 166.334244, Loss rec 81.831345, loss rec t1 121.467293, loss kl 1.456444, loss_trans 0.055983, loss flux 69.632874, loss flux t1 70.704079, prod bhp loss 14.814045, prod bhp loss t1 23.624191\n",
      "Epoch 2/20, Batch 401/1500, Loss 139.846558, Loss rec 66.780136, loss rec t1 103.034393, loss kl 1.269288, loss_trans 0.057858, loss flux 62.172161, loss flux t1 62.322395, prod bhp loss 10.836390, prod bhp loss t1 20.099667\n",
      "Epoch 2/20, Batch 501/1500, Loss 175.297852, Loss rec 94.632828, loss rec t1 139.671173, loss kl 1.479544, loss_trans 0.089928, loss flux 66.776062, loss flux t1 67.597313, prod bhp loss 13.799029, prod bhp loss t1 26.305346\n",
      "Epoch 2/20, Batch 601/1500, Loss 186.944687, Loss rec 99.855347, loss rec t1 138.352554, loss kl 2.313104, loss_trans 0.049937, loss flux 71.446533, loss flux t1 74.669365, prod bhp loss 15.592866, prod bhp loss t1 20.618187\n",
      "Epoch 2/20, Batch 701/1500, Loss 143.092499, Loss rec 71.417595, loss rec t1 84.600342, loss kl 1.243221, loss_trans 0.035980, loss flux 59.865841, loss flux t1 61.398838, prod bhp loss 11.773087, prod bhp loss t1 14.638898\n",
      "Epoch 2/20, Batch 801/1500, Loss 139.713593, Loss rec 71.219391, loss rec t1 95.567093, loss kl 0.897576, loss_trans 0.033140, loss flux 57.652821, loss flux t1 57.557217, prod bhp loss 10.808237, prod bhp loss t1 16.509987\n",
      "Epoch 2/20, Batch 901/1500, Loss 170.891190, Loss rec 81.033836, loss rec t1 92.146034, loss kl 2.286335, loss_trans 0.033200, loss flux 69.898354, loss flux t1 71.567101, prod bhp loss 19.925802, prod bhp loss t1 20.243633\n",
      "Epoch 2/20, Batch 1001/1500, Loss 134.922165, Loss rec 67.816757, loss rec t1 104.656357, loss kl 0.532822, loss_trans 0.029518, loss flux 51.022472, loss flux t1 52.870510, prod bhp loss 16.053425, prod bhp loss t1 25.190472\n",
      "Epoch 2/20, Batch 1101/1500, Loss 136.212738, Loss rec 65.416832, loss rec t1 90.798416, loss kl 1.618132, loss_trans 0.039780, loss flux 59.009617, loss flux t1 59.292465, prod bhp loss 11.746512, prod bhp loss t1 18.222416\n",
      "Epoch 2/20, Batch 1201/1500, Loss 201.856628, Loss rec 115.452232, loss rec t1 89.825073, loss kl 1.974984, loss_trans 0.068087, loss flux 61.618153, loss flux t1 62.986481, prod bhp loss 24.718159, prod bhp loss t1 13.767830\n",
      "Epoch 2/20, Batch 1301/1500, Loss 106.811638, Loss rec 44.471550, loss rec t1 55.373135, loss kl 0.545274, loss_trans 0.022708, loss flux 51.263149, loss flux t1 50.922142, prod bhp loss 11.054235, prod bhp loss t1 14.387461\n",
      "Epoch 2/20, Batch 1401/1500, Loss 92.655594, Loss rec 38.404942, loss rec t1 96.468010, loss kl 1.096292, loss_trans 0.075538, loss flux 45.505470, loss flux t1 59.932636, prod bhp loss 8.669640, prod bhp loss t1 21.045959\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 2/20, Train loss 94.855629, Eval loss 167.133820\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 3/20, Batch 1/1500, Loss 86.657478, Loss rec 35.618935, loss rec t1 98.671082, loss kl 0.798972, loss_trans 0.080258, loss flux 41.501831, loss flux t1 57.538651, prod bhp loss 9.456450, prod bhp loss t1 22.873463\n",
      "Epoch 3/20, Batch 101/1500, Loss 99.400436, Loss rec 48.519463, loss rec t1 137.075104, loss kl 0.399005, loss_trans 0.066537, loss flux 37.489300, loss flux t1 51.999477, prod bhp loss 13.325140, prod bhp loss t1 28.052429\n",
      "Epoch 3/20, Batch 201/1500, Loss 112.789383, Loss rec 57.380375, loss rec t1 123.965378, loss kl 1.247755, loss_trans 0.061714, loss flux 45.865414, loss flux t1 61.274517, prod bhp loss 9.481878, prod bhp loss t1 20.999039\n",
      "Epoch 3/20, Batch 301/1500, Loss 104.258667, Loss rec 41.308990, loss rec t1 61.548473, loss kl 0.873135, loss_trans 0.026081, loss flux 53.953556, loss flux t1 55.973495, prod bhp loss 8.970048, prod bhp loss t1 16.382999\n",
      "Epoch 3/20, Batch 401/1500, Loss 92.225914, Loss rec 37.148178, loss rec t1 50.552231, loss kl 0.759283, loss_trans 0.026898, loss flux 46.906128, loss flux t1 48.039307, prod bhp loss 8.144705, prod bhp loss t1 13.668045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Batch 501/1500, Loss 133.767517, Loss rec 66.767303, loss rec t1 98.238159, loss kl 1.012325, loss_trans 0.059785, loss flux 52.567505, loss flux t1 54.096107, prod bhp loss 14.372931, prod bhp loss t1 24.943945\n",
      "Epoch 3/20, Batch 601/1500, Loss 122.362473, Loss rec 54.737404, loss rec t1 68.182030, loss kl 1.579574, loss_trans 0.028503, loss flux 55.658260, loss flux t1 57.787800, prod bhp loss 11.938307, prod bhp loss t1 14.589559\n",
      "Epoch 3/20, Batch 701/1500, Loss 98.870918, Loss rec 42.943928, loss rec t1 48.502678, loss kl 0.864948, loss_trans 0.014382, loss flux 48.489822, loss flux t1 50.228676, prod bhp loss 7.422793, prod bhp loss t1 10.278042\n",
      "Epoch 3/20, Batch 801/1500, Loss 87.735382, Loss rec 37.677307, loss rec t1 48.880745, loss kl 0.669064, loss_trans 0.020694, loss flux 43.825905, loss flux t1 45.513710, prod bhp loss 6.211479, prod bhp loss t1 12.184855\n",
      "Epoch 3/20, Batch 901/1500, Loss 102.861656, Loss rec 40.594734, loss rec t1 45.609966, loss kl 1.629957, loss_trans 0.024618, loss flux 52.875153, loss flux t1 54.030651, prod bhp loss 9.367149, prod bhp loss t1 10.383266\n",
      "Epoch 3/20, Batch 1001/1500, Loss 91.883965, Loss rec 40.426971, loss rec t1 51.195938, loss kl 0.404559, loss_trans 0.019234, loss flux 43.356876, loss flux t1 44.651478, prod bhp loss 8.080891, prod bhp loss t1 11.387661\n",
      "Epoch 3/20, Batch 1101/1500, Loss 96.829269, Loss rec 39.363663, loss rec t1 64.477531, loss kl 1.369821, loss_trans 0.030552, loss flux 49.241112, loss flux t1 50.069138, prod bhp loss 8.193947, prod bhp loss t1 18.368660\n",
      "Epoch 3/20, Batch 1201/1500, Loss 139.309601, Loss rec 79.151016, loss rec t1 170.251648, loss kl 2.063995, loss_trans 0.064528, loss flux 50.871582, loss flux t1 53.239441, prod bhp loss 9.222468, prod bhp loss t1 27.620462\n",
      "Epoch 3/20, Batch 1301/1500, Loss 84.187920, Loss rec 33.371819, loss rec t1 50.802391, loss kl 0.430282, loss_trans 0.021021, loss flux 43.140629, loss flux t1 43.946083, prod bhp loss 7.654451, prod bhp loss t1 13.117003\n",
      "Epoch 3/20, Batch 1401/1500, Loss 72.165024, Loss rec 27.479462, loss rec t1 75.027931, loss kl 1.153931, loss_trans 0.064205, loss flux 37.529110, loss flux t1 53.367363, prod bhp loss 7.092237, prod bhp loss t1 17.033920\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 3/20, Train loss 67.435143, Eval loss 109.499542\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 4/20, Batch 1/1500, Loss 74.980652, Loss rec 29.850367, loss rec t1 88.641174, loss kl 0.740572, loss_trans 0.059308, loss flux 33.108826, loss flux t1 49.359539, prod bhp loss 11.962151, prod bhp loss t1 23.886131\n",
      "Epoch 4/20, Batch 101/1500, Loss 56.518482, Loss rec 21.851387, loss rec t1 104.796066, loss kl 0.396799, loss_trans 0.070304, loss flux 30.319759, loss flux t1 45.472137, prod bhp loss 4.277030, prod bhp loss t1 21.618633\n",
      "Epoch 4/20, Batch 201/1500, Loss 73.857742, Loss rec 29.859074, loss rec t1 69.461533, loss kl 0.966116, loss_trans 0.051512, loss flux 37.248344, loss flux t1 52.315601, prod bhp loss 6.698809, prod bhp loss t1 13.590742\n",
      "Epoch 4/20, Batch 301/1500, Loss 90.098038, Loss rec 35.979713, loss rec t1 58.748833, loss kl 0.761025, loss_trans 0.018723, loss flux 44.293274, loss flux t1 46.128075, prod bhp loss 9.806330, prod bhp loss t1 16.971066\n",
      "Epoch 4/20, Batch 401/1500, Loss 75.377113, Loss rec 31.787380, loss rec t1 51.166710, loss kl 0.591562, loss_trans 0.020375, loss flux 38.610641, loss flux t1 40.617569, prod bhp loss 4.958710, prod bhp loss t1 13.343605\n",
      "Epoch 4/20, Batch 501/1500, Loss 96.497787, Loss rec 44.215416, loss rec t1 92.006020, loss kl 0.817975, loss_trans 0.049844, loss flux 43.233681, loss flux t1 45.274963, prod bhp loss 8.998848, prod bhp loss t1 25.744556\n",
      "Epoch 4/20, Batch 601/1500, Loss 118.334846, Loss rec 55.428108, loss rec t1 72.786736, loss kl 1.312718, loss_trans 0.020776, loss flux 45.590870, loss flux t1 46.929638, prod bhp loss 17.295090, prod bhp loss t1 19.961788\n",
      "Epoch 4/20, Batch 701/1500, Loss 83.163750, Loss rec 33.997372, loss rec t1 38.398300, loss kl 0.735144, loss_trans 0.009111, loss flux 40.168110, loss flux t1 41.872097, prod bhp loss 8.989161, prod bhp loss t1 9.799361\n",
      "Epoch 4/20, Batch 801/1500, Loss 83.687752, Loss rec 39.136051, loss rec t1 47.070847, loss kl 0.535500, loss_trans 0.017567, loss flux 36.553047, loss flux t1 38.213696, prod bhp loss 7.981080, prod bhp loss t1 10.627321\n",
      "Epoch 4/20, Batch 901/1500, Loss 88.149689, Loss rec 33.913757, loss rec t1 42.247620, loss kl 1.643084, loss_trans 0.018070, loss flux 46.696533, loss flux t1 47.982185, prod bhp loss 7.521329, prod bhp loss t1 10.466361\n",
      "Epoch 4/20, Batch 1001/1500, Loss 80.131683, Loss rec 36.286026, loss rec t1 41.103317, loss kl 0.353961, loss_trans 0.016316, loss flux 35.700798, loss flux t1 37.494255, prod bhp loss 8.128540, prod bhp loss t1 9.269640\n",
      "Epoch 4/20, Batch 1101/1500, Loss 90.623116, Loss rec 38.094490, loss rec t1 57.048565, loss kl 1.149322, loss_trans 0.026914, loss flux 42.256927, loss flux t1 43.412228, prod bhp loss 10.244783, prod bhp loss t1 16.648371\n",
      "Epoch 4/20, Batch 1201/1500, Loss 90.240753, Loss rec 36.993065, loss rec t1 49.285873, loss kl 1.766168, loss_trans 0.035480, loss flux 42.400860, loss flux t1 43.514736, prod bhp loss 10.811359, prod bhp loss t1 12.130556\n",
      "Epoch 4/20, Batch 1301/1500, Loss 73.073143, Loss rec 27.697090, loss rec t1 34.172127, loss kl 0.360096, loss_trans 0.015862, loss flux 34.495449, loss flux t1 35.377750, prod bhp loss 10.864739, prod bhp loss t1 11.617582\n",
      "Epoch 4/20, Batch 1401/1500, Loss 68.335716, Loss rec 25.703424, loss rec t1 93.535667, loss kl 0.880327, loss_trans 0.066417, loss flux 32.614811, loss flux t1 49.056595, prod bhp loss 9.951063, prod bhp loss t1 26.771601\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 4/20, Train loss 56.294415, Eval loss 100.199738\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 5/20, Batch 1/1500, Loss 51.408466, Loss rec 18.742674, loss rec t1 65.376450, loss kl 0.634903, loss_trans 0.055712, loss flux 27.599077, loss flux t1 43.737339, prod bhp loss 5.011003, prod bhp loss t1 15.314770\n",
      "Epoch 5/20, Batch 101/1500, Loss 47.602718, Loss rec 17.895493, loss rec t1 95.289413, loss kl 0.344597, loss_trans 0.064549, loss flux 26.089548, loss flux t1 41.305046, prod bhp loss 3.553131, prod bhp loss t1 22.040005\n",
      "Epoch 5/20, Batch 201/1500, Loss 78.835861, Loss rec 34.157314, loss rec t1 62.012836, loss kl 0.837438, loss_trans 0.041004, loss flux 32.296284, loss flux t1 47.085644, prod bhp loss 12.341257, prod bhp loss t1 12.928250\n",
      "Epoch 5/20, Batch 301/1500, Loss 97.096336, Loss rec 41.691879, loss rec t1 54.236298, loss kl 0.848940, loss_trans 0.022533, loss flux 40.259014, loss flux t1 42.042789, prod bhp loss 15.122910, prod bhp loss t1 18.494690\n",
      "Epoch 5/20, Batch 401/1500, Loss 78.340935, Loss rec 31.949015, loss rec t1 42.891376, loss kl 0.615847, loss_trans 0.023055, loss flux 35.899532, loss flux t1 37.486042, prod bhp loss 10.469329, prod bhp loss t1 14.151464\n",
      "Epoch 5/20, Batch 501/1500, Loss 84.695168, Loss rec 41.392029, loss rec t1 87.612648, loss kl 0.808377, loss_trans 0.047476, loss flux 38.325493, loss flux t1 40.324261, prod bhp loss 4.930171, prod bhp loss t1 25.450745\n",
      "Epoch 5/20, Batch 601/1500, Loss 99.194916, Loss rec 42.648804, loss rec t1 55.281292, loss kl 1.302351, loss_trans 0.016803, loss flux 41.438652, loss flux t1 42.480801, prod bhp loss 15.090664, prod bhp loss t1 17.813871\n",
      "Epoch 5/20, Batch 701/1500, Loss 70.049873, Loss rec 27.469177, loss rec t1 31.672401, loss kl 0.635456, loss_trans 0.007429, loss flux 35.671490, loss flux t1 36.827049, prod bhp loss 6.901777, prod bhp loss t1 7.908941\n",
      "Epoch 5/20, Batch 801/1500, Loss 59.135094, Loss rec 21.003485, loss rec t1 30.421719, loss kl 0.503672, loss_trans 0.017405, loss flux 31.848841, loss flux t1 33.511547, prod bhp loss 6.265362, prod bhp loss t1 11.510178\n",
      "Epoch 5/20, Batch 901/1500, Loss 71.340172, Loss rec 26.215887, loss rec t1 32.643486, loss kl 1.433146, loss_trans 0.014611, loss flux 40.129463, loss flux t1 41.413673, prod bhp loss 4.980212, prod bhp loss t1 8.670815\n",
      "Epoch 5/20, Batch 1001/1500, Loss 61.033596, Loss rec 23.107349, loss rec t1 30.830139, loss kl 0.314422, loss_trans 0.014706, loss flux 31.948521, loss flux t1 33.536076, prod bhp loss 5.963019, prod bhp loss t1 10.029084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Batch 1101/1500, Loss 73.488335, Loss rec 28.958164, loss rec t1 50.976875, loss kl 1.210084, loss_trans 0.027695, loss flux 38.618668, loss flux t1 39.780762, prod bhp loss 5.883809, prod bhp loss t1 17.941092\n",
      "Epoch 5/20, Batch 1201/1500, Loss 75.075943, Loss rec 29.345175, loss rec t1 63.047771, loss kl 1.806052, loss_trans 0.032393, loss flux 39.779140, loss flux t1 40.624901, prod bhp loss 5.919237, prod bhp loss t1 19.361492\n",
      "Epoch 5/20, Batch 1301/1500, Loss 58.879692, Loss rec 21.510487, loss rec t1 35.602089, loss kl 0.351746, loss_trans 0.016381, loss flux 31.508165, loss flux t1 32.218674, prod bhp loss 5.844657, prod bhp loss t1 14.026278\n",
      "Epoch 5/20, Batch 1401/1500, Loss 61.680401, Loss rec 24.104862, loss rec t1 77.133842, loss kl 0.995898, loss_trans 0.060550, loss flux 29.485481, loss flux t1 45.970722, prod bhp loss 8.029507, prod bhp loss t1 18.379951\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 5/20, Train loss 49.105812, Eval loss 93.977654\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 6/20, Batch 1/1500, Loss 53.033875, Loss rec 19.557434, loss rec t1 67.703827, loss kl 0.709794, loss_trans 0.057703, loss flux 26.602135, loss flux t1 42.376938, prod bhp loss 6.816607, prod bhp loss t1 17.295923\n",
      "Epoch 6/20, Batch 101/1500, Loss 53.355968, Loss rec 19.719345, loss rec t1 102.719528, loss kl 0.354574, loss_trans 0.072002, loss flux 24.699327, loss flux t1 40.586235, prod bhp loss 8.865297, prod bhp loss t1 22.509619\n",
      "Epoch 6/20, Batch 201/1500, Loss 51.988277, Loss rec 18.666164, loss rec t1 58.448891, loss kl 0.832560, loss_trans 0.036861, loss flux 28.954262, loss flux t1 44.729931, prod bhp loss 4.330988, prod bhp loss t1 12.661900\n",
      "Epoch 6/20, Batch 301/1500, Loss 77.283539, Loss rec 30.921463, loss rec t1 50.437172, loss kl 0.708547, loss_trans 0.017306, loss flux 36.518471, loss flux t1 38.303856, prod bhp loss 9.826304, prod bhp loss t1 17.536858\n",
      "Epoch 6/20, Batch 401/1500, Loss 61.523876, Loss rec 24.318699, loss rec t1 34.921288, loss kl 0.550663, loss_trans 0.017631, loss flux 30.708387, loss flux t1 32.650764, prod bhp loss 6.479159, prod bhp loss t1 11.450862\n",
      "Epoch 6/20, Batch 501/1500, Loss 70.640366, Loss rec 28.596970, loss rec t1 92.918388, loss kl 0.723233, loss_trans 0.042382, loss flux 32.347984, loss flux t1 35.016079, prod bhp loss 9.653032, prod bhp loss t1 30.870857\n",
      "Epoch 6/20, Batch 601/1500, Loss 68.225563, Loss rec 24.364204, loss rec t1 35.580727, loss kl 1.308255, loss_trans 0.016309, loss flux 37.833378, loss flux t1 38.780212, prod bhp loss 6.011671, prod bhp loss t1 12.522311\n",
      "Epoch 6/20, Batch 701/1500, Loss 70.414375, Loss rec 29.467548, loss rec t1 37.152771, loss kl 0.623822, loss_trans 0.006044, loss flux 32.072056, loss flux t1 33.415791, prod bhp loss 8.868727, prod bhp loss t1 12.366702\n",
      "Epoch 6/20, Batch 801/1500, Loss 55.539757, Loss rec 19.238131, loss rec t1 27.463955, loss kl 0.458923, loss_trans 0.015344, loss flux 29.634884, loss flux t1 31.153646, prod bhp loss 6.651397, prod bhp loss t1 11.049862\n",
      "Epoch 6/20, Batch 901/1500, Loss 65.620567, Loss rec 22.728680, loss rec t1 28.158449, loss kl 1.440581, loss_trans 0.014644, loss flux 36.483955, loss flux t1 38.020226, prod bhp loss 6.393289, prod bhp loss t1 8.186090\n",
      "Epoch 6/20, Batch 1001/1500, Loss 59.241390, Loss rec 24.282169, loss rec t1 33.869591, loss kl 0.315044, loss_trans 0.014596, loss flux 30.583576, loss flux t1 32.608723, prod bhp loss 4.361049, prod bhp loss t1 10.159808\n",
      "Epoch 6/20, Batch 1101/1500, Loss 65.708336, Loss rec 23.822393, loss rec t1 46.931900, loss kl 1.201241, loss_trans 0.026922, loss flux 34.459579, loss flux t1 35.477142, prod bhp loss 7.399439, prod bhp loss t1 16.451279\n",
      "Epoch 6/20, Batch 1201/1500, Loss 65.854721, Loss rec 24.864767, loss rec t1 51.522949, loss kl 1.887227, loss_trans 0.032612, loss flux 35.072880, loss flux t1 35.596989, prod bhp loss 5.884468, prod bhp loss t1 19.999079\n",
      "Epoch 6/20, Batch 1301/1500, Loss 45.729301, Loss rec 13.699517, loss rec t1 27.942665, loss kl 0.341639, loss_trans 0.013716, loss flux 27.946877, loss flux t1 28.597157, prod bhp loss 4.069191, prod bhp loss t1 12.883673\n",
      "Epoch 6/20, Batch 1401/1500, Loss 55.522850, Loss rec 20.759291, loss rec t1 83.215530, loss kl 0.887755, loss_trans 0.066268, loss flux 26.191231, loss flux t1 42.367386, prod bhp loss 8.506057, prod bhp loss t1 20.127073\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 6/20, Train loss 44.316307, Eval loss 81.333282\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 7/20, Batch 1/1500, Loss 39.439903, Loss rec 13.538048, loss rec t1 60.868591, loss kl 0.600017, loss_trans 0.057795, loss flux 22.449564, loss flux t1 38.773315, prod bhp loss 3.394498, prod bhp loss t1 15.247675\n",
      "Epoch 7/20, Batch 101/1500, Loss 42.923428, Loss rec 15.972221, loss rec t1 96.650444, loss kl 0.349168, loss_trans 0.068155, loss flux 21.765116, loss flux t1 38.374088, prod bhp loss 5.117940, prod bhp loss t1 21.241590\n",
      "Epoch 7/20, Batch 201/1500, Loss 50.735752, Loss rec 18.305214, loss rec t1 63.862831, loss kl 0.849371, loss_trans 0.038716, loss flux 26.634663, loss flux t1 42.964371, prod bhp loss 5.757161, prod bhp loss t1 13.870203\n",
      "Epoch 7/20, Batch 301/1500, Loss 70.868347, Loss rec 26.479641, loss rec t1 53.909962, loss kl 0.670451, loss_trans 0.015504, loss flux 31.930063, loss flux t1 34.040707, prod bhp loss 12.443142, prod bhp loss t1 21.378994\n",
      "Epoch 7/20, Batch 401/1500, Loss 46.407093, Loss rec 16.040001, loss rec t1 32.421825, loss kl 0.516275, loss_trans 0.017644, loss flux 26.886232, loss flux t1 28.963196, prod bhp loss 3.463217, prod bhp loss t1 13.777317\n",
      "Epoch 7/20, Batch 501/1500, Loss 65.481781, Loss rec 27.725239, loss rec t1 79.834282, loss kl 0.717116, loss_trans 0.040561, loss flux 30.047070, loss flux t1 32.611206, prod bhp loss 7.668918, prod bhp loss t1 27.705036\n",
      "Epoch 7/20, Batch 601/1500, Loss 57.909779, Loss rec 19.413776, loss rec t1 32.434334, loss kl 1.281761, loss_trans 0.016072, loss flux 33.726078, loss flux t1 35.194698, prod bhp loss 4.753852, prod bhp loss t1 12.111143\n",
      "Epoch 7/20, Batch 701/1500, Loss 52.355827, Loss rec 19.449829, loss rec t1 24.242481, loss kl 0.600030, loss_trans 0.004736, loss flux 27.988878, loss flux t1 28.933083, prod bhp loss 4.912382, prod bhp loss t1 8.065084\n",
      "Epoch 7/20, Batch 801/1500, Loss 48.500771, Loss rec 17.954288, loss rec t1 33.206894, loss kl 0.464682, loss_trans 0.015131, loss flux 25.597935, loss flux t1 27.430021, prod bhp loss 4.933417, prod bhp loss t1 14.849089\n",
      "Epoch 7/20, Batch 901/1500, Loss 67.381554, Loss rec 24.709112, loss rec t1 24.757584, loss kl 1.320097, loss_trans 0.011864, loss flux 33.292137, loss flux t1 34.101070, prod bhp loss 9.368442, prod bhp loss t1 8.239635\n",
      "Epoch 7/20, Batch 1001/1500, Loss 56.617931, Loss rec 20.975737, loss rec t1 30.077232, loss kl 0.319210, loss_trans 0.012963, loss flux 27.407534, loss flux t1 29.250874, prod bhp loss 8.221700, prod bhp loss t1 8.953357\n",
      "Epoch 7/20, Batch 1101/1500, Loss 57.923889, Loss rec 20.543325, loss rec t1 42.357353, loss kl 1.113543, loss_trans 0.024816, loss flux 31.807751, loss flux t1 32.998066, prod bhp loss 5.548001, prod bhp loss t1 17.155022\n",
      "Epoch 7/20, Batch 1201/1500, Loss 61.795841, Loss rec 23.312378, loss rec t1 35.127247, loss kl 1.664927, loss_trans 0.026004, loss flux 31.974062, loss flux t1 33.325100, prod bhp loss 6.483398, prod bhp loss t1 13.549513\n",
      "Epoch 7/20, Batch 1301/1500, Loss 43.657223, Loss rec 13.106293, loss rec t1 22.183632, loss kl 0.313522, loss_trans 0.012295, loss flux 25.825899, loss flux t1 26.477251, prod bhp loss 4.712739, prod bhp loss t1 11.546059\n",
      "Epoch 7/20, Batch 1401/1500, Loss 49.308025, Loss rec 17.492577, loss rec t1 71.842194, loss kl 0.846032, loss_trans 0.067400, loss flux 24.534077, loss flux t1 40.443134, prod bhp loss 7.213972, prod bhp loss t1 17.241058\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 7/20, Train loss 44.367077, Eval loss 78.382629\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 8/20, Batch 1/1500, Loss 41.522835, Loss rec 14.066032, loss rec t1 62.822559, loss kl 0.560494, loss_trans 0.056811, loss flux 20.626274, loss flux t1 37.484009, prod bhp loss 6.773715, prod bhp loss t1 16.929655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Batch 101/1500, Loss 39.320084, Loss rec 13.972629, loss rec t1 92.861191, loss kl 0.307724, loss_trans 0.062860, loss flux 20.601231, loss flux t1 36.140800, prod bhp loss 4.683363, prod bhp loss t1 22.195560\n",
      "Epoch 8/20, Batch 201/1500, Loss 50.775909, Loss rec 23.319038, loss rec t1 70.410522, loss kl 0.778952, loss_trans 0.049519, loss flux 23.707146, loss flux t1 41.057819, prod bhp loss 3.700208, prod bhp loss t1 13.330336\n",
      "Epoch 8/20, Batch 301/1500, Loss 52.771805, Loss rec 16.908665, loss rec t1 32.410778, loss kl 0.702722, loss_trans 0.015178, loss flux 30.023849, loss flux t1 31.479666, prod bhp loss 5.824113, prod bhp loss t1 15.194958\n",
      "Epoch 8/20, Batch 401/1500, Loss 63.801495, Loss rec 29.286896, loss rec t1 47.881599, loss kl 0.502374, loss_trans 0.014386, loss flux 27.456240, loss flux t1 28.946518, prod bhp loss 7.043976, prod bhp loss t1 13.538947\n",
      "Epoch 8/20, Batch 501/1500, Loss 57.572712, Loss rec 21.764191, loss rec t1 74.737465, loss kl 0.719887, loss_trans 0.037602, loss flux 28.181921, loss flux t1 30.174049, prod bhp loss 7.588995, prod bhp loss t1 28.506849\n",
      "Epoch 8/20, Batch 601/1500, Loss 66.179276, Loss rec 26.416687, loss rec t1 34.650078, loss kl 1.281685, loss_trans 0.013823, loss flux 31.612177, loss flux t1 32.550632, prod bhp loss 8.136585, prod bhp loss t1 12.430462\n",
      "Epoch 8/20, Batch 701/1500, Loss 51.581337, Loss rec 19.158072, loss rec t1 22.742327, loss kl 0.619070, loss_trans 0.005256, loss flux 26.899902, loss flux t1 27.853163, prod bhp loss 5.518106, prod bhp loss t1 8.295376\n",
      "Epoch 8/20, Batch 801/1500, Loss 47.800381, Loss rec 19.501829, loss rec t1 34.612514, loss kl 0.440724, loss_trans 0.015031, loss flux 23.887545, loss flux t1 25.989454, prod bhp loss 4.395977, prod bhp loss t1 13.181580\n",
      "Epoch 8/20, Batch 901/1500, Loss 52.781506, Loss rec 17.081005, loss rec t1 23.885239, loss kl 1.330155, loss_trans 0.010947, loss flux 31.040281, loss flux t1 32.643166, prod bhp loss 4.649272, prod bhp loss t1 7.865621\n",
      "Epoch 8/20, Batch 1001/1500, Loss 55.047974, Loss rec 22.431768, loss rec t1 32.690369, loss kl 0.286710, loss_trans 0.010971, loss flux 26.088865, loss flux t1 28.104092, prod bhp loss 6.516369, prod bhp loss t1 9.622093\n",
      "Epoch 8/20, Batch 1101/1500, Loss 53.838684, Loss rec 20.742241, loss rec t1 39.428566, loss kl 1.149240, loss_trans 0.026153, loss flux 29.205744, loss flux t1 29.936939, prod bhp loss 3.864547, prod bhp loss t1 15.746070\n",
      "Epoch 8/20, Batch 1201/1500, Loss 52.975246, Loss rec 18.839481, loss rec t1 47.108017, loss kl 1.708998, loss_trans 0.024959, loss flux 29.474653, loss flux t1 31.023403, prod bhp loss 4.636152, prod bhp loss t1 18.994019\n",
      "Epoch 8/20, Batch 1301/1500, Loss 41.267281, Loss rec 12.789860, loss rec t1 22.954668, loss kl 0.348323, loss_trans 0.013320, loss flux 24.800865, loss flux t1 25.758165, prod bhp loss 3.663233, prod bhp loss t1 11.163559\n",
      "Epoch 8/20, Batch 1401/1500, Loss 50.930798, Loss rec 20.334469, loss rec t1 68.298210, loss kl 0.911768, loss_trans 0.063915, loss flux 22.619911, loss flux t1 38.897514, prod bhp loss 7.912503, prod bhp loss t1 16.900282\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 8/20, Train loss 39.212063, Eval loss 87.119499\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 9/20, Batch 1/1500, Loss 40.381634, Loss rec 17.258823, loss rec t1 59.838440, loss kl 0.589343, loss_trans 0.057568, loss flux 19.074156, loss flux t1 35.330040, prod bhp loss 3.991088, prod bhp loss t1 13.570482\n",
      "Epoch 9/20, Batch 101/1500, Loss 40.470638, Loss rec 14.049127, loss rec t1 101.885315, loss kl 0.372773, loss_trans 0.069356, loss flux 21.263750, loss flux t1 37.295376, prod bhp loss 5.088404, prod bhp loss t1 23.345573\n",
      "Epoch 9/20, Batch 201/1500, Loss 41.375095, Loss rec 13.972745, loss rec t1 61.819016, loss kl 0.838259, loss_trans 0.043106, loss flux 22.140139, loss flux t1 40.758430, prod bhp loss 5.219105, prod bhp loss t1 12.429474\n",
      "Epoch 9/20, Batch 301/1500, Loss 51.317154, Loss rec 16.373268, loss rec t1 31.488869, loss kl 0.723879, loss_trans 0.015181, loss flux 27.917393, loss flux t1 29.528357, prod bhp loss 7.011311, prod bhp loss t1 15.496492\n",
      "Epoch 9/20, Batch 401/1500, Loss 45.699482, Loss rec 15.878159, loss rec t1 30.820194, loss kl 0.523414, loss_trans 0.016407, loss flux 24.001535, loss flux t1 25.875851, prod bhp loss 5.803383, prod bhp loss t1 13.837845\n",
      "Epoch 9/20, Batch 501/1500, Loss 48.858681, Loss rec 18.269894, loss rec t1 66.493027, loss kl 0.761313, loss_trans 0.044323, loss flux 26.087412, loss flux t1 28.518221, prod bhp loss 4.457049, prod bhp loss t1 26.056355\n",
      "Epoch 9/20, Batch 601/1500, Loss 57.408512, Loss rec 20.693140, loss rec t1 26.732792, loss kl 1.293120, loss_trans 0.013333, loss flux 30.077227, loss flux t1 31.186756, prod bhp loss 6.624811, prod bhp loss t1 10.285114\n",
      "Epoch 9/20, Batch 701/1500, Loss 48.102299, Loss rec 17.427576, loss rec t1 21.464912, loss kl 0.688689, loss_trans 0.006086, loss flux 25.883282, loss flux t1 27.130787, prod bhp loss 4.785355, prod bhp loss t1 7.188555\n",
      "Epoch 9/20, Batch 801/1500, Loss 38.819004, Loss rec 12.517617, loss rec t1 24.788248, loss kl 0.442091, loss_trans 0.017427, loss flux 22.758020, loss flux t1 24.621510, prod bhp loss 3.525939, prod bhp loss t1 12.770042\n",
      "Epoch 9/20, Batch 901/1500, Loss 53.121521, Loss rec 17.857712, loss rec t1 27.228926, loss kl 1.470133, loss_trans 0.012462, loss flux 29.195997, loss flux t1 30.692812, prod bhp loss 6.055348, prod bhp loss t1 10.725504\n",
      "Epoch 9/20, Batch 1001/1500, Loss 42.227242, Loss rec 14.465584, loss rec t1 21.083832, loss kl 0.283659, loss_trans 0.011030, loss flux 23.725590, loss flux t1 25.496243, prod bhp loss 4.025041, prod bhp loss t1 8.399755\n",
      "Epoch 9/20, Batch 1101/1500, Loss 50.757706, Loss rec 17.292204, loss rec t1 37.194927, loss kl 1.167536, loss_trans 0.025592, loss flux 26.516369, loss flux t1 27.270231, prod bhp loss 6.923541, prod bhp loss t1 16.072573\n",
      "Epoch 9/20, Batch 1201/1500, Loss 54.103924, Loss rec 19.292879, loss rec t1 55.946861, loss kl 1.902876, loss_trans 0.031166, loss flux 28.479971, loss flux t1 29.308338, prod bhp loss 6.299907, prod bhp loss t1 23.041374\n",
      "Epoch 9/20, Batch 1301/1500, Loss 37.365261, Loss rec 11.367056, loss rec t1 21.040047, loss kl 0.360534, loss_trans 0.011502, loss flux 23.292215, loss flux t1 24.608019, prod bhp loss 2.694488, prod bhp loss t1 10.688164\n",
      "Epoch 9/20, Batch 1401/1500, Loss 36.937260, Loss rec 11.515929, loss rec t1 64.091103, loss kl 0.878578, loss_trans 0.056142, loss flux 21.024103, loss flux t1 36.455933, prod bhp loss 4.341086, prod bhp loss t1 19.221708\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 9/20, Train loss 32.592541, Eval loss 77.225845\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 10/20, Batch 1/1500, Loss 32.844204, Loss rec 11.271434, loss rec t1 58.094276, loss kl 0.628495, loss_trans 0.060495, loss flux 18.827902, loss flux t1 35.196461, prod bhp loss 2.684370, prod bhp loss t1 15.241797\n",
      "Epoch 10/20, Batch 101/1500, Loss 35.539116, Loss rec 11.593044, loss rec t1 81.286049, loss kl 0.348346, loss_trans 0.073002, loss flux 18.804199, loss flux t1 34.416550, prod bhp loss 5.068869, prod bhp loss t1 20.350151\n",
      "Epoch 10/20, Batch 201/1500, Loss 35.264568, Loss rec 11.288414, loss rec t1 54.126865, loss kl 0.808410, loss_trans 0.038379, loss flux 20.525040, loss flux t1 38.435768, prod bhp loss 3.412734, prod bhp loss t1 13.230145\n",
      "Epoch 10/20, Batch 301/1500, Loss 43.259197, Loss rec 12.449862, loss rec t1 28.349403, loss kl 0.714971, loss_trans 0.015798, loss flux 25.725595, loss flux t1 27.774729, prod bhp loss 5.067945, prod bhp loss t1 14.511927\n",
      "Epoch 10/20, Batch 401/1500, Loss 44.932892, Loss rec 16.235855, loss rec t1 28.260426, loss kl 0.533329, loss_trans 0.016403, loss flux 23.829540, loss flux t1 25.904552, prod bhp loss 4.851092, prod bhp loss t1 11.765967\n",
      "Epoch 10/20, Batch 501/1500, Loss 48.152744, Loss rec 18.613899, loss rec t1 61.911407, loss kl 0.753396, loss_trans 0.043926, loss flux 24.745073, loss flux t1 27.095512, prod bhp loss 4.749849, prod bhp loss t1 24.111317\n",
      "Epoch 10/20, Batch 601/1500, Loss 53.861794, Loss rec 20.242180, loss rec t1 24.753731, loss kl 1.282625, loss_trans 0.013459, loss flux 27.674006, loss flux t1 28.506836, prod bhp loss 5.932152, prod bhp loss t1 9.053860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Batch 701/1500, Loss 43.583588, Loss rec 15.938660, loss rec t1 20.009575, loss kl 0.677735, loss_trans 0.007046, loss flux 23.795338, loss flux t1 25.291010, prod bhp loss 3.842543, prod bhp loss t1 7.353870\n",
      "Epoch 10/20, Batch 801/1500, Loss 36.821091, Loss rec 11.931694, loss rec t1 26.440323, loss kl 0.451307, loss_trans 0.020315, loss flux 21.748516, loss flux t1 23.743818, prod bhp loss 3.120565, prod bhp loss t1 13.807207\n",
      "Epoch 10/20, Batch 901/1500, Loss 46.482677, Loss rec 14.418891, loss rec t1 25.179073, loss kl 1.411686, loss_trans 0.014353, loss flux 27.189486, loss flux t1 28.615213, prod bhp loss 4.859947, prod bhp loss t1 9.930984\n",
      "Epoch 10/20, Batch 1001/1500, Loss 41.371861, Loss rec 14.274059, loss rec t1 30.917412, loss kl 0.281029, loss_trans 0.014850, loss flux 22.767321, loss flux t1 25.473013, prod bhp loss 4.315629, prod bhp loss t1 15.002759\n",
      "Epoch 10/20, Batch 1101/1500, Loss 61.606773, Loss rec 24.161793, loss rec t1 43.460632, loss kl 1.239890, loss_trans 0.026437, loss flux 25.090170, loss flux t1 26.277033, prod bhp loss 12.328377, prod bhp loss t1 17.397343\n",
      "Epoch 10/20, Batch 1201/1500, Loss 52.764130, Loss rec 20.620708, loss rec t1 38.100677, loss kl 1.889783, loss_trans 0.019564, loss flux 27.940313, loss flux t1 28.035772, prod bhp loss 4.183544, prod bhp loss t1 15.666077\n",
      "Epoch 10/20, Batch 1301/1500, Loss 42.093887, Loss rec 13.075719, loss rec t1 30.694475, loss kl 0.384195, loss_trans 0.014009, loss flux 22.235760, loss flux t1 23.924225, prod bhp loss 6.768401, prod bhp loss t1 14.861782\n",
      "Epoch 10/20, Batch 1401/1500, Loss 42.772499, Loss rec 16.315542, loss rec t1 73.570625, loss kl 0.929608, loss_trans 0.060469, loss flux 21.096928, loss flux t1 35.547855, prod bhp loss 5.299560, prod bhp loss t1 19.637455\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 10/20, Train loss 33.103195, Eval loss 84.578461\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 11/20, Batch 1/1500, Loss 32.709484, Loss rec 12.007433, loss rec t1 65.264099, loss kl 0.625542, loss_trans 0.063629, loss flux 17.121998, loss flux t1 33.065449, prod bhp loss 3.516425, prod bhp loss t1 17.489542\n",
      "Epoch 11/20, Batch 101/1500, Loss 31.059544, Loss rec 10.039005, loss rec t1 74.581863, loss kl 0.318104, loss_trans 0.048287, loss flux 16.845165, loss flux t1 32.763912, prod bhp loss 4.127086, prod bhp loss t1 21.868038\n",
      "Epoch 11/20, Batch 201/1500, Loss 36.011539, Loss rec 12.857847, loss rec t1 51.205585, loss kl 0.823042, loss_trans 0.039825, loss flux 19.562841, loss flux t1 36.334553, prod bhp loss 3.551025, prod bhp loss t1 12.372494\n",
      "Epoch 11/20, Batch 301/1500, Loss 44.661446, Loss rec 16.121288, loss rec t1 31.564531, loss kl 0.724687, loss_trans 0.015615, loss flux 24.401915, loss flux t1 25.816502, prod bhp loss 4.122630, prod bhp loss t1 13.751074\n",
      "Epoch 11/20, Batch 401/1500, Loss 34.380749, Loss rec 10.731425, loss rec t1 24.365633, loss kl 0.521674, loss_trans 0.016724, loss flux 21.039341, loss flux t1 22.701361, prod bhp loss 2.593256, prod bhp loss t1 12.483261\n",
      "Epoch 11/20, Batch 501/1500, Loss 43.017960, Loss rec 14.609276, loss rec t1 60.468773, loss kl 0.751044, loss_trans 0.039992, loss flux 23.212511, loss flux t1 25.388172, prod bhp loss 5.156180, prod bhp loss t1 25.317795\n",
      "Epoch 11/20, Batch 601/1500, Loss 47.599426, Loss rec 16.961979, loss rec t1 21.926908, loss kl 1.272434, loss_trans 0.013369, loss flux 25.434793, loss flux t1 26.543333, prod bhp loss 5.189282, prod bhp loss t1 9.300740\n",
      "Epoch 11/20, Batch 701/1500, Loss 40.554234, Loss rec 13.264656, loss rec t1 14.401943, loss kl 0.643729, loss_trans 0.007451, loss flux 22.236197, loss flux t1 22.974247, prod bhp loss 5.045931, prod bhp loss t1 5.433840\n",
      "Epoch 11/20, Batch 801/1500, Loss 37.540691, Loss rec 13.370362, loss rec t1 26.504211, loss kl 0.464752, loss_trans 0.022314, loss flux 21.066021, loss flux t1 22.897419, prod bhp loss 3.081998, prod bhp loss t1 13.766589\n",
      "Epoch 11/20, Batch 901/1500, Loss 44.603527, Loss rec 13.643425, loss rec t1 18.406677, loss kl 1.463617, loss_trans 0.011938, loss flux 26.039204, loss flux t1 27.298901, prod bhp loss 4.908964, prod bhp loss t1 7.261255\n",
      "Epoch 11/20, Batch 1001/1500, Loss 41.832684, Loss rec 14.144016, loss rec t1 17.136883, loss kl 0.322958, loss_trans 0.011175, loss flux 21.375607, loss flux t1 22.460438, prod bhp loss 6.301888, prod bhp loss t1 6.984009\n",
      "Epoch 11/20, Batch 1101/1500, Loss 42.725826, Loss rec 14.595459, loss rec t1 35.391106, loss kl 1.164372, loss_trans 0.026538, loss flux 24.329813, loss flux t1 25.283888, prod bhp loss 3.774017, prod bhp loss t1 16.624945\n",
      "Epoch 11/20, Batch 1201/1500, Loss 45.123302, Loss rec 14.370979, loss rec t1 20.425562, loss kl 1.774243, loss_trans 0.016336, loss flux 25.363136, loss flux t1 25.616880, prod bhp loss 5.372854, prod bhp loss t1 9.131215\n",
      "Epoch 11/20, Batch 1301/1500, Loss 34.284672, Loss rec 9.696595, loss rec t1 23.645454, loss kl 0.370998, loss_trans 0.013690, loss flux 20.551430, loss flux t1 22.125856, prod bhp loss 4.022958, prod bhp loss t1 12.713157\n",
      "Epoch 11/20, Batch 1401/1500, Loss 31.099659, Loss rec 9.393995, loss rec t1 68.211960, loss kl 0.846521, loss_trans 0.058349, loss flux 18.978498, loss flux t1 33.888676, prod bhp loss 2.668818, prod bhp loss t1 19.225214\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 11/20, Train loss 34.402847, Eval loss 67.893578\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 12/20, Batch 1/1500, Loss 36.454510, Loss rec 12.360758, loss rec t1 48.675983, loss kl 0.587048, loss_trans 0.044258, loss flux 16.661446, loss flux t1 31.043753, prod bhp loss 7.388049, prod bhp loss t1 16.865877\n",
      "Epoch 12/20, Batch 101/1500, Loss 36.279636, Loss rec 13.832084, loss rec t1 63.012566, loss kl 0.328767, loss_trans 0.040766, loss flux 16.281036, loss flux t1 30.468178, prod bhp loss 6.125747, prod bhp loss t1 20.707417\n",
      "Epoch 12/20, Batch 201/1500, Loss 37.764385, Loss rec 12.612416, loss rec t1 47.606956, loss kl 0.847129, loss_trans 0.034924, loss flux 18.648129, loss flux t1 34.462517, prod bhp loss 6.468920, prod bhp loss t1 13.531885\n",
      "Epoch 12/20, Batch 301/1500, Loss 50.040756, Loss rec 22.497192, loss rec t1 41.732536, loss kl 0.753497, loss_trans 0.017898, loss flux 23.030930, loss flux t1 24.845087, prod bhp loss 4.494737, prod bhp loss t1 15.169072\n",
      "Epoch 12/20, Batch 401/1500, Loss 35.991741, Loss rec 11.669978, loss rec t1 24.310707, loss kl 0.534556, loss_trans 0.015738, loss flux 20.361856, loss flux t1 22.327686, prod bhp loss 3.944168, prod bhp loss t1 11.474060\n",
      "Epoch 12/20, Batch 501/1500, Loss 45.052643, Loss rec 17.272356, loss rec t1 58.110035, loss kl 0.774918, loss_trans 0.041678, loss flux 22.338274, loss flux t1 24.657553, prod bhp loss 5.400332, prod bhp loss t1 23.483540\n",
      "Epoch 12/20, Batch 601/1500, Loss 46.199566, Loss rec 16.110996, loss rec t1 26.723515, loss kl 1.264319, loss_trans 0.011249, loss flux 24.286760, loss flux t1 25.731676, prod bhp loss 5.790560, prod bhp loss t1 11.770709\n",
      "Epoch 12/20, Batch 701/1500, Loss 40.507019, Loss rec 13.463492, loss rec t1 15.884707, loss kl 0.667569, loss_trans 0.009060, loss flux 22.106592, loss flux t1 23.063911, prod bhp loss 4.927876, prod bhp loss t1 6.566295\n",
      "Epoch 12/20, Batch 801/1500, Loss 34.343906, Loss rec 10.474809, loss rec t1 30.318840, loss kl 0.482054, loss_trans 0.024272, loss flux 19.418570, loss flux t1 21.932550, prod bhp loss 4.426255, prod bhp loss t1 16.452082\n",
      "Epoch 12/20, Batch 901/1500, Loss 50.467232, Loss rec 16.818914, loss rec t1 27.160995, loss kl 1.528687, loss_trans 0.012908, loss flux 25.734285, loss flux t1 26.327829, prod bhp loss 7.901123, prod bhp loss t1 12.845736\n",
      "Epoch 12/20, Batch 1001/1500, Loss 36.629063, Loss rec 11.919112, loss rec t1 21.662788, loss kl 0.300388, loss_trans 0.008717, loss flux 21.328020, loss flux t1 23.255423, prod bhp loss 3.373213, prod bhp loss t1 9.025838\n",
      "Epoch 12/20, Batch 1101/1500, Loss 42.286694, Loss rec 13.893658, loss rec t1 33.830528, loss kl 1.225939, loss_trans 0.024904, loss flux 23.051489, loss flux t1 24.329742, prod bhp loss 5.316647, prod bhp loss t1 15.626961\n",
      "Epoch 12/20, Batch 1201/1500, Loss 39.169819, Loss rec 12.497744, loss rec t1 22.985495, loss kl 1.886793, loss_trans 0.016862, loss flux 23.638771, loss flux t1 24.180666, prod bhp loss 3.016447, prod bhp loss t1 11.557803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Batch 1301/1500, Loss 38.634052, Loss rec 14.498227, loss rec t1 24.007837, loss kl 0.355383, loss_trans 0.012748, loss flux 19.863861, loss flux t1 21.430305, prod bhp loss 4.259217, prod bhp loss t1 9.236208\n",
      "Epoch 12/20, Batch 1401/1500, Loss 33.453888, Loss rec 9.778265, loss rec t1 65.110550, loss kl 0.883693, loss_trans 0.055392, loss flux 18.585289, loss flux t1 34.231232, prod bhp loss 5.034940, prod bhp loss t1 18.295868\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 12/20, Train loss 26.364985, Eval loss 67.427391\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 13/20, Batch 1/1500, Loss 28.386251, Loss rec 9.461993, loss rec t1 46.432938, loss kl 0.693724, loss_trans 0.046725, loss flux 16.254896, loss flux t1 29.622316, prod bhp loss 2.622639, prod bhp loss t1 15.296923\n",
      "Epoch 13/20, Batch 101/1500, Loss 28.866442, Loss rec 10.129685, loss rec t1 40.151329, loss kl 0.349053, loss_trans 0.027138, loss flux 16.100250, loss flux t1 28.616247, prod bhp loss 2.609369, prod bhp loss t1 15.743257\n",
      "Epoch 13/20, Batch 201/1500, Loss 31.467731, Loss rec 9.220441, loss rec t1 37.184166, loss kl 0.842919, loss_trans 0.033215, loss flux 18.509665, loss flux t1 31.767477, prod bhp loss 3.704412, prod bhp loss t1 13.545125\n",
      "Epoch 13/20, Batch 301/1500, Loss 38.191349, Loss rec 11.110170, loss rec t1 26.338326, loss kl 0.784310, loss_trans 0.018536, loss flux 21.877291, loss flux t1 23.868349, prod bhp loss 5.185350, prod bhp loss t1 13.399367\n",
      "Epoch 13/20, Batch 401/1500, Loss 41.724857, Loss rec 13.615913, loss rec t1 24.627872, loss kl 0.595580, loss_trans 0.019562, loss flux 21.884880, loss flux t1 23.580568, prod bhp loss 6.204501, prod bhp loss t1 12.150076\n",
      "Epoch 13/20, Batch 501/1500, Loss 41.387745, Loss rec 14.216721, loss rec t1 65.332840, loss kl 0.822610, loss_trans 0.044446, loss flux 21.228283, loss flux t1 23.159678, prod bhp loss 5.898297, prod bhp loss t1 26.447584\n",
      "Epoch 13/20, Batch 601/1500, Loss 50.839222, Loss rec 19.605888, loss rec t1 23.203072, loss kl 1.454166, loss_trans 0.012492, loss flux 24.512955, loss flux t1 25.169228, prod bhp loss 6.707885, prod bhp loss t1 8.278914\n",
      "Epoch 13/20, Batch 701/1500, Loss 35.806530, Loss rec 11.058846, loss rec t1 14.812277, loss kl 0.670558, loss_trans 0.008201, loss flux 21.176353, loss flux t1 22.102423, prod bhp loss 3.563130, prod bhp loss t1 6.608459\n",
      "Epoch 13/20, Batch 801/1500, Loss 32.014847, Loss rec 9.562553, loss rec t1 24.983570, loss kl 0.487152, loss_trans 0.023199, loss flux 19.577267, loss flux t1 21.626055, prod bhp loss 2.851825, prod bhp loss t1 13.431950\n",
      "Epoch 13/20, Batch 901/1500, Loss 45.757336, Loss rec 15.129496, loss rec t1 25.316776, loss kl 1.576063, loss_trans 0.010881, loss flux 24.068605, loss flux t1 25.545794, prod bhp loss 6.548355, prod bhp loss t1 10.587334\n",
      "Epoch 13/20, Batch 1001/1500, Loss 32.562344, Loss rec 10.516898, loss rec t1 18.017267, loss kl 0.318253, loss_trans 0.009893, loss flux 19.489738, loss flux t1 21.085424, prod bhp loss 2.545815, prod bhp loss t1 8.893367\n",
      "Epoch 13/20, Batch 1101/1500, Loss 37.865536, Loss rec 11.673206, loss rec t1 26.361834, loss kl 1.196887, loss_trans 0.022239, loss flux 22.440176, loss flux t1 23.389982, prod bhp loss 3.729916, prod bhp loss t1 13.572015\n",
      "Epoch 13/20, Batch 1201/1500, Loss 44.632187, Loss rec 14.652353, loss rec t1 14.953407, loss kl 1.860312, loss_trans 0.015289, loss flux 23.387583, loss flux t1 23.247185, prod bhp loss 6.576962, prod bhp loss t1 6.827844\n",
      "Epoch 13/20, Batch 1301/1500, Loss 34.481544, Loss rec 10.987827, loss rec t1 19.519840, loss kl 0.369958, loss_trans 0.012157, loss flux 19.415539, loss flux t1 20.665668, prod bhp loss 4.066020, prod bhp loss t1 10.575853\n",
      "Epoch 13/20, Batch 1401/1500, Loss 37.065018, Loss rec 12.902272, loss rec t1 89.863632, loss kl 0.976355, loss_trans 0.072916, loss flux 17.927618, loss flux t1 32.940186, prod bhp loss 6.162214, prod bhp loss t1 21.343046\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 13/20, Train loss 24.726126, Eval loss 66.149269\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 14/20, Batch 1/1500, Loss 26.148577, Loss rec 7.447495, loss rec t1 40.407497, loss kl 0.718531, loss_trans 0.040050, loss flux 15.590487, loss flux t1 28.427319, prod bhp loss 3.070544, prod bhp loss t1 15.352382\n",
      "Epoch 14/20, Batch 101/1500, Loss 32.861927, Loss rec 12.285122, loss rec t1 35.030945, loss kl 0.367259, loss_trans 0.023964, loss flux 15.622128, loss flux t1 26.123278, prod bhp loss 4.930715, prod bhp loss t1 12.644786\n",
      "Epoch 14/20, Batch 201/1500, Loss 28.308094, Loss rec 8.343370, loss rec t1 37.472897, loss kl 0.894964, loss_trans 0.034474, loss flux 17.046013, loss flux t1 30.280714, prod bhp loss 2.884236, prod bhp loss t1 15.871358\n",
      "Epoch 14/20, Batch 301/1500, Loss 36.378407, Loss rec 11.226561, loss rec t1 23.839960, loss kl 0.786496, loss_trans 0.017765, loss flux 21.616755, loss flux t1 22.929564, prod bhp loss 3.517325, prod bhp loss t1 12.657159\n",
      "Epoch 14/20, Batch 401/1500, Loss 35.579739, Loss rec 11.382055, loss rec t1 20.917656, loss kl 0.567442, loss_trans 0.017804, loss flux 19.252296, loss flux t1 21.211267, prod bhp loss 4.927584, prod bhp loss t1 10.817476\n",
      "Epoch 14/20, Batch 501/1500, Loss 33.604916, Loss rec 10.627070, loss rec t1 53.527126, loss kl 0.801314, loss_trans 0.042829, loss flux 19.826118, loss flux t1 21.979210, prod bhp loss 3.108898, prod bhp loss t1 24.472740\n",
      "Epoch 14/20, Batch 601/1500, Loss 44.300560, Loss rec 14.052565, loss rec t1 15.041937, loss kl 1.453854, loss_trans 0.013526, loss flux 22.677443, loss flux t1 23.771553, prod bhp loss 7.557026, prod bhp loss t1 7.528665\n",
      "Epoch 14/20, Batch 701/1500, Loss 31.937763, Loss rec 9.731632, loss rec t1 14.274496, loss kl 0.666800, loss_trans 0.007943, loss flux 19.430965, loss flux t1 20.878595, prod bhp loss 2.767221, prod bhp loss t1 6.202127\n",
      "Epoch 14/20, Batch 801/1500, Loss 33.432678, Loss rec 10.506642, loss rec t1 29.752483, loss kl 0.506407, loss_trans 0.023813, loss flux 18.472210, loss flux t1 20.753584, prod bhp loss 4.430017, prod bhp loss t1 16.805557\n",
      "Epoch 14/20, Batch 901/1500, Loss 38.458839, Loss rec 11.448495, loss rec t1 18.255249, loss kl 1.666586, loss_trans 0.012343, loss flux 23.389193, loss flux t1 25.146906, prod bhp loss 3.608805, prod bhp loss t1 6.826479\n",
      "Epoch 14/20, Batch 1001/1500, Loss 32.798130, Loss rec 9.613346, loss rec t1 18.474968, loss kl 0.332673, loss_trans 0.009605, loss flux 19.408754, loss flux t1 20.288790, prod bhp loss 3.766424, prod bhp loss t1 9.532993\n",
      "Epoch 14/20, Batch 1101/1500, Loss 37.374889, Loss rec 11.863048, loss rec t1 27.656349, loss kl 1.245863, loss_trans 0.022720, loss flux 21.010887, loss flux t1 22.318607, prod bhp loss 4.478234, prod bhp loss t1 14.237675\n",
      "Epoch 14/20, Batch 1201/1500, Loss 39.188278, Loss rec 12.343558, loss rec t1 26.836353, loss kl 2.032396, loss_trans 0.015282, loss flux 22.506838, loss flux t1 23.245974, prod bhp loss 4.322601, prod bhp loss t1 13.026441\n",
      "Epoch 14/20, Batch 1301/1500, Loss 32.520519, Loss rec 9.602081, loss rec t1 19.239285, loss kl 0.401552, loss_trans 0.011878, loss flux 18.497665, loss flux t1 20.156099, prod bhp loss 4.408893, prod bhp loss t1 10.668386\n",
      "Epoch 14/20, Batch 1401/1500, Loss 30.420845, Loss rec 9.296675, loss rec t1 31.279095, loss kl 1.001901, loss_trans 0.034259, loss flux 17.037554, loss flux t1 29.212614, prod bhp loss 4.052358, prod bhp loss t1 10.915172\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 14/20, Train loss 24.456406, Eval loss 66.721359\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 15/20, Batch 1/1500, Loss 24.042965, Loss rec 7.050522, loss rec t1 42.228371, loss kl 0.718391, loss_trans 0.049266, loss flux 14.587601, loss flux t1 26.815895, prod bhp loss 2.355576, prod bhp loss t1 15.120214\n",
      "Epoch 15/20, Batch 101/1500, Loss 31.655323, Loss rec 10.262613, loss rec t1 23.636642, loss kl 0.382302, loss_trans 0.020183, loss flux 15.318955, loss flux t1 24.243992, prod bhp loss 6.053571, prod bhp loss t1 9.129173\n",
      "Epoch 15/20, Batch 201/1500, Loss 26.934748, Loss rec 7.825118, loss rec t1 31.302866, loss kl 0.910493, loss_trans 0.030772, loss flux 16.472847, loss flux t1 29.305815, prod bhp loss 2.606011, prod bhp loss t1 12.479677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Batch 301/1500, Loss 37.096344, Loss rec 11.540946, loss rec t1 31.350548, loss kl 0.804325, loss_trans 0.023568, loss flux 20.526648, loss flux t1 23.087677, prod bhp loss 5.005183, prod bhp loss t1 15.189991\n",
      "Epoch 15/20, Batch 401/1500, Loss 29.466064, Loss rec 8.770996, loss rec t1 17.045967, loss kl 0.563765, loss_trans 0.015346, loss flux 17.832813, loss flux t1 20.074335, prod bhp loss 2.846910, prod bhp loss t1 10.201005\n",
      "Epoch 15/20, Batch 501/1500, Loss 40.345398, Loss rec 16.898621, loss rec t1 48.211502, loss kl 0.848844, loss_trans 0.039446, loss flux 19.501198, loss flux t1 21.212622, prod bhp loss 3.906133, prod bhp loss t1 20.724104\n",
      "Epoch 15/20, Batch 601/1500, Loss 50.018139, Loss rec 21.421396, loss rec t1 23.216169, loss kl 1.505534, loss_trans 0.011007, loss flux 21.581827, loss flux t1 22.612822, prod bhp loss 7.003912, prod bhp loss t1 7.541952\n",
      "Epoch 15/20, Batch 701/1500, Loss 35.246037, Loss rec 11.677794, loss rec t1 17.415852, loss kl 0.742031, loss_trans 0.011140, loss flux 19.984739, loss flux t1 21.084352, prod bhp loss 3.572366, prod bhp loss t1 7.916843\n",
      "Epoch 15/20, Batch 801/1500, Loss 30.341154, Loss rec 8.712868, loss rec t1 29.412823, loss kl 0.506370, loss_trans 0.027460, loss flux 17.831635, loss flux t1 20.089226, prod bhp loss 3.769192, prod bhp loss t1 16.255594\n",
      "Epoch 15/20, Batch 901/1500, Loss 35.103554, Loss rec 9.401087, loss rec t1 14.619793, loss kl 1.642564, loss_trans 0.010090, loss flux 22.462196, loss flux t1 23.776056, prod bhp loss 3.230179, prod bhp loss t1 6.136869\n",
      "Epoch 15/20, Batch 1001/1500, Loss 44.157272, Loss rec 16.302029, loss rec t1 33.400654, loss kl 0.321126, loss_trans 0.011351, loss flux 18.799347, loss flux t1 20.549110, prod bhp loss 9.044545, prod bhp loss t1 14.888718\n",
      "Epoch 15/20, Batch 1101/1500, Loss 34.285194, Loss rec 11.183153, loss rec t1 21.762905, loss kl 1.285425, loss_trans 0.018732, loss flux 20.370783, loss flux t1 21.537003, prod bhp loss 2.712527, prod bhp loss t1 11.499968\n",
      "Epoch 15/20, Batch 1201/1500, Loss 38.007637, Loss rec 12.173704, loss rec t1 17.026791, loss kl 2.012256, loss_trans 0.015069, loss flux 21.784243, loss flux t1 22.445330, prod bhp loss 4.034624, prod bhp loss t1 8.719315\n",
      "Epoch 15/20, Batch 1301/1500, Loss 27.872944, Loss rec 7.119397, loss rec t1 17.723976, loss kl 0.391916, loss_trans 0.011099, loss flux 18.049109, loss flux t1 19.702280, prod bhp loss 2.693341, prod bhp loss t1 10.675866\n",
      "Epoch 15/20, Batch 1401/1500, Loss 25.927614, Loss rec 7.034246, loss rec t1 26.662102, loss kl 0.974084, loss_trans 0.028297, loss flux 16.109329, loss flux t1 27.220377, prod bhp loss 2.755741, prod bhp loss t1 11.434490\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 15/20, Train loss 26.017801, Eval loss 62.609203\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 16/20, Batch 1/1500, Loss 25.331432, Loss rec 7.331463, loss rec t1 26.290527, loss kl 0.707334, loss_trans 0.028982, loss flux 14.363510, loss flux t1 25.066944, prod bhp loss 3.607477, prod bhp loss t1 10.413493\n",
      "Epoch 16/20, Batch 101/1500, Loss 28.030796, Loss rec 9.270871, loss rec t1 24.831196, loss kl 0.391470, loss_trans 0.019008, loss flux 14.011852, loss flux t1 23.816227, prod bhp loss 4.729064, prod bhp loss t1 9.491338\n",
      "Epoch 16/20, Batch 201/1500, Loss 26.318739, Loss rec 7.735236, loss rec t1 29.321331, loss kl 0.943697, loss_trans 0.027397, loss flux 15.896124, loss flux t1 26.763899, prod bhp loss 2.659983, prod bhp loss t1 12.361736\n",
      "Epoch 16/20, Batch 301/1500, Loss 36.191196, Loss rec 10.774476, loss rec t1 27.196465, loss kl 0.877368, loss_trans 0.021472, loss flux 19.929176, loss flux t1 21.871752, prod bhp loss 5.466074, prod bhp loss t1 13.447541\n",
      "Epoch 16/20, Batch 401/1500, Loss 38.581417, Loss rec 14.319398, loss rec t1 19.516891, loss kl 0.614305, loss_trans 0.016757, loss flux 18.549110, loss flux t1 20.421797, prod bhp loss 5.696150, prod bhp loss t1 9.792952\n",
      "Epoch 16/20, Batch 501/1500, Loss 36.724548, Loss rec 12.318504, loss rec t1 39.152252, loss kl 0.916777, loss_trans 0.041160, loss flux 18.761066, loss flux t1 20.799091, prod bhp loss 5.603819, prod bhp loss t1 19.039680\n",
      "Epoch 16/20, Batch 601/1500, Loss 45.844131, Loss rec 16.373280, loss rec t1 29.216150, loss kl 1.479522, loss_trans 0.016369, loss flux 21.527954, loss flux t1 22.391935, prod bhp loss 7.926528, prod bhp loss t1 13.917918\n",
      "Epoch 16/20, Batch 701/1500, Loss 32.928242, Loss rec 10.144485, loss rec t1 19.969540, loss kl 0.752099, loss_trans 0.009722, loss flux 19.012896, loss flux t1 20.537626, prod bhp loss 3.761138, prod bhp loss t1 9.855017\n",
      "Epoch 16/20, Batch 801/1500, Loss 31.876892, Loss rec 9.586756, loss rec t1 27.714933, loss kl 0.566249, loss_trans 0.025147, loss flux 17.278555, loss flux t1 19.731039, prod bhp loss 4.986434, prod bhp loss t1 14.374580\n",
      "Epoch 16/20, Batch 901/1500, Loss 34.684029, Loss rec 10.394729, loss rec t1 14.152824, loss kl 1.713422, loss_trans 0.008209, loss flux 21.718470, loss flux t1 23.330162, prod bhp loss 2.562624, prod bhp loss t1 4.714818\n",
      "Epoch 16/20, Batch 1001/1500, Loss 41.793831, Loss rec 14.708512, loss rec t1 31.952076, loss kl 0.320844, loss_trans 0.012182, loss flux 18.524837, loss flux t1 20.650375, prod bhp loss 8.548302, prod bhp loss t1 15.629543\n",
      "Epoch 16/20, Batch 1101/1500, Loss 46.900101, Loss rec 18.325823, loss rec t1 23.913290, loss kl 1.301024, loss_trans 0.016619, loss flux 20.432659, loss flux t1 21.529184, prod bhp loss 8.124995, prod bhp loss t1 10.417309\n",
      "Epoch 16/20, Batch 1201/1500, Loss 45.303024, Loss rec 16.580528, loss rec t1 38.121933, loss kl 2.244435, loss_trans 0.022626, loss flux 22.051977, loss flux t1 22.877041, prod bhp loss 6.647891, prod bhp loss t1 17.863007\n",
      "Epoch 16/20, Batch 1301/1500, Loss 28.233234, Loss rec 7.185151, loss rec t1 19.706631, loss kl 0.415703, loss_trans 0.010579, loss flux 17.670456, loss flux t1 20.000248, prod bhp loss 3.367050, prod bhp loss t1 11.882154\n",
      "Epoch 16/20, Batch 1401/1500, Loss 29.225985, Loss rec 9.740152, loss rec t1 74.205635, loss kl 1.044708, loss_trans 0.073783, loss flux 16.759542, loss flux t1 28.694754, prod bhp loss 2.652505, prod bhp loss t1 19.582882\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 16/20, Train loss 22.382868, Eval loss 64.320145\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 17/20, Batch 1/1500, Loss 21.883133, Loss rec 6.338545, loss rec t1 26.053446, loss kl 0.770379, loss_trans 0.030602, loss flux 13.369475, loss flux t1 23.874193, prod bhp loss 2.144513, prod bhp loss t1 11.468503\n",
      "Epoch 17/20, Batch 101/1500, Loss 24.075560, Loss rec 7.817074, loss rec t1 22.602209, loss kl 0.382477, loss_trans 0.016993, loss flux 13.255217, loss flux t1 21.819811, prod bhp loss 2.986277, prod bhp loss t1 10.037424\n",
      "Epoch 17/20, Batch 201/1500, Loss 24.112736, Loss rec 6.609293, loss rec t1 23.825661, loss kl 0.954918, loss_trans 0.026528, loss flux 15.358090, loss flux t1 25.898005, prod bhp loss 2.118825, prod bhp loss t1 11.652950\n",
      "Epoch 17/20, Batch 301/1500, Loss 30.087353, Loss rec 7.636732, loss rec t1 21.732752, loss kl 0.841789, loss_trans 0.017778, loss flux 19.678310, loss flux t1 22.251886, prod bhp loss 2.754532, prod bhp loss t1 12.254511\n",
      "Epoch 17/20, Batch 401/1500, Loss 38.055046, Loss rec 14.249022, loss rec t1 20.879776, loss kl 0.636127, loss_trans 0.020271, loss flux 18.704369, loss flux t1 20.448612, prod bhp loss 5.081381, prod bhp loss t1 11.047734\n",
      "Epoch 17/20, Batch 501/1500, Loss 34.676956, Loss rec 11.389927, loss rec t1 48.511501, loss kl 0.911083, loss_trans 0.041717, loss flux 18.874741, loss flux t1 21.410599, prod bhp loss 4.370572, prod bhp loss t1 22.055840\n",
      "Epoch 17/20, Batch 601/1500, Loss 32.020744, Loss rec 9.190856, loss rec t1 15.342688, loss kl 1.583311, loss_trans 0.013811, loss flux 20.307323, loss flux t1 21.500631, prod bhp loss 2.508754, prod bhp loss t1 7.739419\n",
      "Epoch 17/20, Batch 701/1500, Loss 29.424772, Loss rec 8.289902, loss rec t1 13.660294, loss kl 0.772473, loss_trans 0.008609, loss flux 18.699400, loss flux t1 19.432508, prod bhp loss 2.426862, prod bhp loss t1 6.447611\n",
      "Epoch 17/20, Batch 801/1500, Loss 29.685936, Loss rec 8.835615, loss rec t1 22.317923, loss kl 0.536697, loss_trans 0.022540, loss flux 17.004200, loss flux t1 19.245392, prod bhp loss 3.823582, prod bhp loss t1 12.408619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Batch 901/1500, Loss 35.839592, Loss rec 11.602075, loss rec t1 17.429419, loss kl 1.739032, loss_trans 0.012710, loss flux 21.138317, loss flux t1 22.597641, prod bhp loss 3.086489, prod bhp loss t1 5.980621\n",
      "Epoch 17/20, Batch 1001/1500, Loss 31.173719, Loss rec 10.167690, loss rec t1 15.198217, loss kl 0.353552, loss_trans 0.011197, loss flux 17.612162, loss flux t1 19.178368, prod bhp loss 3.382669, prod bhp loss t1 8.890501\n",
      "Epoch 17/20, Batch 1101/1500, Loss 45.128357, Loss rec 21.578274, loss rec t1 31.803448, loss kl 1.411554, loss_trans 0.014703, loss flux 20.208590, loss flux t1 21.595541, prod bhp loss 3.326788, prod bhp loss t1 10.182642\n",
      "Epoch 17/20, Batch 1201/1500, Loss 34.031891, Loss rec 10.426437, loss rec t1 14.837912, loss kl 2.208182, loss_trans 0.012558, loss flux 20.717802, loss flux t1 21.112904, prod bhp loss 2.875094, prod bhp loss t1 7.329598\n",
      "Epoch 17/20, Batch 1301/1500, Loss 27.969257, Loss rec 8.051281, loss rec t1 14.146624, loss kl 0.424138, loss_trans 0.008707, loss flux 16.787041, loss flux t1 18.154524, prod bhp loss 3.122229, prod bhp loss t1 8.090329\n",
      "Epoch 17/20, Batch 1401/1500, Loss 31.620890, Loss rec 13.048422, loss rec t1 41.826385, loss kl 1.053010, loss_trans 0.027726, loss flux 16.127327, loss flux t1 26.943663, prod bhp loss 2.417414, prod bhp loss t1 10.486307\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 17/20, Train loss 21.810930, Eval loss 64.217522\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 18/20, Batch 1/1500, Loss 21.779196, Loss rec 5.916374, loss rec t1 25.903685, loss kl 0.782961, loss_trans 0.026961, loss flux 13.656395, loss flux t1 22.911137, prod bhp loss 2.179466, prod bhp loss t1 11.570473\n",
      "Epoch 18/20, Batch 101/1500, Loss 21.795925, Loss rec 6.447478, loss rec t1 28.432549, loss kl 0.392937, loss_trans 0.019765, loss flux 12.437784, loss flux t1 21.973465, prod bhp loss 2.890901, prod bhp loss t1 12.477640\n",
      "Epoch 18/20, Batch 201/1500, Loss 24.807447, Loss rec 6.788277, loss rec t1 26.401451, loss kl 0.993777, loss_trans 0.025804, loss flux 15.162243, loss flux t1 26.080326, prod bhp loss 2.831122, prod bhp loss t1 10.921801\n",
      "Epoch 18/20, Batch 301/1500, Loss 35.903507, Loss rec 13.783417, loss rec t1 21.052740, loss kl 0.861677, loss_trans 0.014707, loss flux 19.201027, loss flux t1 21.912540, prod bhp loss 2.904358, prod bhp loss t1 9.279243\n",
      "Epoch 18/20, Batch 401/1500, Loss 28.489059, Loss rec 9.073458, loss rec t1 15.790367, loss kl 0.628622, loss_trans 0.014515, loss flux 17.128559, loss flux t1 19.724291, prod bhp loss 2.272528, prod bhp loss t1 8.723553\n",
      "Epoch 18/20, Batch 501/1500, Loss 32.428543, Loss rec 9.970078, loss rec t1 42.519691, loss kl 0.938518, loss_trans 0.040046, loss flux 18.369001, loss flux t1 20.056881, prod bhp loss 4.049416, prod bhp loss t1 20.723984\n",
      "Epoch 18/20, Batch 601/1500, Loss 34.284962, Loss rec 10.743738, loss rec t1 17.735832, loss kl 1.611481, loss_trans 0.015185, loss flux 19.701752, loss flux t1 20.559984, prod bhp loss 3.824287, prod bhp loss t1 8.664319\n",
      "Epoch 18/20, Batch 701/1500, Loss 29.802061, Loss rec 8.849548, loss rec t1 16.621458, loss kl 0.794090, loss_trans 0.010282, loss flux 17.857038, loss flux t1 18.665192, prod bhp loss 3.085192, prod bhp loss t1 8.495422\n",
      "Epoch 18/20, Batch 801/1500, Loss 28.897158, Loss rec 8.484942, loss rec t1 21.116789, loss kl 0.566442, loss_trans 0.018425, loss flux 16.546825, loss flux t1 18.864880, prod bhp loss 3.846963, prod bhp loss t1 13.111643\n",
      "Epoch 18/20, Batch 901/1500, Loss 33.256512, Loss rec 8.695137, loss rec t1 14.821500, loss kl 1.744951, loss_trans 0.018399, loss flux 20.748707, loss flux t1 22.129419, prod bhp loss 3.794269, prod bhp loss t1 6.353348\n",
      "Epoch 18/20, Batch 1001/1500, Loss 28.829327, Loss rec 8.316940, loss rec t1 14.268022, loss kl 0.357677, loss_trans 0.008743, loss flux 17.249367, loss flux t1 18.958618, prod bhp loss 3.254276, prod bhp loss t1 7.515312\n",
      "Epoch 18/20, Batch 1101/1500, Loss 35.610420, Loss rec 11.498028, loss rec t1 18.023518, loss kl 1.421672, loss_trans 0.014198, loss flux 19.136087, loss flux t1 20.252176, prod bhp loss 4.962106, prod bhp loss t1 9.393883\n",
      "Epoch 18/20, Batch 1201/1500, Loss 37.230667, Loss rec 12.620647, loss rec t1 28.455639, loss kl 2.287962, loss_trans 0.018279, loss flux 20.052231, loss flux t1 20.965975, prod bhp loss 4.539507, prod bhp loss t1 13.547796\n",
      "Epoch 18/20, Batch 1301/1500, Loss 26.909168, Loss rec 7.474585, loss rec t1 11.946056, loss kl 0.445838, loss_trans 0.009000, loss flux 17.055666, loss flux t1 18.401680, prod bhp loss 2.369917, prod bhp loss t1 6.935529\n",
      "Epoch 18/20, Batch 1401/1500, Loss 29.879910, Loss rec 9.053395, loss rec t1 27.423420, loss kl 1.100221, loss_trans 0.029972, loss flux 15.440475, loss flux t1 25.276567, prod bhp loss 5.356068, prod bhp loss t1 10.070414\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 18/20, Train loss 20.703081, Eval loss 59.881184\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 19/20, Batch 1/1500, Loss 20.547333, Loss rec 5.527347, loss rec t1 20.358973, loss kl 0.810380, loss_trans 0.022588, loss flux 12.775117, loss flux t1 20.483171, prod bhp loss 2.222281, prod bhp loss t1 10.627128\n",
      "Epoch 19/20, Batch 101/1500, Loss 25.373386, Loss rec 8.643647, loss rec t1 24.249853, loss kl 0.382870, loss_trans 0.017412, loss flux 12.902327, loss flux t1 20.542761, prod bhp loss 3.810000, prod bhp loss t1 12.625765\n",
      "Epoch 19/20, Batch 201/1500, Loss 26.898451, Loss rec 8.704848, loss rec t1 22.918768, loss kl 0.995826, loss_trans 0.025251, loss flux 14.858869, loss flux t1 23.855900, prod bhp loss 3.309482, prod bhp loss t1 11.499548\n",
      "Epoch 19/20, Batch 301/1500, Loss 36.864491, Loss rec 14.601836, loss rec t1 26.934933, loss kl 0.926056, loss_trans 0.015562, loss flux 18.797354, loss flux t1 21.261660, prod bhp loss 3.449738, prod bhp loss t1 12.832954\n",
      "Epoch 19/20, Batch 401/1500, Loss 43.555767, Loss rec 17.874859, loss rec t1 26.975807, loss kl 0.608874, loss_trans 0.014180, loss flux 17.150051, loss flux t1 19.620287, prod bhp loss 8.516676, prod bhp loss t1 11.182303\n",
      "Epoch 19/20, Batch 501/1500, Loss 38.694389, Loss rec 13.273352, loss rec t1 29.676773, loss kl 1.010043, loss_trans 0.038071, loss flux 18.736168, loss flux t1 19.770561, prod bhp loss 6.646797, prod bhp loss t1 16.627565\n",
      "Epoch 19/20, Batch 601/1500, Loss 42.189861, Loss rec 17.030346, loss rec t1 27.628805, loss kl 1.723171, loss_trans 0.018276, loss flux 20.654787, loss flux t1 21.889193, prod bhp loss 4.486449, prod bhp loss t1 11.748633\n",
      "Epoch 19/20, Batch 701/1500, Loss 31.441387, Loss rec 9.808378, loss rec t1 15.902940, loss kl 0.839817, loss_trans 0.007618, loss flux 17.699945, loss flux t1 18.903233, prod bhp loss 3.925447, prod bhp loss t1 7.744761\n",
      "Epoch 19/20, Batch 801/1500, Loss 30.074442, Loss rec 8.980238, loss rec t1 19.957912, loss kl 0.601299, loss_trans 0.020823, loss flux 16.780434, loss flux t1 18.802109, prod bhp loss 4.292947, prod bhp loss t1 11.659997\n",
      "Epoch 19/20, Batch 901/1500, Loss 35.072468, Loss rec 9.710635, loss rec t1 15.217438, loss kl 2.013145, loss_trans 0.009680, loss flux 20.490395, loss flux t1 21.796259, prod bhp loss 4.861754, prod bhp loss t1 8.031435\n",
      "Epoch 19/20, Batch 1001/1500, Loss 26.697397, Loss rec 7.613045, loss rec t1 16.101942, loss kl 0.390133, loss_trans 0.011505, loss flux 16.494261, loss flux t1 18.439001, prod bhp loss 2.578587, prod bhp loss t1 9.144243\n",
      "Epoch 19/20, Batch 1101/1500, Loss 31.938389, Loss rec 10.625880, loss rec t1 16.111851, loss kl 1.526599, loss_trans 0.013968, loss flux 18.576290, loss flux t1 19.814281, prod bhp loss 2.722250, prod bhp loss t1 7.369316\n",
      "Epoch 19/20, Batch 1201/1500, Loss 32.588627, Loss rec 10.129330, loss rec t1 17.459135, loss kl 2.397145, loss_trans 0.011106, loss flux 19.290173, loss flux t1 20.187326, prod bhp loss 3.158021, prod bhp loss t1 8.984529\n",
      "Epoch 19/20, Batch 1301/1500, Loss 24.853775, Loss rec 6.121559, loss rec t1 12.635579, loss kl 0.468000, loss_trans 0.010883, loss flux 16.185745, loss flux t1 17.926649, prod bhp loss 2.535588, prod bhp loss t1 8.170521\n",
      "Epoch 19/20, Batch 1401/1500, Loss 27.973312, Loss rec 8.147190, loss rec t1 28.168999, loss kl 1.151833, loss_trans 0.033103, loss flux 14.986184, loss flux t1 25.119080, prod bhp loss 4.806837, prod bhp loss t1 10.834497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 19/20, Train loss 20.004053, Eval loss 59.401882\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 20/20, Batch 1/1500, Loss 19.585705, Loss rec 5.272161, loss rec t1 20.974812, loss kl 0.839402, loss_trans 0.023843, loss flux 12.676744, loss flux t1 21.373720, prod bhp loss 1.612955, prod bhp loss t1 9.560857\n",
      "Epoch 20/20, Batch 101/1500, Loss 22.759220, Loss rec 6.350091, loss rec t1 26.035458, loss kl 0.436497, loss_trans 0.024907, loss flux 12.189466, loss flux t1 21.945580, prod bhp loss 4.194754, prod bhp loss t1 11.328128\n",
      "Epoch 20/20, Batch 201/1500, Loss 22.783590, Loss rec 6.259301, loss rec t1 22.420132, loss kl 1.089322, loss_trans 0.023499, loss flux 14.482325, loss flux t1 23.220360, prod bhp loss 2.018467, prod bhp loss t1 12.043717\n",
      "Epoch 20/20, Batch 301/1500, Loss 39.307453, Loss rec 12.030291, loss rec t1 24.915689, loss kl 1.007515, loss_trans 0.013969, loss flux 18.606422, loss flux t1 20.888697, prod bhp loss 8.656771, prod bhp loss t1 12.967157\n",
      "Epoch 20/20, Batch 401/1500, Loss 29.516436, Loss rec 8.580847, loss rec t1 14.665613, loss kl 0.657418, loss_trans 0.015040, loss flux 16.148844, loss flux t1 19.153824, prod bhp loss 4.771705, prod bhp loss t1 6.781019\n",
      "Epoch 20/20, Batch 501/1500, Loss 30.039392, Loss rec 9.307494, loss rec t1 21.114288, loss kl 1.002262, loss_trans 0.021952, loss flux 17.200628, loss flux t1 18.556570, prod bhp loss 3.509320, prod bhp loss t1 12.210510\n",
      "Epoch 20/20, Batch 601/1500, Loss 30.919798, Loss rec 9.404464, loss rec t1 16.744400, loss kl 1.750555, loss_trans 0.017141, loss flux 19.118223, loss flux t1 20.229767, prod bhp loss 2.379971, prod bhp loss t1 8.403956\n",
      "Epoch 20/20, Batch 701/1500, Loss 28.350725, Loss rec 8.342572, loss rec t1 13.738527, loss kl 0.829833, loss_trans 0.006229, loss flux 17.078766, loss flux t1 18.047993, prod bhp loss 2.923158, prod bhp loss t1 6.123207\n",
      "Epoch 20/20, Batch 801/1500, Loss 26.412680, Loss rec 7.714898, loss rec t1 20.695431, loss kl 0.583408, loss_trans 0.018575, loss flux 15.989743, loss flux t1 18.039196, prod bhp loss 2.689463, prod bhp loss t1 12.829353\n",
      "Epoch 20/20, Batch 901/1500, Loss 30.211746, Loss rec 7.799501, loss rec t1 13.121944, loss kl 1.906318, loss_trans 0.010572, loss flux 19.882208, loss flux t1 20.934181, prod bhp loss 2.519464, prod bhp loss t1 6.710477\n",
      "Epoch 20/20, Batch 1001/1500, Loss 31.213236, Loss rec 8.853436, loss rec t1 20.919142, loss kl 0.368496, loss_trans 0.012465, loss flux 16.441828, loss flux t1 18.637194, prod bhp loss 5.905507, prod bhp loss t1 12.288195\n",
      "Epoch 20/20, Batch 1101/1500, Loss 31.249943, Loss rec 9.169830, loss rec t1 15.153027, loss kl 1.498488, loss_trans 0.013225, loss flux 17.714209, loss flux t1 19.002842, prod bhp loss 4.352678, prod bhp loss t1 8.617413\n",
      "Epoch 20/20, Batch 1201/1500, Loss 35.945724, Loss rec 11.184637, loss rec t1 12.658154, loss kl 2.333121, loss_trans 0.007531, loss flux 19.141459, loss flux t1 19.829584, prod bhp loss 5.612101, prod bhp loss t1 5.439698\n",
      "Epoch 20/20, Batch 1301/1500, Loss 26.095058, Loss rec 6.662773, loss rec t1 12.350836, loss kl 0.449550, loss_trans 0.010177, loss flux 15.900366, loss flux t1 17.264383, prod bhp loss 3.521741, prod bhp loss t1 8.251171\n",
      "Epoch 20/20, Batch 1401/1500, Loss 25.157331, Loss rec 7.522501, loss rec t1 41.625092, loss kl 1.152211, loss_trans 0.048598, loss flux 14.826581, loss flux t1 26.382042, prod bhp loss 2.759651, prod bhp loss t1 13.672955\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 20/20, Train loss 21.156122, Eval loss 58.974380\n",
      "\n",
      "\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "# Optimization\n",
    "opt = Adam(lr=learning_rate)\n",
    "\n",
    "trainable_weights = encoder.trainable_weights + decoder.trainable_weights + transition.trainable_weights + wc_encoder.trainable_weights\n",
    "\n",
    "updates = opt.get_updates(loss, trainable_weights)\n",
    "\n",
    "# iterate = K.function([xt, ut, xt1, m_tf, dt], [loss, loss_rec_t, loss_rec_t1, loss_l2_reg, loss_trans, loss_flux_t, loss_flux_t1, binary_sat_loss_t, binary_sat_loss_t1], updates=updates)\n",
    "iterate = K.function([xt, ut, xt1, m_tf, wl_mask, dt], [loss, loss_rec_t, loss_rec_t1, loss_l2_reg, loss_trans, loss_flux_t, loss_flux_t1, loss_prod_bhp_t, loss_prod_bhp_t1], updates=updates)\n",
    "\n",
    "eval_loss = K.function([xt, ut, xt1, m_tf, wl_mask, dt], [loss])\n",
    "\n",
    "num_batch = int(num_train/batch_size)\n",
    "\n",
    "for e in range(epoch):\n",
    "    for ib in range(num_batch):\n",
    "        ind0 = ib * batch_size\n",
    "        state_t_batch  = state_t_train[ind0:ind0+batch_size, ...]\n",
    "        state_t1_batch = state_t1_train[ind0:ind0 + batch_size, ...]\n",
    "        bhp_batch      = bhp_train[ind0:ind0 + batch_size, ...]\n",
    "        m_batch        = m[ind0:ind0 + batch_size, ...]\n",
    "        dt_batch       = dt_train[ind0:ind0 + batch_size, ...]\n",
    "        wl_mask_batch  = wl_mask_train[ind0:ind0 + batch_size, ...]\n",
    "\n",
    "        output = iterate([state_t_batch, bhp_batch, state_t1_batch, m_batch, wl_mask_batch, dt_batch])\n",
    "\n",
    "        # tf.session.run(feed_dict={xt: sat_t_batch, ut: bhp_batch, xt1: sat_t1_batch}, ...\n",
    "        #                fetches= [loss, loss_rec_t, loss_rec_t1, loss_kl, loss_trans, updates])\n",
    "        # But output tensor for the updates operation is not returned\n",
    "        \n",
    "        n_itr = e * num_train + ib * batch_size + batch_size\n",
    "        write_summary(output[0], 'train/total_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        write_summary(output[1]+output[2], 'train/sum_rec_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        write_summary(output[5]+output[6], 'train/sum_flux_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        write_summary(output[7]+output[8], 'train/sum_well_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        \n",
    "        if ib % 100 == 0:\n",
    "            print('Epoch %d/%d, Batch %d/%d, Loss %f, Loss rec %f, loss rec t1 %f, loss kl %f, loss_trans %f, loss flux %f, loss flux t1 %f, prod bhp loss %f, prod bhp loss t1 %f'\n",
    "                  % (e+1, epoch, ib+1, num_batch, output[0], output[1], output[2], output[3], output[4], output[5], output[6], output[7], output[8]))\n",
    "            \n",
    "            eval_loss_val = eval_loss([state_t_eval, bhp_eval, state_t1_eval, m_eval, wl_mask_eval, dt_eval])\n",
    "            write_summary(eval_loss_val[0], 'eval/total_loss', summary_writer, n_itr) # log for tensorboard\n",
    "\n",
    "    \n",
    "    print('====================================================')\n",
    "    print('\\n')\n",
    "    print('Epoch %d/%d, Train loss %f, Eval loss %f' % (e + 1, epoch, output[0], eval_loss_val[0]))\n",
    "    print('\\n')\n",
    "    print('====================================================')\n",
    "\n",
    "encoder.save_weights(output_dir + 'e2c_encoder_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' % (num_train, latent_dim, learning_rate, epoch))\n",
    "decoder.save_weights(output_dir + 'e2c_decoder_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' % (num_train, latent_dim, learning_rate, epoch))\n",
    "transition.save_weights(output_dir + 'e2c_transition_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' % (num_train, latent_dim, learning_rate, epoch))\n",
    "wc_encoder.save_weights(output_dir + 'e2c_wc_encoder_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' % (num_train, latent_dim, learning_rate, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.10.0]",
   "language": "python",
   "name": "conda-env-tf-1.10.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
