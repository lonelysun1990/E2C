{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import e2c as e2c_util\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\", \"2\", \"3\"\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import datetime\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "\n",
    "\n",
    "# GPU memory management\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.session specification\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.85\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(x, t_decoded):\n",
    "    '''\n",
    "    Reconstruction loss for the plain VAE\n",
    "    '''\n",
    "    v = 0.1\n",
    "    # return K.mean(K.sum((K.batch_flatten(x) - K.batch_flatten(t_decoded)) ** 2 / (2*v) + 0.5*K.log(2*np.pi*v), axis=-1))\n",
    "    return K.mean(K.sum((K.batch_flatten(x) - K.batch_flatten(t_decoded)) ** 2 / (2*v), axis=-1))\n",
    "    # return K.sum((K.batch_flatten(x) - K.batch_flatten(t_decoded)) ** 2, axis=-1)\n",
    "\n",
    "\n",
    "def l2_reg_loss(qm):\n",
    "    # 0.5 * (torch.log(pv) - torch.log(qv) + qv / pv + (qm - pm).pow(2) / pv - 1)\n",
    "    # -0.5 * K.sum(1 + t_log_var - K.square(t_mean) - K.exp(t_log_var), axis=-1)\n",
    "#     kl = -0.5 * (1 - p_logv + q_logv - K.exp(q_logv) / K.exp(p_logv) - K.square(qm - pm) / K.exp(p_logv))\n",
    "    l2_reg = 0.5*K.square(qm)\n",
    "    return K.mean(K.sum(l2_reg, axis=-1))\n",
    "\n",
    "\n",
    "def get_flux_loss(m, state, state_pred):\n",
    "    '''\n",
    "    @params:  state, state_pred shape (batch_size, 60, 60, 2)\n",
    "              p, p_pred shape (batch_size, 60, 60, 1)\n",
    "              m shape (batch_size, 60, 60, 1)\n",
    "    \n",
    "    @return:  loss_flux: scalar\n",
    "    \n",
    "    Only consider discrepancies in total flux, not in phases (saturation not used) \n",
    "    '''\n",
    "    \n",
    "    perm = K.exp(m)\n",
    "    p = K.expand_dims(state[:, :, :, 1], -1)\n",
    "    p_pred = K.expand_dims(state_pred[:, :, :, 1], -1)\n",
    "\n",
    "    #print(K.int_shape(xxx))\n",
    "    \n",
    "    tran_x = 1./perm[:, 1:, ...] + 1./perm[:, :-1, ...]\n",
    "    tran_y = 1./perm[:, :, 1:, ...] + 1./perm[:, :, :-1, ...]\n",
    "    flux_x = (p[:, 1:, ...] - p[:, :-1, ...]) / tran_x\n",
    "    flux_y = (p[:, :, 1:, :] - p[:, :, :-1, :]) / tran_y\n",
    "    flux_x_pred = (p_pred[:, 1:, ...] - p_pred[:, :-1, ...]) / tran_x\n",
    "    flux_y_pred = (p_pred[:, :, 1:, :] - p_pred[:, :, :-1, :]) / tran_y\n",
    "\n",
    "    loss_x = K.sum(K.abs(K.batch_flatten(flux_x) - K.batch_flatten(flux_x_pred)), axis=-1)\n",
    "    loss_y = K.sum(K.abs(K.batch_flatten(flux_y) - K.batch_flatten(flux_y_pred)), axis=-1)\n",
    "\n",
    "    loss_flux = K.mean(loss_x + loss_y)\n",
    "    return loss_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_sat_loss(state, state_pred):\n",
    "    \n",
    "    sat_threshold = 0.105\n",
    "    sat = K.expand_dims(state[:, :, :, 0], -1)\n",
    "    sat_pred = K.expand_dims(state_pred[:, :, :, 0], -1)\n",
    "    \n",
    "    \n",
    "    sat_bool = K.greater_equal(sat, sat_threshold) #will return boolean values\n",
    "    sat_bin = K.cast(sat_bool, dtype=K.floatx()) #will convert bool to 0 and 1  \n",
    "    \n",
    "    sat_pred_bool = K.greater_equal(sat_pred, sat_threshold) #will return boolean values\n",
    "    sat_pred_bin = K.cast(sat_pred_bool, dtype=K.floatx()) #will convert bool to 0 and 1  \n",
    "    \n",
    "#     binary_loss = K.sum(K.abs(K.batch_flatten(sat_bin) - K.batch_flatten(sat_pred_bin)), axis=-1)\n",
    "    \n",
    "    binary_loss = losses.binary_crossentropy(sat_bin, sat_pred_bin)\n",
    "    return K.mean(binary_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_well_bhp_loss(state, state_pred, wl_mask):\n",
    "    '''\n",
    "    @params: state: shape (batch_size, 60, 60, 2)\n",
    "             state_pred: shape (batch_size, 60, 60, 2)\n",
    "             prod_well_loc: shape (batch_size, 5, 2)\n",
    "             \n",
    "    p_true: shape (batch_size, 60, 60, 1)\n",
    "    p_pred: shape (batch_size, 60, 60, 1)\n",
    "    \n",
    "    @return: bhp_loss: scalar\n",
    "    '''\n",
    "    \n",
    "    p_true = K.expand_dims(state[:, :, :, 1], -1) # shape (batch_size, 60, 60 ,1)\n",
    "    p_pred = K.expand_dims(state_pred[:, :, :, 1], -1)\n",
    "    \n",
    "    bhp_loss = K.mean(K.sum(K.abs(p_true -p_pred) * wl_mask[:,:,np.newaxis], axis=(1,2,3)))\n",
    "    \n",
    "    return bhp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_e2c(latent_dim, u_dim, input_shape, sigma=0.0):\n",
    "    '''\n",
    "    Creates a E2C.\n",
    "\n",
    "    Args:\n",
    "        latent_dim: dimensionality of latent space\n",
    "        return_kl_loss_op: whether to return the operation for\n",
    "                           computing the KL divergence loss.\n",
    "\n",
    "    Returns:\n",
    "        The VAE model. If return_kl_loss_op is True, then the\n",
    "        operation for computing the KL divergence loss is\n",
    "        additionally returned.\n",
    "    '''\n",
    "\n",
    "    encoder_ = e2c_util.create_encoder(latent_dim, input_shape, sigma=sigma)\n",
    "    decoder_ = e2c_util.create_decoder(latent_dim, input_shape)\n",
    "    transition_ = e2c_util.create_trans(latent_dim, u_dim)\n",
    "    wc_encoder_ = e2c_util.create_wc_encoder(latent_dim, input_shape)\n",
    "\n",
    "    return encoder_, decoder_, transition_, wc_encoder_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plain E2C model and associated loss operations\n",
    "\n",
    "################### case specification ######################\n",
    "\n",
    "# data_dir = '/data/cees/zjin/lstm_rom/datasets/9W_BHP/'\n",
    "# data_dir = '/data/cees/zjin/lstm_rom/datasets/9W_BHP_RATE/'\n",
    "# data_dir = '/data/cees/zjin/lstm_rom/datasets/9W_MS_BHP_RATE/'\n",
    "data_dir = '/data3/Astro/personal/zjin/datasets/9W_MS_BHP_RATE_GAU/'\n",
    "# data_dir = '/data3/Astro/personal/zjin/datasets/7W_CHA/'\n",
    "\n",
    "output_dir = '/data3/Astro/lstm_rom/e2c_larry/saved_models/'\n",
    "\n",
    "# case_name = '9w_bhp'\n",
    "# case_name = '9w_bhp_rate'\n",
    "case_name = '9w_ms_bhp_rate'\n",
    "# case_name = '7w_cha'\n",
    "\n",
    "# case_suffix = '_single_out_rel_2'\n",
    "# case_suffix = '_fix_wl_rel_8'\n",
    "case_suffix = '_var_wl_rel_1'\n",
    "\n",
    "train_suffix = '_with_p'\n",
    "\n",
    "model_suffix = '_flux_loss'\n",
    "# model_suffix = '_ae_no_l2_ep_10'\n",
    "# model_suffix = '_no_fl'\n",
    "\n",
    "\n",
    "n_train_run = 300\n",
    "n_eval_run = 100\n",
    "num_t = 20 \n",
    "# dt = 200 // num_t\n",
    "dt = 100\n",
    "n_train_step = n_train_run * num_t\n",
    "n_eval_step = n_eval_run * num_t\n",
    "\n",
    "\n",
    "train_file = case_name + '_e2c_train' + case_suffix + train_suffix + '_n%d_dt%dday_nt%d_nrun%d.mat' %(n_train_step, dt, num_t, n_train_run)\n",
    "eval_file = case_name + '_e2c_eval' + case_suffix + train_suffix +'_n%d_dt%dday_nt%d_nrun%d.mat' %(n_eval_step, dt, num_t, n_eval_run)\n",
    "\n",
    "#################### model specification ##################\n",
    "epoch = 20\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4\n",
    "latent_dim = 50\n",
    "\n",
    "# u_dim = 9*2 # control dimension\n",
    "u_dim = latent_dim # wwll control encoded to latent dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "hf_r = h5py.File(data_dir + train_file, 'r')\n",
    "state_t_train = np.array(hf_r.get('state_t'))\n",
    "state_t1_train = np.array(hf_r.get('state_t1'))\n",
    "bhp_train = np.array(hf_r.get('bhp'))\n",
    "dt_train = np.array(hf_r.get('dt'))\n",
    "wl_mask_train = np.array(hf_r.get('wl_mask'))\n",
    "hf_r.close()\n",
    "\n",
    "num_train = state_t_train.shape[0]\n",
    "# dt_train = np.ones((num_train,1)) # dt=20days, normalized to 1\n",
    "\n",
    "hf_r = h5py.File(data_dir + eval_file, 'r')\n",
    "state_t_eval = np.array(hf_r.get('state_t'))\n",
    "state_t1_eval = np.array(hf_r.get('state_t1'))\n",
    "bhp_eval = np.array(hf_r.get('bhp'))\n",
    "dt_eval = np.array(hf_r.get('dt'))\n",
    "wl_mask_eval = np.array(hf_r.get('wl_mask'))\n",
    "hf_r.close()\n",
    "\n",
    "num_eval = state_t_eval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m shape is  (60, 60, 1)\n",
      "m_eval shape is  (2000, 60, 60, 1)\n",
      "m shape is  (6000, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "# m = np.loadtxt(\"/data/cees/zjin/lstm_rom/sim_runs/case6_9w_bhp_rate_ms_h5/template/logk1.dat\")\n",
    "m = np.loadtxt(\"/data3/Astro/personal/zjin/sim_runs/case8_9w_bhp_rate_ms_gau/template/logk1.dat\") # Gaussian\n",
    "# m = np.loadtxt(\"/data3/Astro/personal/zjin/sim_runs/case9_cha/template/logk1.dat\") # channelized\n",
    "\n",
    "m = m.reshape(60, 60, 1)\n",
    "print('m shape is ', m.shape)\n",
    "#     m_tf = K.placeholder((batch_size, 60, 60 ,1))\n",
    "m_tf = Input(shape=(60, 60, 1))\n",
    "\n",
    "\n",
    "m_eval = np.repeat(np.expand_dims(m, axis = 0), state_t_eval.shape[0], axis = 0)\n",
    "print(\"m_eval shape is \", m_eval.shape)\n",
    "m = np.repeat(np.expand_dims(m,axis = 0), state_t_train.shape[0], axis = 0)\n",
    "print(\"m shape is \", m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(e2c_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct E2C\n",
    "input_shape = (60, 60, 2)\n",
    "\n",
    "#############################################\n",
    "# here we use a UAE framework, sigma = 0.0001\n",
    "#############################################\n",
    "encoder, decoder, transition, wc_encoder = create_e2c(latent_dim, u_dim, input_shape, sigma=0.0) \n",
    "\n",
    "\n",
    "xt = Input(shape=input_shape)\n",
    "xt1 = Input(shape=input_shape)\n",
    "# ut = Input(shape=(u_dim, ))\n",
    "ut = Input(shape=input_shape) # (60,60,2)\n",
    "dt = Input(shape=(1,))\n",
    "wl_mask = Input(shape=(60, 60)) # both prod and inj\n",
    "\n",
    "zt = encoder(xt)\n",
    "xt_rec = decoder(zt)\n",
    "\n",
    "\n",
    "ut_encoded = wc_encoder(ut)\n",
    "zt1 = encoder(xt1)\n",
    "\n",
    "zt1_pred = transition([zt, ut_encoded, dt])\n",
    "xt1_pred = decoder(zt1_pred)\n",
    "\n",
    "# Compute loss\n",
    "loss_rec_t = reconstruction_loss(xt, xt_rec)\n",
    "loss_rec_t1 = reconstruction_loss(xt1, xt1_pred)\n",
    "\n",
    "loss_flux_t = get_flux_loss(m_tf, xt, xt_rec) / 1000.\n",
    "loss_flux_t1 = get_flux_loss(m_tf, xt1, xt1_pred) / 1000.\n",
    "\n",
    "binary_sat_loss_t = get_binary_sat_loss(xt, xt_rec) * 1\n",
    "binary_sat_loss_t1 = get_binary_sat_loss(xt1, xt1_pred) * 1\n",
    "\n",
    "loss_prod_bhp_t = get_well_bhp_loss(xt, xt_rec, wl_mask) * 1\n",
    "loss_prod_bhp_t1 = get_well_bhp_loss(xt1, xt1_pred, wl_mask) * 1\n",
    "\n",
    "loss_l2_reg = l2_reg_loss(zt)  # log(1.) = 0.\n",
    "\n",
    "\n",
    "## -- loss bound: combine data losses\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_l2_reg  + loss_flux_t + loss_flux_t1\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_kl + binary_sat_loss_t + binary_sat_loss_t1\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_l2_reg  + loss_flux_t + loss_flux_t1 + loss_prod_bhp_t + loss_prod_bhp_t1 # JCP 2019 Gaussian case\n",
    "loss_bound = loss_rec_t + loss_rec_t1  + loss_flux_t + loss_flux_t1 + loss_prod_bhp_t + loss_prod_bhp_t1 # UAE\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_l2_reg # no flux/bhp loss comparison\n",
    "\n",
    "\n",
    "\n",
    "# Use zt_logvar to approximate zt1_logvar_pred\n",
    "loss_trans = l2_reg_loss(zt1_pred - zt1)\n",
    "# loss_trans = kl_normal_loss(zt1_mean_pred, zt1_logvar_pred, zt1_mean, zt1_logvar)\n",
    "\n",
    "\n",
    "trans_loss_weight = 1.0 # lambda in E2C paper Eq. (11)\n",
    "loss = loss_bound + trans_loss_weight * loss_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17-14:03\n"
     ]
    }
   ],
   "source": [
    "## log for tensorboard\n",
    "def write_summary(value, tag, summary_writer, global_step):\n",
    "    \"\"\"Write a single summary value to tensorboard\"\"\"\n",
    "    summary = tf.Summary()\n",
    "    summary.value.add(tag=tag, simple_value=value)\n",
    "    summary_writer.add_summary(summary, global_step)\n",
    "\n",
    "## used to generate log directory\n",
    "currentDT = datetime.datetime.now()\n",
    "current_time = str(currentDT).replace(\" \", \"-\")[:-10]\n",
    "print(current_time)\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('logs/' + case_name + case_suffix + '_ep' + str(epoch) + '_tr' + str(n_train_run) + '_' + current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 1/1500, Loss 16998.460938, Loss rec 7621.967285, loss rec t1 8587.764648, loss kl 0.037656, loss_trans 0.043145, loss flux 59.968620, loss flux t1 79.230949, prod bhp loss 313.883148, prod bhp loss t1 335.603088\n",
      "Epoch 1/10, Batch 51/1500, Loss 2069.970459, Loss rec 379.654236, loss rec t1 1254.191406, loss kl 9.352005, loss_trans 10.955702, loss flux 153.029648, loss flux t1 147.739883, prod bhp loss 35.288845, prod bhp loss t1 89.110550\n",
      "Epoch 1/10, Batch 101/1500, Loss 1292.242554, Loss rec 461.143921, loss rec t1 427.905243, loss kl 10.342924, loss_trans 10.590265, loss flux 148.115234, loss flux t1 156.272263, prod bhp loss 47.818321, prod bhp loss t1 40.397308\n",
      "Epoch 1/10, Batch 151/1500, Loss 1547.635376, Loss rec 573.058533, loss rec t1 543.290649, loss kl 14.458227, loss_trans 13.846134, loss flux 155.002548, loss flux t1 166.146835, prod bhp loss 51.548645, prod bhp loss t1 44.742023\n",
      "Epoch 1/10, Batch 201/1500, Loss 1130.814331, Loss rec 401.421478, loss rec t1 396.186676, loss kl 10.691322, loss_trans 8.139448, loss flux 126.211098, loss flux t1 136.596039, prod bhp loss 30.526468, prod bhp loss t1 31.733147\n",
      "Epoch 1/10, Batch 251/1500, Loss 1016.361511, Loss rec 339.925903, loss rec t1 352.176422, loss kl 10.572657, loss_trans 6.981635, loss flux 124.291916, loss flux t1 124.802063, prod bhp loss 33.029999, prod bhp loss t1 35.153564\n",
      "Epoch 1/10, Batch 301/1500, Loss 1139.105713, Loss rec 375.104370, loss rec t1 440.518829, loss kl 6.413651, loss_trans 3.608625, loss flux 114.880127, loss flux t1 117.136444, prod bhp loss 39.474834, prod bhp loss t1 48.382442\n",
      "Epoch 1/10, Batch 351/1500, Loss 955.814453, Loss rec 342.194641, loss rec t1 372.136902, loss kl 5.794026, loss_trans 2.722461, loss flux 83.749374, loss flux t1 84.591942, prod bhp loss 33.827866, prod bhp loss t1 36.591244\n",
      "Epoch 1/10, Batch 401/1500, Loss 1019.143555, Loss rec 371.651093, loss rec t1 422.547119, loss kl 5.033099, loss_trans 2.076379, loss flux 75.697800, loss flux t1 75.896851, prod bhp loss 33.149971, prod bhp loss t1 38.124344\n",
      "Epoch 1/10, Batch 451/1500, Loss 948.455505, Loss rec 318.695282, loss rec t1 386.010651, loss kl 5.906444, loss_trans 2.780588, loss flux 78.236359, loss flux t1 86.084457, prod bhp loss 33.502628, prod bhp loss t1 43.145580\n",
      "Epoch 1/10, Batch 501/1500, Loss 1171.319214, Loss rec 402.414490, loss rec t1 521.766785, loss kl 4.344398, loss_trans 1.798524, loss flux 78.462601, loss flux t1 79.653473, prod bhp loss 37.348629, prod bhp loss t1 49.874603\n",
      "Epoch 1/10, Batch 551/1500, Loss 982.636902, Loss rec 359.594971, loss rec t1 400.358246, loss kl 3.925539, loss_trans 1.542121, loss flux 80.115379, loss flux t1 79.995064, prod bhp loss 27.048691, prod bhp loss t1 33.982414\n",
      "Epoch 1/10, Batch 601/1500, Loss 1053.450562, Loss rec 386.672272, loss rec t1 417.011322, loss kl 5.660244, loss_trans 2.474457, loss flux 90.674919, loss flux t1 90.836830, prod bhp loss 32.084702, prod bhp loss t1 33.695961\n",
      "Epoch 1/10, Batch 651/1500, Loss 1105.989380, Loss rec 435.083191, loss rec t1 432.155945, loss kl 4.641877, loss_trans 1.796820, loss flux 86.212616, loss flux t1 84.174599, prod bhp loss 32.922382, prod bhp loss t1 33.643845\n",
      "Epoch 1/10, Batch 701/1500, Loss 773.220154, Loss rec 276.752930, loss rec t1 307.930847, loss kl 3.242187, loss_trans 1.393962, loss flux 68.714928, loss flux t1 69.138672, prod bhp loss 22.020576, prod bhp loss t1 27.268276\n",
      "Epoch 1/10, Batch 751/1500, Loss 859.624146, Loss rec 291.010345, loss rec t1 364.477295, loss kl 3.510482, loss_trans 1.615457, loss flux 72.807213, loss flux t1 75.902550, prod bhp loss 21.309185, prod bhp loss t1 32.502079\n",
      "Epoch 1/10, Batch 801/1500, Loss 791.833435, Loss rec 285.494537, loss rec t1 317.416321, loss kl 2.596961, loss_trans 0.886482, loss flux 68.901924, loss flux t1 68.777107, prod bhp loss 23.152615, prod bhp loss t1 27.204460\n",
      "Epoch 1/10, Batch 851/1500, Loss 769.511658, Loss rec 270.184448, loss rec t1 299.923767, loss kl 2.339894, loss_trans 1.055338, loss flux 76.721504, loss flux t1 74.994026, prod bhp loss 21.412035, prod bhp loss t1 25.220551\n",
      "Epoch 1/10, Batch 901/1500, Loss 912.300049, Loss rec 319.501526, loss rec t1 359.535400, loss kl 4.414313, loss_trans 1.821508, loss flux 88.162033, loss flux t1 86.955055, prod bhp loss 25.621897, prod bhp loss t1 30.702576\n",
      "Epoch 1/10, Batch 951/1500, Loss 916.882751, Loss rec 238.956299, loss rec t1 455.669647, loss kl 3.141600, loss_trans 1.497056, loss flux 70.423615, loss flux t1 82.222275, prod bhp loss 22.917213, prod bhp loss t1 45.196568\n",
      "Epoch 1/10, Batch 1001/1500, Loss 703.992188, Loss rec 238.937683, loss rec t1 269.851990, loss kl 2.051551, loss_trans 0.951409, loss flux 64.736847, loss flux t1 66.319061, prod bhp loss 31.602364, prod bhp loss t1 31.592865\n",
      "Epoch 1/10, Batch 1051/1500, Loss 584.099487, Loss rec 174.093231, loss rec t1 218.993408, loss kl 3.874088, loss_trans 1.528480, loss flux 75.363075, loss flux t1 73.314316, prod bhp loss 17.196941, prod bhp loss t1 23.610003\n",
      "Epoch 1/10, Batch 1101/1500, Loss 651.672302, Loss rec 205.052719, loss rec t1 266.902710, loss kl 3.592647, loss_trans 1.530527, loss flux 69.629662, loss flux t1 68.082115, prod bhp loss 16.558495, prod bhp loss t1 23.916136\n",
      "Epoch 1/10, Batch 1151/1500, Loss 655.012512, Loss rec 210.116272, loss rec t1 238.618561, loss kl 4.096629, loss_trans 2.151277, loss flux 75.234116, loss flux t1 76.486107, prod bhp loss 25.199753, prod bhp loss t1 27.206450\n",
      "Epoch 1/10, Batch 1201/1500, Loss 695.523987, Loss rec 216.666718, loss rec t1 270.454590, loss kl 4.466355, loss_trans 2.114478, loss flux 79.873100, loss flux t1 77.882866, prod bhp loss 22.199320, prod bhp loss t1 26.332857\n",
      "Epoch 1/10, Batch 1251/1500, Loss 723.082825, Loss rec 222.599304, loss rec t1 309.617615, loss kl 2.496073, loss_trans 1.108354, loss flux 67.273804, loss flux t1 66.963562, prod bhp loss 23.915161, prod bhp loss t1 31.605057\n",
      "Epoch 1/10, Batch 1301/1500, Loss 478.885559, Loss rec 135.754303, loss rec t1 177.160309, loss kl 1.446208, loss_trans 0.624569, loss flux 64.665146, loss flux t1 62.316639, prod bhp loss 14.659714, prod bhp loss t1 23.704826\n",
      "Epoch 1/10, Batch 1351/1500, Loss 639.623474, Loss rec 183.584778, loss rec t1 253.105392, loss kl 2.846938, loss_trans 1.372856, loss flux 72.399895, loss flux t1 72.313705, prod bhp loss 23.591400, prod bhp loss t1 33.255344\n",
      "Epoch 1/10, Batch 1401/1500, Loss 599.120300, Loss rec 142.967545, loss rec t1 270.062866, loss kl 2.027946, loss_trans 1.024509, loss flux 60.272221, loss flux t1 71.167450, prod bhp loss 17.487900, prod bhp loss t1 36.137764\n",
      "Epoch 1/10, Batch 1451/1500, Loss 460.795746, Loss rec 102.312630, loss rec t1 176.379532, loss kl 3.378633, loss_trans 1.742104, loss flux 62.483063, loss flux t1 75.836624, prod bhp loss 13.924913, prod bhp loss t1 28.116890\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 1/10, Train loss 399.762207, Eval loss 527.774658\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 2/10, Batch 1/1500, Loss 458.116180, Loss rec 110.616814, loss rec t1 176.391388, loss kl 1.941657, loss_trans 0.934761, loss flux 58.083565, loss flux t1 70.585411, prod bhp loss 16.631958, prod bhp loss t1 24.872271\n",
      "Epoch 2/10, Batch 51/1500, Loss 688.006531, Loss rec 103.189087, loss rec t1 399.168365, loss kl 0.948605, loss_trans 0.623028, loss flux 51.786640, loss flux t1 67.435837, prod bhp loss 16.564716, prod bhp loss t1 49.238846\n",
      "Epoch 2/10, Batch 101/1500, Loss 451.318207, Loss rec 84.843857, loss rec t1 206.647797, loss kl 1.260345, loss_trans 0.670422, loss flux 49.022709, loss flux t1 60.815369, prod bhp loss 17.353220, prod bhp loss t1 31.964882\n",
      "Epoch 2/10, Batch 151/1500, Loss 501.668427, Loss rec 128.605942, loss rec t1 190.416870, loss kl 2.837174, loss_trans 1.133650, loss flux 67.590248, loss flux t1 69.382156, prod bhp loss 17.623894, prod bhp loss t1 26.915634\n",
      "Epoch 2/10, Batch 201/1500, Loss 405.949799, Loss rec 94.911469, loss rec t1 153.729935, loss kl 2.050253, loss_trans 0.928964, loss flux 56.233246, loss flux t1 68.309975, prod bhp loss 11.980006, prod bhp loss t1 19.856230\n",
      "Epoch 2/10, Batch 251/1500, Loss 451.370544, Loss rec 119.799118, loss rec t1 161.420868, loss kl 2.589951, loss_trans 1.076153, loss flux 66.152031, loss flux t1 63.129177, prod bhp loss 16.200333, prod bhp loss t1 23.592865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Batch 301/1500, Loss 480.711121, Loss rec 114.513763, loss rec t1 171.381470, loss kl 1.493479, loss_trans 0.669249, loss flux 67.812332, loss flux t1 68.961311, prod bhp loss 25.120136, prod bhp loss t1 32.252842\n",
      "Epoch 2/10, Batch 351/1500, Loss 322.053284, Loss rec 72.592041, loss rec t1 95.900421, loss kl 1.671621, loss_trans 0.601464, loss flux 61.474728, loss flux t1 60.839954, prod bhp loss 12.537811, prod bhp loss t1 18.106846\n",
      "Epoch 2/10, Batch 401/1500, Loss 421.334320, Loss rec 101.546768, loss rec t1 150.238739, loss kl 1.468535, loss_trans 0.410820, loss flux 58.671902, loss flux t1 57.769142, prod bhp loss 22.233505, prod bhp loss t1 30.463470\n",
      "Epoch 2/10, Batch 451/1500, Loss 498.505798, Loss rec 115.604385, loss rec t1 195.894501, loss kl 2.491703, loss_trans 1.117937, loss flux 55.202736, loss flux t1 69.734344, prod bhp loss 27.769165, prod bhp loss t1 33.182705\n",
      "Epoch 2/10, Batch 501/1500, Loss 504.866913, Loss rec 122.419647, loss rec t1 202.814209, loss kl 1.683727, loss_trans 0.603862, loss flux 64.888084, loss flux t1 66.026733, prod bhp loss 15.691499, prod bhp loss t1 32.422882\n",
      "Epoch 2/10, Batch 551/1500, Loss 362.422333, Loss rec 82.750282, loss rec t1 113.596375, loss kl 1.783628, loss_trans 0.617692, loss flux 62.027382, loss flux t1 63.300457, prod bhp loss 17.279415, prod bhp loss t1 22.850739\n",
      "Epoch 2/10, Batch 601/1500, Loss 423.375031, Loss rec 104.956635, loss rec t1 138.186935, loss kl 2.156582, loss_trans 0.683854, loss flux 68.474068, loss flux t1 68.723000, prod bhp loss 18.315735, prod bhp loss t1 24.034803\n",
      "Epoch 2/10, Batch 651/1500, Loss 399.541168, Loss rec 104.496704, loss rec t1 126.320419, loss kl 2.089039, loss_trans 0.697651, loss flux 66.821922, loss flux t1 64.788292, prod bhp loss 15.180880, prod bhp loss t1 21.235294\n",
      "Epoch 2/10, Batch 701/1500, Loss 322.441162, Loss rec 77.907738, loss rec t1 96.848587, loss kl 1.454925, loss_trans 0.415962, loss flux 57.861168, loss flux t1 58.900269, prod bhp loss 14.254927, prod bhp loss t1 16.252529\n",
      "Epoch 2/10, Batch 751/1500, Loss 314.404236, Loss rec 78.080872, loss rec t1 78.172356, loss kl 1.738955, loss_trans 0.701736, loss flux 59.938271, loss flux t1 60.370777, prod bhp loss 19.060791, prod bhp loss t1 18.079426\n",
      "Epoch 2/10, Batch 801/1500, Loss 265.791992, Loss rec 54.639153, loss rec t1 74.002632, loss kl 1.198026, loss_trans 0.282685, loss flux 54.819019, loss flux t1 53.636761, prod bhp loss 11.919244, prod bhp loss t1 16.492479\n",
      "Epoch 2/10, Batch 851/1500, Loss 282.276703, Loss rec 56.466232, loss rec t1 80.140778, loss kl 1.219980, loss_trans 0.397963, loss flux 57.830330, loss flux t1 55.478035, prod bhp loss 12.235912, prod bhp loss t1 19.727486\n",
      "Epoch 2/10, Batch 901/1500, Loss 328.710388, Loss rec 75.187881, loss rec t1 86.495850, loss kl 2.223448, loss_trans 0.543206, loss flux 66.430351, loss flux t1 67.139183, prod bhp loss 15.450444, prod bhp loss t1 17.463459\n",
      "Epoch 2/10, Batch 951/1500, Loss 475.121918, Loss rec 76.381973, loss rec t1 221.782867, loss kl 1.819740, loss_trans 0.653221, loss flux 54.252728, loss flux t1 70.596558, prod bhp loss 14.780293, prod bhp loss t1 36.674263\n",
      "Epoch 2/10, Batch 1001/1500, Loss 250.618790, Loss rec 56.407681, loss rec t1 66.538666, loss kl 0.807480, loss_trans 0.243114, loss flux 51.188820, loss flux t1 51.893105, prod bhp loss 10.569594, prod bhp loss t1 13.777815\n",
      "Epoch 2/10, Batch 1051/1500, Loss 312.229919, Loss rec 76.135849, loss rec t1 86.594315, loss kl 1.899580, loss_trans 0.569684, loss flux 60.334366, loss flux t1 59.171082, prod bhp loss 13.200060, prod bhp loss t1 16.224567\n",
      "Epoch 2/10, Batch 1101/1500, Loss 313.229218, Loss rec 75.677109, loss rec t1 93.142311, loss kl 1.809750, loss_trans 0.522320, loss flux 57.749290, loss flux t1 56.567955, prod bhp loss 11.522945, prod bhp loss t1 18.047276\n",
      "Epoch 2/10, Batch 1151/1500, Loss 363.441376, Loss rec 85.967712, loss rec t1 119.326004, loss kl 1.849565, loss_trans 0.591925, loss flux 57.949043, loss flux t1 59.346161, prod bhp loss 16.124931, prod bhp loss t1 24.135578\n",
      "Epoch 2/10, Batch 1201/1500, Loss 379.058411, Loss rec 81.571831, loss rec t1 133.659210, loss kl 2.417929, loss_trans 0.795345, loss flux 62.291981, loss flux t1 61.173965, prod bhp loss 14.117771, prod bhp loss t1 25.448269\n",
      "Epoch 2/10, Batch 1251/1500, Loss 454.531647, Loss rec 128.473404, loss rec t1 165.759674, loss kl 1.304609, loss_trans 0.429335, loss flux 51.170395, loss flux t1 52.478058, prod bhp loss 25.012157, prod bhp loss t1 31.208626\n",
      "Epoch 2/10, Batch 1301/1500, Loss 217.564163, Loss rec 40.314892, loss rec t1 55.592377, loss kl 0.696669, loss_trans 0.234218, loss flux 48.735031, loss flux t1 47.495846, prod bhp loss 10.683668, prod bhp loss t1 14.508118\n",
      "Epoch 2/10, Batch 1351/1500, Loss 350.635376, Loss rec 80.946663, loss rec t1 111.134384, loss kl 1.455235, loss_trans 0.457812, loss flux 53.580105, loss flux t1 53.822857, prod bhp loss 22.332756, prod bhp loss t1 28.360790\n",
      "Epoch 2/10, Batch 1401/1500, Loss 313.638916, Loss rec 49.234810, loss rec t1 121.950409, loss kl 1.327367, loss_trans 0.484279, loss flux 45.395737, loss flux t1 60.004879, prod bhp loss 10.265949, prod bhp loss t1 26.302860\n",
      "Epoch 2/10, Batch 1451/1500, Loss 316.808289, Loss rec 56.476265, loss rec t1 109.333557, loss kl 2.028932, loss_trans 0.711823, loss flux 50.823162, loss flux t1 66.325127, prod bhp loss 10.346736, prod bhp loss t1 22.791630\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 2/10, Train loss 290.783997, Eval loss 334.809021\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 3/10, Batch 1/1500, Loss 370.015717, Loss rec 66.477219, loss rec t1 146.561844, loss kl 1.107522, loss_trans 0.391697, loss flux 45.408653, loss flux t1 61.278564, prod bhp loss 18.748140, prod bhp loss t1 31.149582\n",
      "Epoch 3/10, Batch 51/1500, Loss 457.180634, Loss rec 34.226189, loss rec t1 273.158447, loss kl 0.592189, loss_trans 0.426678, loss flux 40.446926, loss flux t1 59.798512, prod bhp loss 9.860165, prod bhp loss t1 39.263718\n",
      "Epoch 3/10, Batch 101/1500, Loss 331.477570, Loss rec 50.544487, loss rec t1 142.262680, loss kl 0.730199, loss_trans 0.326492, loss flux 37.750214, loss flux t1 52.328903, prod bhp loss 18.882450, prod bhp loss t1 29.382322\n",
      "Epoch 3/10, Batch 151/1500, Loss 502.259338, Loss rec 137.241623, loss rec t1 180.262558, loss kl 1.760534, loss_trans 0.417418, loss flux 60.301849, loss flux t1 60.818607, prod bhp loss 28.206741, prod bhp loss t1 35.010567\n",
      "Epoch 3/10, Batch 201/1500, Loss 323.610352, Loss rec 57.591347, loss rec t1 124.926537, loss kl 1.267401, loss_trans 0.431630, loss flux 45.887020, loss flux t1 61.223061, prod bhp loss 10.950756, prod bhp loss t1 22.600010\n",
      "Epoch 3/10, Batch 251/1500, Loss 281.396240, Loss rec 65.597427, loss rec t1 81.827003, loss kl 1.532552, loss_trans 0.439623, loss flux 53.596428, loss flux t1 51.734322, prod bhp loss 11.734707, prod bhp loss t1 16.466702\n",
      "Epoch 3/10, Batch 301/1500, Loss 276.594910, Loss rec 51.457947, loss rec t1 74.251595, loss kl 0.933408, loss_trans 0.281608, loss flux 55.353573, loss flux t1 56.915604, prod bhp loss 16.282331, prod bhp loss t1 22.052248\n",
      "Epoch 3/10, Batch 351/1500, Loss 230.699509, Loss rec 40.530479, loss rec t1 58.846611, loss kl 1.037441, loss_trans 0.254604, loss flux 49.938877, loss flux t1 50.348785, prod bhp loss 12.036320, prod bhp loss t1 18.743832\n",
      "Epoch 3/10, Batch 401/1500, Loss 242.839966, Loss rec 45.519875, loss rec t1 69.251122, loss kl 0.911848, loss_trans 0.160457, loss flux 47.897972, loss flux t1 48.861462, prod bhp loss 10.962789, prod bhp loss t1 20.186275\n",
      "Epoch 3/10, Batch 451/1500, Loss 310.666107, Loss rec 54.060051, loss rec t1 114.615425, loss kl 1.572420, loss_trans 0.548483, loss flux 44.045097, loss flux t1 59.780704, prod bhp loss 15.801577, prod bhp loss t1 21.814766\n",
      "Epoch 3/10, Batch 501/1500, Loss 371.452393, Loss rec 67.314919, loss rec t1 150.686737, loss kl 1.032286, loss_trans 0.279892, loss flux 52.221066, loss flux t1 54.176098, prod bhp loss 14.388116, prod bhp loss t1 32.385597\n",
      "Epoch 3/10, Batch 551/1500, Loss 237.214890, Loss rec 44.841351, loss rec t1 62.523865, loss kl 1.048527, loss_trans 0.244867, loss flux 49.094337, loss flux t1 51.566643, prod bhp loss 11.762372, prod bhp loss t1 17.181427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Batch 601/1500, Loss 323.274506, Loss rec 75.469635, loss rec t1 88.818420, loss kl 1.538796, loss_trans 0.330426, loss flux 59.022690, loss flux t1 59.569454, prod bhp loss 19.058285, prod bhp loss t1 21.005606\n",
      "Epoch 3/10, Batch 651/1500, Loss 313.668182, Loss rec 72.479477, loss rec t1 94.222046, loss kl 1.399804, loss_trans 0.317713, loss flux 56.274853, loss flux t1 54.213322, prod bhp loss 14.514521, prod bhp loss t1 21.646225\n",
      "Epoch 3/10, Batch 701/1500, Loss 224.213074, Loss rec 47.616035, loss rec t1 54.829407, loss kl 0.962479, loss_trans 0.195670, loss flux 48.576088, loss flux t1 49.958412, prod bhp loss 10.705578, prod bhp loss t1 12.331879\n",
      "Epoch 3/10, Batch 751/1500, Loss 214.221115, Loss rec 43.531059, loss rec t1 46.162704, loss kl 1.208065, loss_trans 0.366457, loss flux 48.376644, loss flux t1 49.147579, prod bhp loss 12.933926, prod bhp loss t1 13.702745\n",
      "Epoch 3/10, Batch 801/1500, Loss 186.275879, Loss rec 31.904953, loss rec t1 43.766308, loss kl 0.771807, loss_trans 0.150341, loss flux 43.129845, loss flux t1 43.456238, prod bhp loss 10.171412, prod bhp loss t1 13.696756\n",
      "Epoch 3/10, Batch 851/1500, Loss 288.871429, Loss rec 51.096848, loss rec t1 95.383278, loss kl 0.812187, loss_trans 0.189615, loss flux 49.955509, loss flux t1 49.332970, prod bhp loss 15.665804, prod bhp loss t1 27.247406\n",
      "Epoch 3/10, Batch 901/1500, Loss 235.152847, Loss rec 44.404198, loss rec t1 54.506660, loss kl 1.578669, loss_trans 0.265936, loss flux 54.901382, loss flux t1 56.130482, prod bhp loss 11.211190, prod bhp loss t1 13.733002\n",
      "Epoch 3/10, Batch 951/1500, Loss 367.287109, Loss rec 42.955494, loss rec t1 174.798050, loss kl 1.335830, loss_trans 0.417276, loss flux 44.462616, loss flux t1 62.007744, prod bhp loss 10.326017, prod bhp loss t1 32.319916\n",
      "Epoch 3/10, Batch 1001/1500, Loss 192.035446, Loss rec 34.237808, loss rec t1 48.095055, loss kl 0.521336, loss_trans 0.105053, loss flux 42.619030, loss flux t1 43.984467, prod bhp loss 8.120584, prod bhp loss t1 14.873444\n",
      "Epoch 3/10, Batch 1051/1500, Loss 248.681091, Loss rec 53.205215, loss rec t1 61.346634, loss kl 1.403069, loss_trans 0.300594, loss flux 52.226402, loss flux t1 51.822720, prod bhp loss 14.061911, prod bhp loss t1 15.717612\n",
      "Epoch 3/10, Batch 1101/1500, Loss 248.408401, Loss rec 51.157425, loss rec t1 70.323929, loss kl 1.303175, loss_trans 0.294152, loss flux 50.687489, loss flux t1 50.807232, prod bhp loss 8.840555, prod bhp loss t1 16.297604\n",
      "Epoch 3/10, Batch 1151/1500, Loss 255.578262, Loss rec 54.283817, loss rec t1 73.364433, loss kl 1.401207, loss_trans 0.316125, loss flux 49.545273, loss flux t1 50.933903, prod bhp loss 10.693336, prod bhp loss t1 16.441357\n",
      "Epoch 3/10, Batch 1201/1500, Loss 423.679474, Loss rec 77.283478, loss rec t1 190.658264, loss kl 1.964538, loss_trans 0.447019, loss flux 51.639751, loss flux t1 51.978558, prod bhp loss 15.516613, prod bhp loss t1 36.155815\n",
      "Epoch 3/10, Batch 1251/1500, Loss 292.180389, Loss rec 73.108818, loss rec t1 96.090469, loss kl 0.972229, loss_trans 0.229292, loss flux 43.433556, loss flux t1 44.682236, prod bhp loss 14.296851, prod bhp loss t1 20.339207\n",
      "Epoch 3/10, Batch 1301/1500, Loss 167.624786, Loss rec 27.178185, loss rec t1 39.113297, loss kl 0.515739, loss_trans 0.122621, loss flux 40.961197, loss flux t1 40.064796, prod bhp loss 8.163261, prod bhp loss t1 12.021418\n",
      "Epoch 3/10, Batch 1351/1500, Loss 261.757782, Loss rec 50.067093, loss rec t1 80.299202, loss kl 1.020699, loss_trans 0.247296, loss flux 45.824352, loss flux t1 46.766949, prod bhp loss 15.800847, prod bhp loss t1 22.752033\n",
      "Epoch 3/10, Batch 1401/1500, Loss 286.244598, Loss rec 51.711670, loss rec t1 101.253761, loss kl 1.128033, loss_trans 0.310515, loss flux 38.981609, loss flux t1 53.673058, prod bhp loss 18.073788, prod bhp loss t1 22.240211\n",
      "Epoch 3/10, Batch 1451/1500, Loss 239.537292, Loss rec 33.476929, loss rec t1 80.498177, loss kl 1.586250, loss_trans 0.365996, loss flux 40.872383, loss flux t1 57.344994, prod bhp loss 8.100910, prod bhp loss t1 18.877920\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 3/10, Train loss 226.165466, Eval loss 280.125824\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 4/10, Batch 1/1500, Loss 285.924408, Loss rec 41.518524, loss rec t1 111.622719, loss kl 0.801818, loss_trans 0.234201, loss flux 36.568867, loss flux t1 53.321064, prod bhp loss 15.823544, prod bhp loss t1 26.835499\n",
      "Epoch 4/10, Batch 51/1500, Loss 403.520233, Loss rec 25.593555, loss rec t1 243.709106, loss kl 0.491144, loss_trans 0.398511, loss flux 33.170990, loss flux t1 52.811260, prod bhp loss 10.137615, prod bhp loss t1 37.699226\n",
      "Epoch 4/10, Batch 101/1500, Loss 250.600693, Loss rec 26.441933, loss rec t1 114.303818, loss kl 0.541106, loss_trans 0.200214, loss flux 30.270561, loss flux t1 45.645893, prod bhp loss 10.391710, prod bhp loss t1 23.346552\n",
      "Epoch 4/10, Batch 151/1500, Loss 354.881775, Loss rec 89.125809, loss rec t1 114.823853, loss kl 1.424764, loss_trans 0.226929, loss flux 52.645489, loss flux t1 53.412731, prod bhp loss 19.978472, prod bhp loss t1 24.668484\n",
      "Epoch 4/10, Batch 201/1500, Loss 241.359299, Loss rec 33.872158, loss rec t1 88.397812, loss kl 0.924268, loss_trans 0.221279, loss flux 38.574142, loss flux t1 54.728104, prod bhp loss 7.270926, prod bhp loss t1 18.294878\n",
      "Epoch 4/10, Batch 251/1500, Loss 248.612228, Loss rec 56.668106, loss rec t1 72.869568, loss kl 1.112133, loss_trans 0.229435, loss flux 44.943558, loss flux t1 44.224045, prod bhp loss 12.309029, prod bhp loss t1 17.368486\n",
      "Epoch 4/10, Batch 301/1500, Loss 230.274826, Loss rec 37.008617, loss rec t1 59.367348, loss kl 0.715411, loss_trans 0.143919, loss flux 47.268456, loss flux t1 48.915478, prod bhp loss 15.919489, prod bhp loss t1 21.651520\n",
      "Epoch 4/10, Batch 351/1500, Loss 173.687881, Loss rec 26.857683, loss rec t1 38.217804, loss kl 0.787480, loss_trans 0.134172, loss flux 42.195587, loss flux t1 42.172722, prod bhp loss 9.562706, prod bhp loss t1 14.547215\n",
      "Epoch 4/10, Batch 401/1500, Loss 197.173126, Loss rec 33.939644, loss rec t1 54.444942, loss kl 0.685541, loss_trans 0.085006, loss flux 39.599407, loss flux t1 41.471626, prod bhp loss 9.363719, prod bhp loss t1 18.268766\n",
      "Epoch 4/10, Batch 451/1500, Loss 251.079254, Loss rec 35.649384, loss rec t1 94.011353, loss kl 1.259625, loss_trans 0.346948, loss flux 37.277016, loss flux t1 53.103123, prod bhp loss 11.469040, prod bhp loss t1 19.222391\n",
      "Epoch 4/10, Batch 501/1500, Loss 264.833679, Loss rec 39.521515, loss rec t1 96.253937, loss kl 0.842708, loss_trans 0.171239, loss flux 44.180851, loss flux t1 46.100288, prod bhp loss 10.464927, prod bhp loss t1 28.140945\n",
      "Epoch 4/10, Batch 551/1500, Loss 205.827805, Loss rec 38.256725, loss rec t1 55.514008, loss kl 0.818497, loss_trans 0.150412, loss flux 42.597050, loss flux t1 44.636463, prod bhp loss 8.251225, prod bhp loss t1 16.421936\n",
      "Epoch 4/10, Batch 601/1500, Loss 250.004974, Loss rec 54.651009, loss rec t1 64.353966, loss kl 1.206266, loss_trans 0.182736, loss flux 49.775600, loss flux t1 50.900707, prod bhp loss 13.943939, prod bhp loss t1 16.197004\n",
      "Epoch 4/10, Batch 651/1500, Loss 211.318085, Loss rec 36.746078, loss rec t1 55.120560, loss kl 1.035185, loss_trans 0.154263, loss flux 47.273666, loss flux t1 46.698689, prod bhp loss 7.857516, prod bhp loss t1 17.467312\n",
      "Epoch 4/10, Batch 701/1500, Loss 197.655807, Loss rec 38.083008, loss rec t1 48.933376, loss kl 0.727719, loss_trans 0.102345, loss flux 41.266018, loss flux t1 42.363293, prod bhp loss 11.415602, prod bhp loss t1 15.492163\n",
      "Epoch 4/10, Batch 751/1500, Loss 171.988129, Loss rec 28.275019, loss rec t1 35.306786, loss kl 0.935027, loss_trans 0.207318, loss flux 40.998222, loss flux t1 42.836285, prod bhp loss 11.118968, prod bhp loss t1 13.245527\n",
      "Epoch 4/10, Batch 801/1500, Loss 153.557785, Loss rec 24.537228, loss rec t1 35.032532, loss kl 0.610119, loss_trans 0.086167, loss flux 36.139423, loss flux t1 36.882507, prod bhp loss 7.658677, prod bhp loss t1 13.221248\n",
      "Epoch 4/10, Batch 851/1500, Loss 189.959702, Loss rec 25.389698, loss rec t1 53.154610, loss kl 0.598907, loss_trans 0.102763, loss flux 41.405025, loss flux t1 41.353340, prod bhp loss 8.605455, prod bhp loss t1 19.948803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Batch 901/1500, Loss 190.965057, Loss rec 34.361046, loss rec t1 41.466694, loss kl 1.239509, loss_trans 0.146438, loss flux 46.390610, loss flux t1 48.142509, prod bhp loss 9.651222, prod bhp loss t1 10.806534\n",
      "Epoch 4/10, Batch 951/1500, Loss 329.030273, Loss rec 33.052937, loss rec t1 160.835175, loss kl 1.084533, loss_trans 0.301620, loss flux 37.792908, loss flux t1 56.241661, prod bhp loss 11.064322, prod bhp loss t1 29.741682\n",
      "Epoch 4/10, Batch 1001/1500, Loss 162.802216, Loss rec 34.039917, loss rec t1 34.106491, loss kl 0.455316, loss_trans 0.063784, loss flux 37.200134, loss flux t1 37.974522, prod bhp loss 9.635582, prod bhp loss t1 9.781801\n",
      "Epoch 4/10, Batch 1051/1500, Loss 189.171005, Loss rec 33.794918, loss rec t1 39.926479, loss kl 1.175774, loss_trans 0.178833, loss flux 45.797112, loss flux t1 45.895416, prod bhp loss 10.907959, prod bhp loss t1 12.670288\n",
      "Epoch 4/10, Batch 1101/1500, Loss 209.114975, Loss rec 37.739662, loss rec t1 56.647713, loss kl 0.994876, loss_trans 0.162626, loss flux 43.737499, loss flux t1 44.629433, prod bhp loss 9.780714, prod bhp loss t1 16.417330\n",
      "Epoch 4/10, Batch 1151/1500, Loss 185.847778, Loss rec 33.294468, loss rec t1 46.112221, loss kl 1.201802, loss_trans 0.186702, loss flux 42.708317, loss flux t1 44.262836, prod bhp loss 7.146595, prod bhp loss t1 12.136646\n",
      "Epoch 4/10, Batch 1201/1500, Loss 272.523651, Loss rec 47.550976, loss rec t1 100.043854, loss kl 1.579750, loss_trans 0.240545, loss flux 44.889076, loss flux t1 44.762970, prod bhp loss 9.816785, prod bhp loss t1 25.219448\n",
      "Epoch 4/10, Batch 1251/1500, Loss 176.555191, Loss rec 32.110214, loss rec t1 45.492741, loss kl 0.883887, loss_trans 0.140899, loss flux 37.192970, loss flux t1 38.882126, prod bhp loss 8.687399, prod bhp loss t1 14.048865\n",
      "Epoch 4/10, Batch 1301/1500, Loss 135.097031, Loss rec 18.830730, loss rec t1 28.816780, loss kl 0.431938, loss_trans 0.069756, loss flux 35.332901, loss flux t1 34.823681, prod bhp loss 6.512849, prod bhp loss t1 10.710321\n",
      "Epoch 4/10, Batch 1351/1500, Loss 230.772583, Loss rec 43.767780, loss rec t1 65.001793, loss kl 0.849162, loss_trans 0.143512, loss flux 40.908760, loss flux t1 42.210556, prod bhp loss 17.453583, prod bhp loss t1 21.286591\n",
      "Epoch 4/10, Batch 1401/1500, Loss 281.667938, Loss rec 48.033737, loss rec t1 106.390106, loss kl 0.996259, loss_trans 0.205704, loss flux 34.513142, loss flux t1 51.403053, prod bhp loss 17.521374, prod bhp loss t1 23.600819\n",
      "Epoch 4/10, Batch 1451/1500, Loss 205.581848, Loss rec 22.848082, loss rec t1 67.608597, loss kl 1.284302, loss_trans 0.230517, loss flux 36.590225, loss flux t1 52.953373, prod bhp loss 7.656215, prod bhp loss t1 17.694845\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 4/10, Train loss 194.844559, Eval loss 261.203064\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 5/10, Batch 1/1500, Loss 230.582306, Loss rec 27.183662, loss rec t1 93.269836, loss kl 0.651924, loss_trans 0.160073, loss flux 29.117472, loss flux t1 45.662930, prod bhp loss 11.462175, prod bhp loss t1 23.726151\n",
      "Epoch 5/10, Batch 51/1500, Loss 385.543518, Loss rec 20.850315, loss rec t1 238.065918, loss kl 0.427735, loss_trans 0.329711, loss flux 29.219631, loss flux t1 48.413784, prod bhp loss 10.643550, prod bhp loss t1 38.020584\n",
      "Epoch 5/10, Batch 101/1500, Loss 225.892105, Loss rec 19.254478, loss rec t1 107.533966, loss kl 0.459646, loss_trans 0.147482, loss flux 26.754156, loss flux t1 42.209442, prod bhp loss 7.377411, prod bhp loss t1 22.615170\n",
      "Epoch 5/10, Batch 151/1500, Loss 378.243134, Loss rec 98.131714, loss rec t1 127.527893, loss kl 1.248735, loss_trans 0.137239, loss flux 46.365448, loss flux t1 48.187050, prod bhp loss 26.470026, prod bhp loss t1 31.423752\n",
      "Epoch 5/10, Batch 201/1500, Loss 200.205078, Loss rec 23.997446, loss rec t1 72.007561, loss kl 0.774060, loss_trans 0.159698, loss flux 32.538334, loss flux t1 48.554199, prod bhp loss 6.501978, prod bhp loss t1 16.445862\n",
      "Epoch 5/10, Batch 251/1500, Loss 184.099777, Loss rec 36.413948, loss rec t1 48.034317, loss kl 0.985781, loss_trans 0.152990, loss flux 38.764374, loss flux t1 38.349701, prod bhp loss 8.341329, prod bhp loss t1 14.043119\n",
      "Epoch 5/10, Batch 301/1500, Loss 202.927933, Loss rec 29.820639, loss rec t1 53.506596, loss kl 0.604174, loss_trans 0.089255, loss flux 41.340778, loss flux t1 43.128258, prod bhp loss 14.453004, prod bhp loss t1 20.589409\n",
      "Epoch 5/10, Batch 351/1500, Loss 147.734314, Loss rec 20.875641, loss rec t1 31.179245, loss kl 0.653556, loss_trans 0.084565, loss flux 36.979683, loss flux t1 37.324848, prod bhp loss 8.398796, prod bhp loss t1 12.891548\n",
      "Epoch 5/10, Batch 401/1500, Loss 168.987228, Loss rec 25.210983, loss rec t1 45.571308, loss kl 0.566090, loss_trans 0.056110, loss flux 34.907604, loss flux t1 36.663792, prod bhp loss 8.705776, prod bhp loss t1 17.871651\n",
      "Epoch 5/10, Batch 451/1500, Loss 236.274689, Loss rec 31.276131, loss rec t1 93.755096, loss kl 1.054868, loss_trans 0.242257, loss flux 32.973682, loss flux t1 47.933556, prod bhp loss 9.695434, prod bhp loss t1 20.398516\n",
      "Epoch 5/10, Batch 501/1500, Loss 240.566315, Loss rec 34.844757, loss rec t1 89.943817, loss kl 0.713482, loss_trans 0.110244, loss flux 38.508751, loss flux t1 40.234528, prod bhp loss 9.489504, prod bhp loss t1 27.434704\n",
      "Epoch 5/10, Batch 551/1500, Loss 202.466537, Loss rec 36.127159, loss rec t1 58.066101, loss kl 0.709277, loss_trans 0.107493, loss flux 37.941494, loss flux t1 39.809555, prod bhp loss 11.015218, prod bhp loss t1 19.399529\n",
      "Epoch 5/10, Batch 601/1500, Loss 210.758591, Loss rec 38.633377, loss rec t1 57.083767, loss kl 0.983374, loss_trans 0.122549, loss flux 43.450665, loss flux t1 44.992527, prod bhp loss 10.080967, prod bhp loss t1 16.394741\n",
      "Epoch 5/10, Batch 651/1500, Loss 181.787674, Loss rec 31.273136, loss rec t1 46.354347, loss kl 0.853625, loss_trans 0.101490, loss flux 40.901627, loss flux t1 40.754642, prod bhp loss 6.097501, prod bhp loss t1 16.304924\n",
      "Epoch 5/10, Batch 701/1500, Loss 184.328873, Loss rec 32.293694, loss rec t1 45.859505, loss kl 0.617098, loss_trans 0.069023, loss flux 37.500797, loss flux t1 38.130802, prod bhp loss 13.075558, prod bhp loss t1 17.399498\n",
      "Epoch 5/10, Batch 751/1500, Loss 160.379196, Loss rec 27.437031, loss rec t1 35.200077, loss kl 0.797322, loss_trans 0.141287, loss flux 36.992332, loss flux t1 38.122604, prod bhp loss 9.270638, prod bhp loss t1 13.215246\n",
      "Epoch 5/10, Batch 801/1500, Loss 137.496811, Loss rec 20.968287, loss rec t1 32.277462, loss kl 0.550122, loss_trans 0.059335, loss flux 31.870457, loss flux t1 32.759361, prod bhp loss 6.590727, prod bhp loss t1 12.971176\n",
      "Epoch 5/10, Batch 851/1500, Loss 157.939285, Loss rec 23.563049, loss rec t1 34.431225, loss kl 0.510080, loss_trans 0.068168, loss flux 36.300423, loss flux t1 36.091797, prod bhp loss 11.654619, prod bhp loss t1 15.830021\n",
      "Epoch 5/10, Batch 901/1500, Loss 176.434036, Loss rec 32.671227, loss rec t1 34.583637, loss kl 1.042255, loss_trans 0.091972, loss flux 41.044449, loss flux t1 42.727432, prod bhp loss 12.957668, prod bhp loss t1 12.357656\n",
      "Epoch 5/10, Batch 951/1500, Loss 275.899780, Loss rec 22.194849, loss rec t1 135.997772, loss kl 0.936908, loss_trans 0.244831, loss flux 33.562824, loss flux t1 51.899342, prod bhp loss 6.737731, prod bhp loss t1 25.262407\n",
      "Epoch 5/10, Batch 1001/1500, Loss 139.280533, Loss rec 20.040586, loss rec t1 31.557116, loss kl 0.370841, loss_trans 0.041645, loss flux 34.468189, loss flux t1 35.319656, prod bhp loss 6.243092, prod bhp loss t1 11.610252\n",
      "Epoch 5/10, Batch 1051/1500, Loss 241.754501, Loss rec 55.483383, loss rec t1 58.281067, loss kl 1.118290, loss_trans 0.137458, loss flux 42.721024, loss flux t1 42.377556, prod bhp loss 20.837036, prod bhp loss t1 21.916981\n",
      "Epoch 5/10, Batch 1101/1500, Loss 193.435028, Loss rec 38.173805, loss rec t1 55.360001, loss kl 0.884275, loss_trans 0.119045, loss flux 38.595425, loss flux t1 39.308800, prod bhp loss 7.018878, prod bhp loss t1 14.859086\n",
      "Epoch 5/10, Batch 1151/1500, Loss 159.868835, Loss rec 26.659161, loss rec t1 37.087189, loss kl 1.058586, loss_trans 0.120794, loss flux 37.438133, loss flux t1 38.932690, prod bhp loss 7.790086, prod bhp loss t1 11.840793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Batch 1201/1500, Loss 200.267670, Loss rec 30.943449, loss rec t1 64.925079, loss kl 1.415661, loss_trans 0.174955, loss flux 39.340954, loss flux t1 39.747799, prod bhp loss 5.989881, prod bhp loss t1 19.145546\n",
      "Epoch 5/10, Batch 1251/1500, Loss 137.671417, Loss rec 21.880875, loss rec t1 29.119711, loss kl 0.823642, loss_trans 0.098331, loss flux 33.729530, loss flux t1 35.560642, prod bhp loss 6.956777, prod bhp loss t1 10.325560\n",
      "Epoch 5/10, Batch 1301/1500, Loss 119.333633, Loss rec 16.317402, loss rec t1 23.163242, loss kl 0.402783, loss_trans 0.047628, loss flux 31.576704, loss flux t1 31.738760, prod bhp loss 6.902711, prod bhp loss t1 9.587182\n",
      "Epoch 5/10, Batch 1351/1500, Loss 172.754868, Loss rec 26.126499, loss rec t1 45.662487, loss kl 0.748421, loss_trans 0.090629, loss flux 36.687469, loss flux t1 38.106369, prod bhp loss 9.720388, prod bhp loss t1 16.361042\n",
      "Epoch 5/10, Batch 1401/1500, Loss 209.647964, Loss rec 25.581730, loss rec t1 74.874428, loss kl 0.858303, loss_trans 0.149431, loss flux 30.996859, loss flux t1 47.546314, prod bhp loss 11.433567, prod bhp loss t1 19.065651\n",
      "Epoch 5/10, Batch 1451/1500, Loss 193.651016, Loss rec 21.841000, loss rec t1 64.024269, loss kl 1.086625, loss_trans 0.172432, loss flux 33.253548, loss flux t1 49.558838, prod bhp loss 7.730352, prod bhp loss t1 17.070595\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 5/10, Train loss 175.866211, Eval loss 247.464966\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 6/10, Batch 1/1500, Loss 218.644653, Loss rec 26.737465, loss rec t1 90.591476, loss kl 0.583157, loss_trans 0.128698, loss flux 26.320274, loss flux t1 42.307095, prod bhp loss 9.680878, prod bhp loss t1 22.878773\n",
      "Epoch 6/10, Batch 51/1500, Loss 359.210785, Loss rec 16.952044, loss rec t1 227.747681, loss kl 0.413565, loss_trans 0.253521, loss flux 26.933392, loss flux t1 46.176472, prod bhp loss 6.675333, prod bhp loss t1 34.472336\n",
      "Epoch 6/10, Batch 101/1500, Loss 210.670303, Loss rec 14.806744, loss rec t1 104.191406, loss kl 0.421662, loss_trans 0.124472, loss flux 24.079479, loss flux t1 39.315571, prod bhp loss 4.314534, prod bhp loss t1 23.838112\n",
      "Epoch 6/10, Batch 151/1500, Loss 235.684982, Loss rec 48.830742, loss rec t1 65.714920, loss kl 1.112539, loss_trans 0.101154, loss flux 40.417080, loss flux t1 42.138454, prod bhp loss 17.196009, prod bhp loss t1 21.286613\n",
      "Epoch 6/10, Batch 201/1500, Loss 207.051712, Loss rec 23.706749, loss rec t1 80.602783, loss kl 0.720233, loss_trans 0.135858, loss flux 30.370493, loss flux t1 46.643818, prod bhp loss 7.287935, prod bhp loss t1 18.304073\n",
      "Epoch 6/10, Batch 251/1500, Loss 199.585709, Loss rec 42.271408, loss rec t1 53.971039, loss kl 0.969618, loss_trans 0.118053, loss flux 36.710518, loss flux t1 36.510731, prod bhp loss 12.750133, prod bhp loss t1 17.253819\n",
      "Epoch 6/10, Batch 301/1500, Loss 227.350693, Loss rec 40.135284, loss rec t1 69.624557, loss kl 0.525926, loss_trans 0.065836, loss flux 36.901859, loss flux t1 39.007175, prod bhp loss 18.086418, prod bhp loss t1 23.529575\n",
      "Epoch 6/10, Batch 351/1500, Loss 142.115234, Loss rec 21.682884, loss rec t1 31.981106, loss kl 0.585338, loss_trans 0.066747, loss flux 33.125866, loss flux t1 33.346474, prod bhp loss 9.254185, prod bhp loss t1 12.657980\n",
      "Epoch 6/10, Batch 401/1500, Loss 149.091385, Loss rec 21.604139, loss rec t1 39.475151, loss kl 0.513210, loss_trans 0.043545, loss flux 31.411619, loss flux t1 32.623203, prod bhp loss 7.847541, prod bhp loss t1 16.086185\n",
      "Epoch 6/10, Batch 451/1500, Loss 227.391937, Loss rec 27.456295, loss rec t1 93.866852, loss kl 0.930225, loss_trans 0.188505, loss flux 29.861946, loss flux t1 44.848473, prod bhp loss 9.078699, prod bhp loss t1 22.091152\n",
      "Epoch 6/10, Batch 501/1500, Loss 190.075317, Loss rec 23.766685, loss rec t1 65.238098, loss kl 0.689057, loss_trans 0.080529, loss flux 33.908627, loss flux t1 36.168026, prod bhp loss 7.365099, prod bhp loss t1 23.548241\n",
      "Epoch 6/10, Batch 551/1500, Loss 194.204666, Loss rec 31.236816, loss rec t1 55.687099, loss kl 0.672461, loss_trans 0.087200, loss flux 34.417797, loss flux t1 36.887478, prod bhp loss 15.073168, prod bhp loss t1 20.815102\n",
      "Epoch 6/10, Batch 601/1500, Loss 174.136200, Loss rec 29.656162, loss rec t1 40.802635, loss kl 0.872848, loss_trans 0.083441, loss flux 36.944118, loss flux t1 38.033710, prod bhp loss 12.661629, prod bhp loss t1 15.954494\n",
      "Epoch 6/10, Batch 651/1500, Loss 141.879822, Loss rec 19.042505, loss rec t1 29.850063, loss kl 0.755631, loss_trans 0.072632, loss flux 36.688728, loss flux t1 37.096104, prod bhp loss 5.895511, prod bhp loss t1 13.234287\n",
      "Epoch 6/10, Batch 701/1500, Loss 164.610260, Loss rec 28.313080, loss rec t1 39.586575, loss kl 0.576810, loss_trans 0.056866, loss flux 34.627033, loss flux t1 35.394196, prod bhp loss 11.593284, prod bhp loss t1 15.039235\n",
      "Epoch 6/10, Batch 751/1500, Loss 144.868912, Loss rec 18.550285, loss rec t1 34.586594, loss kl 0.707229, loss_trans 0.105851, loss flux 33.568634, loss flux t1 34.876980, prod bhp loss 7.667930, prod bhp loss t1 15.512650\n",
      "Epoch 6/10, Batch 801/1500, Loss 117.000893, Loss rec 13.955168, loss rec t1 25.944984, loss kl 0.491681, loss_trans 0.047861, loss flux 28.437141, loss flux t1 29.706556, prod bhp loss 5.587809, prod bhp loss t1 13.321371\n",
      "Epoch 6/10, Batch 851/1500, Loss 124.011742, Loss rec 12.999378, loss rec t1 26.957214, loss kl 0.485699, loss_trans 0.055783, loss flux 31.674862, loss flux t1 31.901539, prod bhp loss 5.926982, prod bhp loss t1 14.495989\n",
      "Epoch 6/10, Batch 901/1500, Loss 153.663132, Loss rec 26.896067, loss rec t1 28.743372, loss kl 0.955289, loss_trans 0.069808, loss flux 35.950424, loss flux t1 37.895653, prod bhp loss 12.826868, prod bhp loss t1 11.280952\n",
      "Epoch 6/10, Batch 951/1500, Loss 271.926300, Loss rec 21.286894, loss rec t1 135.830200, loss kl 0.884282, loss_trans 0.209552, loss flux 30.748724, loss flux t1 49.472893, prod bhp loss 8.569674, prod bhp loss t1 25.808346\n",
      "Epoch 6/10, Batch 1001/1500, Loss 116.476311, Loss rec 15.309200, loss rec t1 23.311567, loss kl 0.345909, loss_trans 0.034706, loss flux 30.756861, loss flux t1 31.648832, prod bhp loss 5.048505, prod bhp loss t1 10.366642\n",
      "Epoch 6/10, Batch 1051/1500, Loss 274.642822, Loss rec 68.671013, loss rec t1 77.664230, loss kl 1.036924, loss_trans 0.101289, loss flux 38.646034, loss flux t1 38.310822, prod bhp loss 23.952694, prod bhp loss t1 27.296764\n",
      "Epoch 6/10, Batch 1101/1500, Loss 147.773483, Loss rec 21.390558, loss rec t1 37.727200, loss kl 0.820869, loss_trans 0.095796, loss flux 33.877190, loss flux t1 35.178627, prod bhp loss 5.455512, prod bhp loss t1 14.048594\n",
      "Epoch 6/10, Batch 1151/1500, Loss 161.137756, Loss rec 29.888443, loss rec t1 41.973087, loss kl 0.944133, loss_trans 0.096447, loss flux 33.592781, loss flux t1 35.034771, prod bhp loss 8.634285, prod bhp loss t1 11.917957\n",
      "Epoch 6/10, Batch 1201/1500, Loss 158.211670, Loss rec 28.795692, loss rec t1 30.284588, loss kl 1.240172, loss_trans 0.138415, loss flux 37.228386, loss flux t1 37.457157, prod bhp loss 13.443923, prod bhp loss t1 10.863514\n",
      "Epoch 6/10, Batch 1251/1500, Loss 141.223511, Loss rec 24.792662, loss rec t1 32.732296, loss kl 0.741778, loss_trans 0.082065, loss flux 30.981974, loss flux t1 32.822227, prod bhp loss 7.885715, prod bhp loss t1 11.926574\n",
      "Epoch 6/10, Batch 1301/1500, Loss 161.920593, Loss rec 36.802696, loss rec t1 33.672119, loss kl 0.381373, loss_trans 0.037720, loss flux 29.549311, loss flux t1 29.663578, prod bhp loss 17.739363, prod bhp loss t1 14.455807\n",
      "Epoch 6/10, Batch 1351/1500, Loss 132.927521, Loss rec 16.502752, loss rec t1 30.740715, loss kl 0.671556, loss_trans 0.066388, loss flux 32.683418, loss flux t1 34.016132, prod bhp loss 5.182631, prod bhp loss t1 13.735477\n",
      "Epoch 6/10, Batch 1401/1500, Loss 180.082153, Loss rec 14.715256, loss rec t1 71.863091, loss kl 0.723696, loss_trans 0.122730, loss flux 26.301317, loss flux t1 42.190468, prod bhp loss 5.735152, prod bhp loss t1 19.154142\n",
      "Epoch 6/10, Batch 1451/1500, Loss 181.358795, Loss rec 17.859230, loss rec t1 64.525490, loss kl 1.094546, loss_trans 0.143619, loss flux 31.018373, loss flux t1 47.156155, prod bhp loss 4.619039, prod bhp loss t1 16.036900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 6/10, Train loss 147.518066, Eval loss 250.327988\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 7/10, Batch 1/1500, Loss 170.391724, Loss rec 14.879498, loss rec t1 68.147896, loss kl 0.518436, loss_trans 0.110388, loss flux 23.314312, loss flux t1 39.409149, prod bhp loss 5.833701, prod bhp loss t1 18.696800\n",
      "Epoch 7/10, Batch 51/1500, Loss 374.244476, Loss rec 24.198294, loss rec t1 232.079895, loss kl 0.381052, loss_trans 0.270539, loss flux 24.509640, loss flux t1 44.399632, prod bhp loss 11.316113, prod bhp loss t1 37.470383\n",
      "Epoch 7/10, Batch 101/1500, Loss 201.369171, Loss rec 11.742079, loss rec t1 102.094635, loss kl 0.405273, loss_trans 0.109185, loss flux 21.699709, loss flux t1 37.256641, prod bhp loss 5.472520, prod bhp loss t1 22.994394\n",
      "Epoch 7/10, Batch 151/1500, Loss 216.602081, Loss rec 42.586342, loss rec t1 58.179565, loss kl 1.071868, loss_trans 0.087554, loss flux 36.723377, loss flux t1 38.958145, prod bhp loss 18.354828, prod bhp loss t1 21.712265\n",
      "Epoch 7/10, Batch 201/1500, Loss 182.305435, Loss rec 20.904654, loss rec t1 68.008163, loss kl 0.644789, loss_trans 0.113978, loss flux 28.505226, loss flux t1 44.982292, prod bhp loss 4.808269, prod bhp loss t1 14.982848\n",
      "Epoch 7/10, Batch 251/1500, Loss 212.377380, Loss rec 46.151970, loss rec t1 57.304077, loss kl 0.965749, loss_trans 0.090665, loss flux 34.151665, loss flux t1 34.688931, prod bhp loss 17.901386, prod bhp loss t1 22.088684\n",
      "Epoch 7/10, Batch 301/1500, Loss 141.660049, Loss rec 16.450209, loss rec t1 35.539005, loss kl 0.543874, loss_trans 0.059946, loss flux 32.495056, loss flux t1 33.571503, prod bhp loss 6.973817, prod bhp loss t1 16.570509\n",
      "Epoch 7/10, Batch 351/1500, Loss 126.177925, Loss rec 16.267984, loss rec t1 27.724649, loss kl 0.550010, loss_trans 0.055567, loss flux 30.558384, loss flux t1 30.656845, prod bhp loss 8.083611, prod bhp loss t1 12.830883\n",
      "Epoch 7/10, Batch 401/1500, Loss 147.656982, Loss rec 25.566574, loss rec t1 41.949043, loss kl 0.492164, loss_trans 0.037566, loss flux 28.425100, loss flux t1 29.898621, prod bhp loss 7.791591, prod bhp loss t1 13.988475\n",
      "Epoch 7/10, Batch 451/1500, Loss 201.503235, Loss rec 20.971502, loss rec t1 81.033356, loss kl 0.890501, loss_trans 0.159513, loss flux 27.645918, loss flux t1 43.178425, prod bhp loss 8.308578, prod bhp loss t1 20.205948\n",
      "Epoch 7/10, Batch 501/1500, Loss 161.939987, Loss rec 19.915493, loss rec t1 50.474144, loss kl 0.652770, loss_trans 0.064324, loss flux 30.117744, loss flux t1 31.994236, prod bhp loss 8.628322, prod bhp loss t1 20.745714\n",
      "Epoch 7/10, Batch 551/1500, Loss 143.061676, Loss rec 18.756069, loss rec t1 37.298851, loss kl 0.596126, loss_trans 0.069182, loss flux 30.058786, loss flux t1 32.114491, prod bhp loss 9.010719, prod bhp loss t1 15.753586\n",
      "Epoch 7/10, Batch 601/1500, Loss 126.485550, Loss rec 17.913452, loss rec t1 24.441875, loss kl 0.828009, loss_trans 0.061656, loss flux 32.780384, loss flux t1 33.484188, prod bhp loss 6.550628, prod bhp loss t1 11.253365\n",
      "Epoch 7/10, Batch 651/1500, Loss 133.964798, Loss rec 15.048594, loss rec t1 31.976292, loss kl 0.707741, loss_trans 0.058672, loss flux 32.461102, loss flux t1 33.216145, prod bhp loss 5.299936, prod bhp loss t1 15.904053\n",
      "Epoch 7/10, Batch 701/1500, Loss 144.110809, Loss rec 23.703091, loss rec t1 34.107670, loss kl 0.540818, loss_trans 0.045492, loss flux 30.097782, loss flux t1 31.179197, prod bhp loss 10.782668, prod bhp loss t1 14.194909\n",
      "Epoch 7/10, Batch 751/1500, Loss 134.084824, Loss rec 14.689468, loss rec t1 34.352501, loss kl 0.633005, loss_trans 0.081305, loss flux 30.290926, loss flux t1 31.548521, prod bhp loss 6.424232, prod bhp loss t1 16.697870\n",
      "Epoch 7/10, Batch 801/1500, Loss 106.273804, Loss rec 11.193010, loss rec t1 23.464924, loss kl 0.479172, loss_trans 0.042129, loss flux 26.555428, loss flux t1 27.762478, prod bhp loss 4.380723, prod bhp loss t1 12.875110\n",
      "Epoch 7/10, Batch 851/1500, Loss 122.439575, Loss rec 11.838796, loss rec t1 32.056511, loss kl 0.464086, loss_trans 0.050161, loss flux 28.801453, loss flux t1 29.286896, prod bhp loss 5.027396, prod bhp loss t1 15.378354\n",
      "Epoch 7/10, Batch 901/1500, Loss 136.509369, Loss rec 22.798145, loss rec t1 24.743706, loss kl 0.879925, loss_trans 0.054889, loss flux 32.670586, loss flux t1 34.257549, prod bhp loss 12.067537, prod bhp loss t1 9.916959\n",
      "Epoch 7/10, Batch 951/1500, Loss 227.468307, Loss rec 17.845669, loss rec t1 104.706108, loss kl 0.811855, loss_trans 0.187605, loss flux 27.576693, loss flux t1 46.309410, prod bhp loss 7.613308, prod bhp loss t1 23.229528\n",
      "Epoch 7/10, Batch 1001/1500, Loss 104.889870, Loss rec 14.238807, loss rec t1 19.483055, loss kl 0.343323, loss_trans 0.035777, loss flux 27.413754, loss flux t1 28.322521, prod bhp loss 6.115469, prod bhp loss t1 9.280488\n",
      "Epoch 7/10, Batch 1051/1500, Loss 224.546646, Loss rec 52.567364, loss rec t1 63.388691, loss kl 0.913316, loss_trans 0.074546, loss flux 34.863857, loss flux t1 34.568150, prod bhp loss 17.460485, prod bhp loss t1 21.623569\n",
      "Epoch 7/10, Batch 1101/1500, Loss 131.215851, Loss rec 17.604376, loss rec t1 33.266998, loss kl 0.775905, loss_trans 0.077817, loss flux 30.257412, loss flux t1 31.829451, prod bhp loss 4.617023, prod bhp loss t1 13.562767\n",
      "Epoch 7/10, Batch 1151/1500, Loss 155.792587, Loss rec 30.406189, loss rec t1 41.654453, loss kl 0.879513, loss_trans 0.076477, loss flux 29.889208, loss flux t1 31.677704, prod bhp loss 10.012895, prod bhp loss t1 12.075665\n",
      "Epoch 7/10, Batch 1201/1500, Loss 181.320526, Loss rec 48.492290, loss rec t1 31.056841, loss kl 1.127580, loss_trans 0.105042, loss flux 35.190289, loss flux t1 35.348274, prod bhp loss 20.987278, prod bhp loss t1 10.140495\n",
      "Epoch 7/10, Batch 1251/1500, Loss 115.993362, Loss rec 20.127119, loss rec t1 22.028969, loss kl 0.699286, loss_trans 0.068249, loss flux 28.150347, loss flux t1 29.839111, prod bhp loss 8.502939, prod bhp loss t1 7.276631\n",
      "Epoch 7/10, Batch 1301/1500, Loss 117.597198, Loss rec 20.562664, loss rec t1 20.110294, loss kl 0.370440, loss_trans 0.034484, loss flux 26.941807, loss flux t1 27.026138, prod bhp loss 12.296844, prod bhp loss t1 10.624972\n",
      "Epoch 7/10, Batch 1351/1500, Loss 131.683838, Loss rec 20.576643, loss rec t1 28.295631, loss kl 0.651132, loss_trans 0.058158, loss flux 29.415739, loss flux t1 31.119349, prod bhp loss 9.401940, prod bhp loss t1 12.816387\n",
      "Epoch 7/10, Batch 1401/1500, Loss 179.204422, Loss rec 13.683006, loss rec t1 76.062958, loss kl 0.702665, loss_trans 0.112547, loss flux 24.326685, loss flux t1 39.545345, prod bhp loss 7.152810, prod bhp loss t1 18.321066\n",
      "Epoch 7/10, Batch 1451/1500, Loss 168.202454, Loss rec 16.206764, loss rec t1 59.010895, loss kl 1.038334, loss_trans 0.136324, loss flux 27.285435, loss flux t1 44.481628, prod bhp loss 4.852633, prod bhp loss t1 16.228771\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 7/10, Train loss 151.347794, Eval loss 259.210358\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 8/10, Batch 1/1500, Loss 168.105667, Loss rec 15.202704, loss rec t1 64.380707, loss kl 0.478205, loss_trans 0.102048, loss flux 21.753267, loss flux t1 37.603439, prod bhp loss 8.540110, prod bhp loss t1 20.523380\n",
      "Epoch 8/10, Batch 51/1500, Loss 330.405609, Loss rec 15.888407, loss rec t1 203.576416, loss kl 0.368718, loss_trans 0.252404, loss flux 22.975004, loss flux t1 42.485016, prod bhp loss 10.522482, prod bhp loss t1 34.705856\n",
      "Epoch 8/10, Batch 101/1500, Loss 193.885498, Loss rec 10.602593, loss rec t1 100.001503, loss kl 0.391399, loss_trans 0.104928, loss flux 19.852942, loss flux t1 35.698845, prod bhp loss 4.947770, prod bhp loss t1 22.676910\n",
      "Epoch 8/10, Batch 151/1500, Loss 187.924973, Loss rec 36.294491, loss rec t1 46.139332, loss kl 0.995750, loss_trans 0.076218, loss flux 33.859322, loss flux t1 36.206055, prod bhp loss 16.775829, prod bhp loss t1 18.573723\n",
      "Epoch 8/10, Batch 201/1500, Loss 163.691605, Loss rec 17.482038, loss rec t1 59.851227, loss kl 0.621147, loss_trans 0.102806, loss flux 25.788219, loss flux t1 42.524509, prod bhp loss 4.123353, prod bhp loss t1 13.819452\n",
      "Epoch 8/10, Batch 251/1500, Loss 164.229279, Loss rec 28.376472, loss rec t1 42.916573, loss kl 0.872114, loss_trans 0.071550, loss flux 30.544283, loss flux t1 31.289787, prod bhp loss 12.465712, prod bhp loss t1 18.564915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Batch 301/1500, Loss 122.653770, Loss rec 14.093686, loss rec t1 29.224962, loss kl 0.535538, loss_trans 0.050883, loss flux 28.876875, loss flux t1 29.784798, prod bhp loss 5.912075, prod bhp loss t1 14.710499\n",
      "Epoch 8/10, Batch 351/1500, Loss 106.793053, Loss rec 10.939372, loss rec t1 22.411701, loss kl 0.516372, loss_trans 0.047173, loss flux 27.590158, loss flux t1 28.643549, prod bhp loss 5.211547, prod bhp loss t1 11.949556\n",
      "Epoch 8/10, Batch 401/1500, Loss 121.653999, Loss rec 17.677279, loss rec t1 33.341328, loss kl 0.475792, loss_trans 0.036582, loss flux 25.573851, loss flux t1 26.834185, prod bhp loss 4.407091, prod bhp loss t1 13.783686\n",
      "Epoch 8/10, Batch 451/1500, Loss 187.677200, Loss rec 18.620583, loss rec t1 73.904144, loss kl 0.842036, loss_trans 0.137247, loss flux 26.411573, loss flux t1 41.904716, prod bhp loss 8.055933, prod bhp loss t1 18.642982\n",
      "Epoch 8/10, Batch 501/1500, Loss 146.635056, Loss rec 14.451952, loss rec t1 50.031166, loss kl 0.615293, loss_trans 0.054809, loss flux 26.665602, loss flux t1 28.549875, prod bhp loss 4.890119, prod bhp loss t1 21.991531\n",
      "Epoch 8/10, Batch 551/1500, Loss 135.476822, Loss rec 16.926023, loss rec t1 34.652527, loss kl 0.561080, loss_trans 0.056311, loss flux 27.619974, loss flux t1 30.457260, prod bhp loss 9.706164, prod bhp loss t1 16.058571\n",
      "Epoch 8/10, Batch 601/1500, Loss 120.871918, Loss rec 17.006840, loss rec t1 24.695578, loss kl 0.776035, loss_trans 0.051637, loss flux 29.388222, loss flux t1 30.477713, prod bhp loss 7.455686, prod bhp loss t1 11.796240\n",
      "Epoch 8/10, Batch 651/1500, Loss 124.045204, Loss rec 13.048686, loss rec t1 29.701620, loss kl 0.677573, loss_trans 0.051486, loss flux 29.605747, loss flux t1 30.900043, prod bhp loss 5.366663, prod bhp loss t1 15.370973\n",
      "Epoch 8/10, Batch 701/1500, Loss 137.431824, Loss rec 23.337692, loss rec t1 33.717472, loss kl 0.506485, loss_trans 0.039281, loss flux 27.084406, loss flux t1 28.372929, prod bhp loss 11.328780, prod bhp loss t1 13.551275\n",
      "Epoch 8/10, Batch 751/1500, Loss 174.403992, Loss rec 31.551815, loss rec t1 56.739380, loss kl 0.608860, loss_trans 0.080305, loss flux 29.568462, loss flux t1 31.194609, prod bhp loss 6.139058, prod bhp loss t1 19.130354\n",
      "Epoch 8/10, Batch 801/1500, Loss 97.224899, Loss rec 10.715345, loss rec t1 20.775616, loss kl 0.458225, loss_trans 0.035965, loss flux 24.231777, loss flux t1 25.821602, prod bhp loss 4.441957, prod bhp loss t1 11.202640\n",
      "Epoch 8/10, Batch 851/1500, Loss 114.610367, Loss rec 11.018562, loss rec t1 29.765379, loss kl 0.442445, loss_trans 0.045116, loss flux 26.723501, loss flux t1 27.144318, prod bhp loss 4.564931, prod bhp loss t1 15.348557\n",
      "Epoch 8/10, Batch 901/1500, Loss 126.547264, Loss rec 19.999893, loss rec t1 23.241295, loss kl 0.856392, loss_trans 0.050755, loss flux 30.493238, loss flux t1 31.761631, prod bhp loss 11.208039, prod bhp loss t1 9.792408\n",
      "Epoch 8/10, Batch 951/1500, Loss 172.410645, Loss rec 13.717052, loss rec t1 65.055809, loss kl 0.790329, loss_trans 0.172468, loss flux 25.193844, loss flux t1 44.138241, prod bhp loss 7.217185, prod bhp loss t1 16.916050\n",
      "Epoch 8/10, Batch 1001/1500, Loss 100.396950, Loss rec 12.741800, loss rec t1 21.423018, loss kl 0.351062, loss_trans 0.029804, loss flux 25.162838, loss flux t1 26.485983, prod bhp loss 4.918349, prod bhp loss t1 9.635160\n",
      "Epoch 8/10, Batch 1051/1500, Loss 195.520370, Loss rec 44.842766, loss rec t1 50.590336, loss kl 0.884763, loss_trans 0.067051, loss flux 31.732502, loss flux t1 31.647430, prod bhp loss 17.093887, prod bhp loss t1 19.546408\n",
      "Epoch 8/10, Batch 1101/1500, Loss 125.277016, Loss rec 16.692602, loss rec t1 32.522415, loss kl 0.762551, loss_trans 0.072767, loss flux 27.803268, loss flux t1 29.449114, prod bhp loss 5.409369, prod bhp loss t1 13.327478\n",
      "Epoch 8/10, Batch 1151/1500, Loss 129.712402, Loss rec 22.971199, loss rec t1 26.587271, loss kl 0.857268, loss_trans 0.068901, loss flux 28.241383, loss flux t1 29.960573, prod bhp loss 10.786156, prod bhp loss t1 11.096913\n",
      "Epoch 8/10, Batch 1201/1500, Loss 214.378922, Loss rec 68.412292, loss rec t1 37.239506, loss kl 1.066741, loss_trans 0.075400, loss flux 34.424278, loss flux t1 33.833923, prod bhp loss 27.138130, prod bhp loss t1 13.255395\n",
      "Epoch 8/10, Batch 1251/1500, Loss 109.054413, Loss rec 16.168056, loss rec t1 21.960949, loss kl 0.685656, loss_trans 0.060043, loss flux 25.770346, loss flux t1 27.813051, prod bhp loss 7.416213, prod bhp loss t1 9.865757\n",
      "Epoch 8/10, Batch 1301/1500, Loss 91.821594, Loss rec 11.997398, loss rec t1 14.554082, loss kl 0.386421, loss_trans 0.033257, loss flux 24.383064, loss flux t1 25.215252, prod bhp loss 7.573715, prod bhp loss t1 8.064828\n",
      "Epoch 8/10, Batch 1351/1500, Loss 117.567368, Loss rec 17.774719, loss rec t1 26.003637, loss kl 0.630116, loss_trans 0.051247, loss flux 26.722696, loss flux t1 28.117868, prod bhp loss 7.175116, prod bhp loss t1 11.722086\n",
      "Epoch 8/10, Batch 1401/1500, Loss 193.231766, Loss rec 19.751934, loss rec t1 79.022911, loss kl 0.714527, loss_trans 0.108099, loss flux 23.915094, loss flux t1 39.034660, prod bhp loss 10.918625, prod bhp loss t1 20.480457\n",
      "Epoch 8/10, Batch 1451/1500, Loss 149.915085, Loss rec 13.558250, loss rec t1 49.483414, loss kl 0.962747, loss_trans 0.111969, loss flux 25.263235, loss flux t1 41.957813, prod bhp loss 5.286619, prod bhp loss t1 14.253780\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 8/10, Train loss 144.731476, Eval loss 241.143646\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 9/10, Batch 1/1500, Loss 187.305557, Loss rec 23.287804, loss rec t1 68.049110, loss kl 0.458198, loss_trans 0.099626, loss flux 20.711838, loss flux t1 36.657791, prod bhp loss 14.236773, prod bhp loss t1 24.262619\n",
      "Epoch 9/10, Batch 51/1500, Loss 291.461700, Loss rec 10.284914, loss rec t1 182.431961, loss kl 0.365080, loss_trans 0.230828, loss flux 21.484116, loss flux t1 41.247238, prod bhp loss 6.800698, prod bhp loss t1 28.981936\n",
      "Epoch 9/10, Batch 101/1500, Loss 172.134781, Loss rec 11.043951, loss rec t1 85.856834, loss kl 0.397175, loss_trans 0.099955, loss flux 18.390362, loss flux t1 32.560818, prod bhp loss 3.884563, prod bhp loss t1 20.298273\n",
      "Epoch 9/10, Batch 151/1500, Loss 156.880722, Loss rec 30.038815, loss rec t1 37.314560, loss kl 0.962198, loss_trans 0.068766, loss flux 30.530405, loss flux t1 32.986965, prod bhp loss 11.201544, prod bhp loss t1 14.739676\n",
      "Epoch 9/10, Batch 201/1500, Loss 143.026291, Loss rec 10.967729, loss rec t1 51.364723, loss kl 0.596146, loss_trans 0.095566, loss flux 23.198782, loss flux t1 39.993618, prod bhp loss 3.236119, prod bhp loss t1 14.169762\n",
      "Epoch 9/10, Batch 251/1500, Loss 135.361008, Loss rec 21.487560, loss rec t1 34.629486, loss kl 0.819904, loss_trans 0.057751, loss flux 27.873020, loss flux t1 28.557802, prod bhp loss 8.539053, prod bhp loss t1 14.216333\n",
      "Epoch 9/10, Batch 301/1500, Loss 119.426659, Loss rec 15.076405, loss rec t1 28.294319, loss kl 0.502843, loss_trans 0.049897, loss flux 27.230228, loss flux t1 28.505529, prod bhp loss 6.000166, prod bhp loss t1 14.270108\n",
      "Epoch 9/10, Batch 351/1500, Loss 102.180321, Loss rec 10.082254, loss rec t1 21.930740, loss kl 0.508848, loss_trans 0.043135, loss flux 26.000736, loss flux t1 27.178680, prod bhp loss 4.643023, prod bhp loss t1 12.301748\n",
      "Epoch 9/10, Batch 401/1500, Loss 105.932571, Loss rec 15.297131, loss rec t1 27.383440, loss kl 0.479199, loss_trans 0.035487, loss flux 23.184744, loss flux t1 24.450638, prod bhp loss 3.690873, prod bhp loss t1 11.890257\n",
      "Epoch 9/10, Batch 451/1500, Loss 168.668076, Loss rec 15.276050, loss rec t1 67.219490, loss kl 0.851395, loss_trans 0.130161, loss flux 23.509941, loss flux t1 39.413887, prod bhp loss 7.201694, prod bhp loss t1 15.916851\n",
      "Epoch 9/10, Batch 501/1500, Loss 140.217758, Loss rec 13.913914, loss rec t1 48.504486, loss kl 0.618926, loss_trans 0.051160, loss flux 24.899099, loss flux t1 26.629419, prod bhp loss 4.878999, prod bhp loss t1 21.340679\n",
      "Epoch 9/10, Batch 551/1500, Loss 128.825958, Loss rec 16.124256, loss rec t1 32.952850, loss kl 0.547247, loss_trans 0.053102, loss flux 25.549023, loss flux t1 28.598978, prod bhp loss 9.924603, prod bhp loss t1 15.623148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Batch 601/1500, Loss 112.725410, Loss rec 14.696182, loss rec t1 24.417311, loss kl 0.760971, loss_trans 0.047995, loss flux 25.931519, loss flux t1 27.615591, prod bhp loss 7.394941, prod bhp loss t1 12.621868\n",
      "Epoch 9/10, Batch 651/1500, Loss 123.780685, Loss rec 13.470760, loss rec t1 32.434338, loss kl 0.663631, loss_trans 0.048179, loss flux 26.831444, loss flux t1 28.274302, prod bhp loss 6.086837, prod bhp loss t1 16.634827\n",
      "Epoch 9/10, Batch 701/1500, Loss 125.988548, Loss rec 22.860191, loss rec t1 29.850471, loss kl 0.505839, loss_trans 0.040404, loss flux 24.811890, loss flux t1 26.741270, prod bhp loss 10.533954, prod bhp loss t1 11.150368\n",
      "Epoch 9/10, Batch 751/1500, Loss 156.819244, Loss rec 16.052956, loss rec t1 55.798817, loss kl 0.622886, loss_trans 0.087285, loss flux 26.480082, loss flux t1 29.209044, prod bhp loss 5.033558, prod bhp loss t1 24.157505\n",
      "Epoch 9/10, Batch 801/1500, Loss 95.074059, Loss rec 9.811247, loss rec t1 20.392448, loss kl 0.465293, loss_trans 0.033772, loss flux 23.509583, loss flux t1 24.869507, prod bhp loss 4.625721, prod bhp loss t1 11.831771\n",
      "Epoch 9/10, Batch 851/1500, Loss 105.742447, Loss rec 11.640116, loss rec t1 25.537930, loss kl 0.450086, loss_trans 0.042038, loss flux 25.265076, loss flux t1 24.955519, prod bhp loss 4.422786, prod bhp loss t1 13.878984\n",
      "Epoch 9/10, Batch 901/1500, Loss 109.153893, Loss rec 16.407448, loss rec t1 17.824898, loss kl 0.850879, loss_trans 0.051669, loss flux 28.165173, loss flux t1 29.951021, prod bhp loss 9.290453, prod bhp loss t1 7.463235\n",
      "Epoch 9/10, Batch 951/1500, Loss 160.967102, Loss rec 14.583358, loss rec t1 59.896008, loss kl 0.786571, loss_trans 0.181422, loss flux 23.094650, loss flux t1 41.111420, prod bhp loss 5.720433, prod bhp loss t1 16.379820\n",
      "Epoch 9/10, Batch 1001/1500, Loss 93.810257, Loss rec 10.489101, loss rec t1 20.158237, loss kl 0.362284, loss_trans 0.032212, loss flux 23.727848, loss flux t1 24.621395, prod bhp loss 4.421440, prod bhp loss t1 10.360023\n",
      "Epoch 9/10, Batch 1051/1500, Loss 143.265076, Loss rec 30.408485, loss rec t1 31.890245, loss kl 0.850703, loss_trans 0.063175, loss flux 29.270357, loss flux t1 28.888212, prod bhp loss 11.020764, prod bhp loss t1 11.723854\n",
      "Epoch 9/10, Batch 1101/1500, Loss 111.144203, Loss rec 14.532549, loss rec t1 26.314423, loss kl 0.740062, loss_trans 0.068062, loss flux 25.805622, loss flux t1 27.072834, prod bhp loss 5.510027, prod bhp loss t1 11.840696\n",
      "Epoch 9/10, Batch 1151/1500, Loss 117.973137, Loss rec 20.989271, loss rec t1 23.114626, loss kl 0.856488, loss_trans 0.065787, loss flux 26.445591, loss flux t1 28.006861, prod bhp loss 9.165436, prod bhp loss t1 10.185566\n",
      "Epoch 9/10, Batch 1201/1500, Loss 239.922302, Loss rec 80.150925, loss rec t1 45.791935, loss kl 1.002990, loss_trans 0.063036, loss flux 33.785862, loss flux t1 32.921703, prod bhp loss 29.980890, prod bhp loss t1 17.227947\n",
      "Epoch 9/10, Batch 1251/1500, Loss 108.348351, Loss rec 14.106270, loss rec t1 26.422785, loss kl 0.703462, loss_trans 0.061186, loss flux 24.437632, loss flux t1 26.221085, prod bhp loss 5.759933, prod bhp loss t1 11.339462\n",
      "Epoch 9/10, Batch 1301/1500, Loss 107.192558, Loss rec 17.156622, loss rec t1 22.855398, loss kl 0.384613, loss_trans 0.031591, loss flux 23.988075, loss flux t1 25.284822, prod bhp loss 8.381110, prod bhp loss t1 9.494938\n",
      "Epoch 9/10, Batch 1351/1500, Loss 108.823189, Loss rec 14.409553, loss rec t1 23.631449, loss kl 0.651182, loss_trans 0.051379, loss flux 26.224438, loss flux t1 27.629528, prod bhp loss 5.709944, prod bhp loss t1 11.166892\n",
      "Epoch 9/10, Batch 1401/1500, Loss 185.728683, Loss rec 27.074144, loss rec t1 69.044632, loss kl 0.755862, loss_trans 0.113477, loss flux 22.026815, loss flux t1 37.993034, prod bhp loss 11.093239, prod bhp loss t1 18.383337\n",
      "Epoch 9/10, Batch 1451/1500, Loss 176.001053, Loss rec 24.184841, loss rec t1 57.560440, loss kl 0.986212, loss_trans 0.120624, loss flux 25.341309, loss flux t1 42.123631, prod bhp loss 10.543388, prod bhp loss t1 16.126829\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 9/10, Train loss 124.315239, Eval loss 224.821259\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 10/10, Batch 1/1500, Loss 181.281738, Loss rec 16.162895, loss rec t1 77.994553, loss kl 0.490300, loss_trans 0.127109, loss flux 19.198996, loss flux t1 34.249752, prod bhp loss 8.540888, prod bhp loss t1 25.007545\n",
      "Epoch 10/10, Batch 51/1500, Loss 226.183258, Loss rec 13.612436, loss rec t1 124.476982, loss kl 0.385174, loss_trans 0.199086, loss flux 21.496222, loss flux t1 42.923798, prod bhp loss 7.333380, prod bhp loss t1 16.141375\n",
      "Epoch 10/10, Batch 101/1500, Loss 165.465881, Loss rec 9.901910, loss rec t1 81.482948, loss kl 0.395343, loss_trans 0.100777, loss flux 17.296991, loss flux t1 32.349972, prod bhp loss 5.219587, prod bhp loss t1 19.113688\n",
      "Epoch 10/10, Batch 151/1500, Loss 152.235504, Loss rec 24.659245, loss rec t1 41.841385, loss kl 0.943739, loss_trans 0.067062, loss flux 29.555998, loss flux t1 32.026508, prod bhp loss 9.788000, prod bhp loss t1 14.297316\n",
      "Epoch 10/10, Batch 201/1500, Loss 139.986252, Loss rec 9.239050, loss rec t1 52.669662, loss kl 0.599634, loss_trans 0.094333, loss flux 20.633934, loss flux t1 37.349300, prod bhp loss 3.995908, prod bhp loss t1 16.004055\n",
      "Epoch 10/10, Batch 251/1500, Loss 119.516373, Loss rec 17.327156, loss rec t1 30.654762, loss kl 0.840623, loss_trans 0.058765, loss flux 25.829016, loss flux t1 26.502218, prod bhp loss 6.670780, prod bhp loss t1 12.473682\n",
      "Epoch 10/10, Batch 301/1500, Loss 91.106300, Loss rec 7.253348, loss rec t1 17.809589, loss kl 0.532601, loss_trans 0.049937, loss flux 25.007227, loss flux t1 26.743700, prod bhp loss 3.592812, prod bhp loss t1 10.649693\n",
      "Epoch 10/10, Batch 351/1500, Loss 96.879166, Loss rec 10.381885, loss rec t1 21.188717, loss kl 0.524877, loss_trans 0.040361, loss flux 23.851192, loss flux t1 24.759123, prod bhp loss 4.770084, prod bhp loss t1 11.887812\n",
      "Epoch 10/10, Batch 401/1500, Loss 96.666656, Loss rec 10.815451, loss rec t1 25.594343, loss kl 0.472262, loss_trans 0.037934, loss flux 21.324743, loss flux t1 22.852392, prod bhp loss 3.872671, prod bhp loss t1 12.169125\n",
      "Epoch 10/10, Batch 451/1500, Loss 141.449921, Loss rec 13.139030, loss rec t1 49.260155, loss kl 0.839848, loss_trans 0.117685, loss flux 21.722740, loss flux t1 37.385700, prod bhp loss 5.813089, prod bhp loss t1 14.011511\n",
      "Epoch 10/10, Batch 501/1500, Loss 132.233093, Loss rec 15.259504, loss rec t1 42.745224, loss kl 0.620491, loss_trans 0.048978, loss flux 23.484335, loss flux t1 25.212833, prod bhp loss 6.472899, prod bhp loss t1 19.009315\n",
      "Epoch 10/10, Batch 551/1500, Loss 116.390236, Loss rec 14.707040, loss rec t1 30.824032, loss kl 0.542327, loss_trans 0.048167, loss flux 23.991310, loss flux t1 26.194174, prod bhp loss 7.108315, prod bhp loss t1 13.517199\n",
      "Epoch 10/10, Batch 601/1500, Loss 105.314346, Loss rec 13.127228, loss rec t1 21.000982, loss kl 0.780730, loss_trans 0.047517, loss flux 26.182550, loss flux t1 27.623743, prod bhp loss 6.514390, prod bhp loss t1 10.817940\n",
      "Epoch 10/10, Batch 651/1500, Loss 125.635452, Loss rec 13.210527, loss rec t1 36.112026, loss kl 0.651711, loss_trans 0.048413, loss flux 25.739613, loss flux t1 27.239140, prod bhp loss 5.243739, prod bhp loss t1 18.041998\n",
      "Epoch 10/10, Batch 701/1500, Loss 104.915741, Loss rec 15.549246, loss rec t1 23.571800, loss kl 0.519901, loss_trans 0.042831, loss flux 23.388496, loss flux t1 25.996981, prod bhp loss 6.922480, prod bhp loss t1 9.443909\n",
      "Epoch 10/10, Batch 751/1500, Loss 150.355865, Loss rec 14.527149, loss rec t1 55.433136, loss kl 0.640990, loss_trans 0.072609, loss flux 25.403610, loss flux t1 27.645220, prod bhp loss 5.248909, prod bhp loss t1 22.025248\n",
      "Epoch 10/10, Batch 801/1500, Loss 91.804985, Loss rec 9.414625, loss rec t1 21.179327, loss kl 0.480304, loss_trans 0.034650, loss flux 21.339691, loss flux t1 22.676289, prod bhp loss 4.198980, prod bhp loss t1 12.961420\n",
      "Epoch 10/10, Batch 851/1500, Loss 95.112099, Loss rec 8.615927, loss rec t1 21.964457, loss kl 0.463493, loss_trans 0.041879, loss flux 23.476053, loss flux t1 23.779892, prod bhp loss 3.937217, prod bhp loss t1 13.296680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Batch 901/1500, Loss 103.115364, Loss rec 13.093022, loss rec t1 19.613970, loss kl 0.845531, loss_trans 0.053482, loss flux 26.256393, loss flux t1 28.306498, prod bhp loss 7.495326, prod bhp loss t1 8.296675\n",
      "Epoch 10/10, Batch 951/1500, Loss 158.034912, Loss rec 11.840683, loss rec t1 63.101738, loss kl 0.800026, loss_trans 0.150233, loss flux 20.724724, loss flux t1 38.088768, prod bhp loss 6.246641, prod bhp loss t1 17.882113\n",
      "Epoch 10/10, Batch 1001/1500, Loss 83.606812, Loss rec 9.609060, loss rec t1 16.198479, loss kl 0.356361, loss_trans 0.027877, loss flux 22.366554, loss flux t1 23.271360, prod bhp loss 3.957231, prod bhp loss t1 8.176249\n",
      "Epoch 10/10, Batch 1051/1500, Loss 141.319611, Loss rec 29.523558, loss rec t1 32.918831, loss kl 0.837460, loss_trans 0.057644, loss flux 28.312979, loss flux t1 28.094795, prod bhp loss 11.079024, prod bhp loss t1 11.332771\n",
      "Epoch 10/10, Batch 1101/1500, Loss 119.109657, Loss rec 17.173141, loss rec t1 29.846642, loss kl 0.774581, loss_trans 0.067083, loss flux 24.086735, loss flux t1 25.075642, prod bhp loss 8.994629, prod bhp loss t1 13.865784\n",
      "Epoch 10/10, Batch 1151/1500, Loss 126.961754, Loss rec 22.418650, loss rec t1 32.925690, loss kl 0.887524, loss_trans 0.067200, loss flux 24.979771, loss flux t1 27.380432, prod bhp loss 7.111910, prod bhp loss t1 12.078104\n",
      "Epoch 10/10, Batch 1201/1500, Loss 119.922928, Loss rec 24.165489, loss rec t1 18.079910, loss kl 1.048554, loss_trans 0.069678, loss flux 28.711048, loss flux t1 28.868406, prod bhp loss 13.096999, prod bhp loss t1 6.931387\n",
      "Epoch 10/10, Batch 1251/1500, Loss 122.104233, Loss rec 21.213459, loss rec t1 26.629274, loss kl 0.750018, loss_trans 0.070165, loss flux 25.451294, loss flux t1 27.337069, prod bhp loss 9.615650, prod bhp loss t1 11.787313\n",
      "Epoch 10/10, Batch 1301/1500, Loss 82.135994, Loss rec 9.204361, loss rec t1 14.055693, loss kl 0.395859, loss_trans 0.035915, loss flux 22.420010, loss flux t1 23.195847, prod bhp loss 5.121032, prod bhp loss t1 8.103146\n",
      "Epoch 10/10, Batch 1351/1500, Loss 99.607613, Loss rec 12.658866, loss rec t1 21.356003, loss kl 0.653967, loss_trans 0.048172, loss flux 24.064360, loss flux t1 25.552454, prod bhp loss 5.357043, prod bhp loss t1 10.570718\n",
      "Epoch 10/10, Batch 1401/1500, Loss 145.076569, Loss rec 23.505325, loss rec t1 43.234890, loss kl 0.748777, loss_trans 0.105174, loss flux 20.972349, loss flux t1 34.118721, prod bhp loss 10.848886, prod bhp loss t1 12.291210\n",
      "Epoch 10/10, Batch 1451/1500, Loss 172.726913, Loss rec 22.974615, loss rec t1 58.839684, loss kl 0.958900, loss_trans 0.107149, loss flux 23.016485, loss flux t1 39.875900, prod bhp loss 10.770878, prod bhp loss t1 17.142200\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 10/10, Train loss 146.735901, Eval loss 223.421188\n",
      "\n",
      "\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "# Optimization\n",
    "opt = Adam(lr=learning_rate)\n",
    "\n",
    "trainable_weights = encoder.trainable_weights + decoder.trainable_weights + transition.trainable_weights + wc_encoder.trainable_weights\n",
    "\n",
    "updates = opt.get_updates(loss, trainable_weights)\n",
    "\n",
    "# iterate = K.function([xt, ut, xt1, m_tf, dt], [loss, loss_rec_t, loss_rec_t1, loss_l2_reg, loss_trans, loss_flux_t, loss_flux_t1, binary_sat_loss_t, binary_sat_loss_t1], updates=updates)\n",
    "iterate = K.function([xt, ut, xt1, m_tf, wl_mask, dt], [loss, loss_rec_t, loss_rec_t1, loss_l2_reg, loss_trans, loss_flux_t, loss_flux_t1, loss_prod_bhp_t, loss_prod_bhp_t1], updates=updates)\n",
    "\n",
    "eval_loss = K.function([xt, ut, xt1, m_tf, wl_mask, dt], [loss])\n",
    "\n",
    "num_batch = int(num_train/batch_size)\n",
    "\n",
    "for e in range(epoch):\n",
    "    for ib in range(num_batch):\n",
    "        ind0 = ib * batch_size\n",
    "        state_t_batch  = state_t_train[ind0:ind0+batch_size, ...]\n",
    "        state_t1_batch = state_t1_train[ind0:ind0 + batch_size, ...]\n",
    "        bhp_batch      = bhp_train[ind0:ind0 + batch_size, ...]\n",
    "        m_batch        = m[ind0:ind0 + batch_size, ...]\n",
    "        dt_batch       = dt_train[ind0:ind0 + batch_size, ...]\n",
    "        wl_mask_batch  = wl_mask_train[ind0:ind0 + batch_size, ...]\n",
    "\n",
    "        output = iterate([state_t_batch, bhp_batch, state_t1_batch, m_batch, wl_mask_batch, dt_batch])\n",
    "\n",
    "        # tf.session.run(feed_dict={xt: sat_t_batch, ut: bhp_batch, xt1: sat_t1_batch}, ...\n",
    "        #                fetches= [loss, loss_rec_t, loss_rec_t1, loss_kl, loss_trans, updates])\n",
    "        # But output tensor for the updates operation is not returned\n",
    "        \n",
    "        n_itr = e * num_train + ib * batch_size + batch_size\n",
    "        write_summary(output[0], 'train/total_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        write_summary(output[1]+output[2], 'train/sum_rec_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        write_summary(output[5]+output[6], 'train/sum_flux_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        write_summary(output[7]+output[8], 'train/sum_well_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        \n",
    "        if ib % 50 == 0:\n",
    "            print('Epoch %d/%d, Batch %d/%d, Loss %f, Loss rec %f, loss rec t1 %f, loss kl %f, loss_trans %f, loss flux %f, loss flux t1 %f, prod bhp loss %f, prod bhp loss t1 %f'\n",
    "                  % (e+1, epoch, ib+1, num_batch, output[0], output[1], output[2], output[3], output[4], output[5], output[6], output[7], output[8]))\n",
    "            \n",
    "            eval_loss_val = eval_loss([state_t_eval, bhp_eval, state_t1_eval, m_eval, wl_mask_eval, dt_eval])\n",
    "            write_summary(eval_loss_val[0], 'eval/total_loss', summary_writer, n_itr) # log for tensorboard\n",
    "\n",
    "    \n",
    "    print('====================================================')\n",
    "    print('\\n')\n",
    "    print('Epoch %d/%d, Train loss %f, Eval loss %f' % (e + 1, epoch, output[0], eval_loss_val[0]))\n",
    "    print('\\n')\n",
    "    print('====================================================')\n",
    "\n",
    "encoder.save_weights(output_dir + 'e2c_encoder_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' % (num_train, latent_dim, learning_rate, epoch))\n",
    "decoder.save_weights(output_dir + 'e2c_decoder_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' % (num_train, latent_dim, learning_rate, epoch))\n",
    "transition.save_weights(output_dir + 'e2c_transition_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' % (num_train, latent_dim, learning_rate, epoch))\n",
    "wc_encoder.save_weights(output_dir + 'e2c_wc_encoder_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' % (num_train, latent_dim, learning_rate, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.10.0]",
   "language": "python",
   "name": "conda-env-tf-1.10.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
